{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce02c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.distributions import MultivariateNormal, Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "import io\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc92308",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = True\n",
    "LOAD_MODEL = False\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 1800\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 4\n",
    "Model = \"PPO\"\n",
    "save_interval = 100\n",
    "episode_p_interval = 4\n",
    "rew_p_interval = 5\n",
    "\n",
    "#Version\n",
    "version = 1\n",
    "\n",
    "#Hiperparametros\n",
    "LR = 2e-5\n",
    "GAMMA = 0.999\n",
    "\n",
    "#DQN y D3QN Params\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.99\n",
    "BUFFER_SIZE = 10000\n",
    "batch_size = 64\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "#D3QN\n",
    "UPDATE_TARGET_FREQ = 10000\n",
    "\n",
    "#PPO Params\n",
    "N_STEPS = 2048\n",
    "N_UPDATES_PER_ITERATION = 5\n",
    "CLIP = 0.1\n",
    "ENTROPY_COEF = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c752692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found in the directory or directory does not exist.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_last_modified_file(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
    "        return None\n",
    "    files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=os.path.getmtime, reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "model_load_path = get_last_modified_file(target_directory)\n",
    "\n",
    "if model_load_path:\n",
    "    print(f\"The last modified file is: {model_load_path}\")\n",
    "else:\n",
    "    print(\"No files found in the directory or directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985a3502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 17] File exists: '../Saved_Models'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO/Actor'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO/Critic'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/DQN'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/D3QN'\n",
      "Error: [Errno 17] File exists: '../Video'\n",
      "Error: [Errno 17] File exists: '../Video/PPO'\n",
      "Error: [Errno 17] File exists: '../Video/DQN'\n",
      "Error: [Errno 17] File exists: '../Video/D3QN'\n",
      "Error: [Errno 17] File exists: '../Logs/PPO'\n",
      "Error: [Errno 17] File exists: '../Logs/DQN'\n",
      "Error: [Errno 17] File exists: '../Logs/D3QN'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"../Saved_Models\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Actor\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Critic\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "try:\n",
    "    os.mkdir(\"../Video/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Logs/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "try:\n",
    "    os.mkdir(\"../Logs/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Logs/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937d962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/{Model}' #ppt para jit, pth para statedict\n",
    "    model_file_name = f'/{Model}-Sonic-V{version}-E{episode}-S{max_steps_per_episode}.pth'\n",
    "    try:\n",
    "        if Model == \"DQN\":\n",
    "            torch.save(agent.model.state_dict(), model_save_path+model_file_name)\n",
    "        if Model == \"D3QN\":\n",
    "            torch.save(agent.model_online.state_dict(), model_save_path+model_file_name)\n",
    "        if Model == \"PPO\":\n",
    "            torch.save(agent.model_actor.state_dict(), model_save_path+\"/Actor\"+model_file_name)\n",
    "            torch.save(agent.model_critic.state_dict(), model_save_path+\"/Critic\"+model_file_name)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6229fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym-retro environment and make it use discrete\n",
    "    actions for the Sonic game.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ButtonActionWrapper, self).__init__(env)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],\n",
    "                   ['DOWN', 'B'], ['B']]\n",
    "        self._actions = []\n",
    "        for action in actions:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in action:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._actions.append(arr)\n",
    "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
    "\n",
    "    def action(self, a): # pylint: disable=W0221\n",
    "        return self._actions[a].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a2d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, mov_rew=1, score_rew=1, hp_rew=1, ring_rew=0.5, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= self.mov_rew\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4e5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticFrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, n, stickprob):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n = n\n",
    "        self.stickprob = stickprob\n",
    "        self.curac = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.supports_want_render = hasattr(env, \"supports_want_render\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.curac = None\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, ac):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        totrew = 0\n",
    "        for i in range(self.n):\n",
    "            # First step after reset, use action\n",
    "            if self.curac is None:\n",
    "                self.curac = ac\n",
    "            # First substep, delay with probability=stickprob\n",
    "            elif i == 0:\n",
    "                if self.rng.rand() > self.stickprob:\n",
    "                    self.curac = ac\n",
    "            # Second substep, new action definitely kicks in\n",
    "            elif i == 1:\n",
    "                self.curac = ac\n",
    "            if self.supports_want_render and i < self.n - 1:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(\n",
    "                    self.curac,\n",
    "                    want_render=False,\n",
    "                )\n",
    "            else:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(self.curac)\n",
    "            totrew += rew\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        return ob, totrew, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2246aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f3cce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000, epsilon_end=0.01):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model = ConvDQN(input_shape, num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.epsilon_end = epsilon_end\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        transform = T.Lambda(lambda x: x.permute(0,3,1,2).reshape(-1, self.input_shape[1], self.input_shape[2]))\n",
    "        return transform(state)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = self.preprocess_wv(next_state)\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state.unsqueeze(0))).item()\n",
    "            state = self.preprocess_wv(state)\n",
    "            target_f = self.model(state.unsqueeze(0)).to(\"cpu\").detach().numpy()\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(torch.tensor(target_f).to(self.device), self.model(state.unsqueeze(0)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states_tensor)\n",
    "            max_next_q = torch.max(next_q_values, dim=1)[0]\n",
    "        target_q_values = rewards_tensor + self.gamma * max_next_q * (~dones_tensor)\n",
    "        current_q_values = self.model(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.epsilon > 0.05:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d750b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvD3QN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.advance_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        advantages = self.advance_stream(conv_out)\n",
    "        value = self.value_stream(conv_out)\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b6de7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000, update_target_freq=10000, epsilon_end = 0.01):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model_online = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model_online.parameters(), lr=lr)\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.step_counter = 0\n",
    "        self.epsilon_end = epsilon_end\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        transform = T.Lambda(lambda x: x.permute(0,3,1,2).reshape(-1, self.input_shape[1], self.input_shape[2]))\n",
    "        return transform(state)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model_online(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values_online = self.model_online(next_states_tensor)\n",
    "            best_action_online_indices = torch.argmax(next_q_values_online, dim=1).unsqueeze(1)\n",
    "            max_Q_next = self.model_target(next_states_tensor).gather(1, best_action_online_indices).squeeze()\n",
    "        target_q_values = rewards_tensor + self.gamma * max_Q_next * (~dones_tensor)\n",
    "        current_q_values = self.model_online(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.step_counter += 1\n",
    "        self.update_target_network()\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        if self.step_counter % self.update_target_freq == 0:\n",
    "            self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ba8d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPPOActor(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvPPOActor, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        conv_out = self.fc_layers(conv_out)\n",
    "        dist = Categorical(conv_out)\n",
    "        return dist\n",
    "    \n",
    "class ConvPPOCritic(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(ConvPPOCritic, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e59d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPPOAgent:\n",
    "    def __init__(self, env, input_shape, clip=0.2, learning_rate=1e-4, gamma=0.99, n_steps=2048, n_updates_per_iteration=5, entropy_coef=0.01, minibatch_size = 64, max_grad_norm=0.5, lam = 0.95):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.n_steps = n_steps\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_actor = ConvPPOActor(input_shape, self.num_actions).to(self.device)\n",
    "        #self.model_actor.half()\n",
    "        self.model_critic = ConvPPOCritic(input_shape).to(self.device)\n",
    "        #self.model_critic.half()\n",
    "        self.actor_optimizer = optim.Adam(self.model_actor.parameters(), lr=self.lr)\n",
    "        self.critic_optimizer = optim.Adam(self.model_critic.parameters(), lr=self.lr)\n",
    "        self.n_updates = n_updates_per_iteration\n",
    "        self.clip = clip\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.mb_size = minibatch_size\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.lam = lam\n",
    "        self.csv_file = f'../Logs/{Model}/PPO_2.csv'\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        dist = self.model_actor(obs.unsqueeze(0))\n",
    "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.detach().squeeze()\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        # Change from (F, H, W, C) to (F, C, H, W) - if F is frame stack\n",
    "        # or from (H, W, C) to (C, H, W) - if H is frame stack\n",
    "        # For Gym/OpenAI's style where image is (H, W, C) and FrameStack adds a new D0:\n",
    "        # state_tensor: (4, 210, 160, 3) \n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        # This should result in (4, 3, 210, 160) - or (F, C, H, W)\n",
    "        \n",
    "        # If your model (ConvPPOActor/Critic) takes the stacked frames as one channel, \n",
    "        # you need to flatten the F and C dimensions into one: (F*C, H, W)\n",
    "        F, C, H, W = state_tensor.shape\n",
    "        state_tensor = state_tensor.contiguous().view(F * C, H, W) # <<< CORRECT RESHAPE for your CNN\n",
    "        return state_tensor # Returns (C_in, H, W) where C_in is F*C\n",
    "    \n",
    "    def rollout(self):\n",
    "        batch_obs = []      \n",
    "        batch_acts = []            \n",
    "        batch_log_probs = []       \n",
    "        batch_rews = []          \n",
    "        batch_lens = []     \n",
    "        batch_vals = []\n",
    "        batch_dones = []\n",
    "        ep_rews = []\n",
    "        ep_vals = []\n",
    "        ep_dones = []\n",
    "        t = 0 \n",
    "        if not hasattr(self, 'current_obs'):\n",
    "            obs, _ =self.env.reset()\n",
    "            self.current_obs = obs\n",
    "        obs = self.current_obs\n",
    "        with torch.no_grad():\n",
    "            while t < self.n_steps:\n",
    "                t+=1\n",
    "                obs_tensor = self.preprocess_wv(obs)\n",
    "                batch_obs.append(obs_tensor)\n",
    "                action, log_prob = self.get_action(obs_tensor)\n",
    "                val = self.model_critic(obs_tensor.unsqueeze(0)).detach().cpu().item()#.numpy().flatten()[0]\n",
    "                obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                df_log = pd.DataFrame([{'rew': reward,'action':action,'logprob': log_prob}])\n",
    "                if not os.path.exists(self.csv_file):\n",
    "                    df_log.to_csv(self.csv_file, index=False)\n",
    "                else:\n",
    "                    # Append data without writing the header again\n",
    "                    df_log.to_csv(self.csv_file, mode='a', header=False, index=False)\n",
    "                done = terminated or truncated\n",
    "                ep_dones.append(done)\n",
    "                ep_rews.append(reward)\n",
    "                ep_vals.append(val)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                if done:\n",
    "                    batch_lens.append(len(ep_rews))\n",
    "                    batch_rews.append(ep_rews)\n",
    "                    batch_vals.append(ep_vals)\n",
    "                    batch_dones.append(ep_dones)\n",
    "                    obs, _ = self.env.reset()\n",
    "                    ep_rews = []\n",
    "                    ep_vals = []\n",
    "                    ep_dones = []\n",
    "                    self.current_obs = obs\n",
    "        if len(ep_rews) > 0:\n",
    "            batch_lens.append(len(ep_rews))\n",
    "            batch_rews.append(ep_rews)\n",
    "            batch_vals.append(ep_vals)\n",
    "            batch_dones.append(ep_dones)\n",
    "        final_obs_tensor = self.preprocess_wv(obs)\n",
    "        with torch.no_grad():\n",
    "            self.final_val = self.model_critic(final_obs_tensor.unsqueeze(0)).squeeze().cpu().item()\n",
    "\n",
    "        self.current_obs = obs\n",
    "        batch_obs = torch.stack(batch_obs)\n",
    "        #batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.long, device=self.device)\n",
    "        batch_log_probs = torch.stack(batch_log_probs).float()\n",
    "        #batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float32, device=self.device)\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens, batch_vals, batch_dones\n",
    "    \n",
    "    # def compute_rtgs(self, batch_rews):\n",
    "    #     batch_rtgs = []\n",
    "    #     for ep_rews in reversed(batch_rews):\n",
    "    #         discounted_reward = 0 \n",
    "    #         for rew in reversed(ep_rews):\n",
    "    #             discounted_reward = rew + discounted_reward * self.gamma\n",
    "    #             batch_rtgs.insert(0, discounted_reward)\n",
    "    #     batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float, device=self.device)\n",
    "    #     return batch_rtgs\n",
    "\n",
    "    def calculate_gae(self, rewards, values, dones):\n",
    "        \"\"\"\n",
    "        Calculates the Generalized Advantage Estimate (GAE) and the GAE-estimated returns.\n",
    "        Inputs:\n",
    "            rewards: List of lists of rewards (per episode).\n",
    "            values: List of lists of value estimates V(s) (per episode) from the old policy.\n",
    "            dones: List of lists of done flags (per episode).\n",
    "        Outputs:\n",
    "            batch_advantages: Flattened tensor of GAE advantages.\n",
    "            batch_returns: Flattened tensor of GAE-estimated returns.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_advantages = []\n",
    "        batch_returns = []\n",
    "        \n",
    "        for i, (ep_rews, ep_vals, ep_dones) in enumerate(zip(rewards, values, dones)):\n",
    "            advantages = []\n",
    "            returns = []\n",
    "            last_advantage = 0\n",
    "            \n",
    "            # Identify the value for the next state s_{T+1} for bootstrapping\n",
    "            # If the episode is terminated (True 'done' in ep_dones) or this is the final \n",
    "            # segment of the batch AND the last step is 'done', the next value is 0.\n",
    "            # --- SIMPLIFIED AND MORE ROBUST GAE CALCULATION ---\n",
    "\n",
    "            # Initialize the 'next_V' for the last step of the episode/segment\n",
    "            if ep_dones[-1]:\n",
    "                V_next_bootstrap = 0.0\n",
    "            else:\n",
    "                V_next_bootstrap = self.final_val\n",
    "            \n",
    "            # Loop backwards\n",
    "            advantages = []\n",
    "            returns = []\n",
    "            last_advantage = 0\n",
    "            current_V_next = V_next_bootstrap # V(s_{t+1}) for the very last step 't'\n",
    "                \n",
    "            for t in reversed(range(len(ep_rews))):\n",
    "                V_t = ep_vals[t]\n",
    "                R_t = ep_rews[t]\n",
    "                is_terminal = ep_dones[t]\n",
    "            \n",
    "                # V_{t+1} is 0 if terminal, otherwise the bootstrap value\n",
    "                V_t_plus_1 = 0.0 if is_terminal else current_V_next\n",
    "                    \n",
    "                # TD-residual\n",
    "                delta = R_t + self.gamma * V_t_plus_1 - V_t\n",
    "                    \n",
    "                # GAE advantage A_t: A_t = delta_t + gamma * lambda * A_{t+1} * (1 - is_terminal)\n",
    "                advantage = delta + self.gamma * self.lam * (1 - is_terminal) * last_advantage\n",
    "                last_advantage = advantage # For the next step (t-1)\n",
    "                    \n",
    "                # GAE Return R_t (Target for the Critic)\n",
    "                gae_return = advantage + V_t\n",
    "                    \n",
    "                advantages.insert(0, advantage)\n",
    "                returns.insert(0, gae_return)\n",
    "                    \n",
    "                # Update V_{t+1} for the next loop iteration (t-1)\n",
    "                current_V_next = V_t # V(s_t) becomes V(s_{t+1}) for t-1\n",
    "\n",
    "            batch_advantages.extend(advantages)\n",
    "            batch_returns.extend(returns)\n",
    "            \n",
    "        return (torch.tensor(batch_advantages, dtype=torch.float32, device=self.device), \n",
    "                torch.tensor(batch_returns, dtype=torch.float32, device=self.device))\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        V = self.model_critic(batch_obs).view(-1) #.squeeze()\n",
    "        dist = self.model_actor(batch_obs)\n",
    "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        entropy_loss = dist.entropy().mean()\n",
    "        return V, log_probs, entropy_loss\n",
    "    \n",
    "    def learn(self, total_timesteps):\n",
    "        act_t = 0\n",
    "        while act_t < total_timesteps:\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens, batch_vals, batch_dones = self.rollout()\n",
    "            act_t += np.sum(batch_lens)\n",
    "            \n",
    "            # --- MODIFIED: Get both Advantage (A_k) and GAE Returns (batch_rtgs) ---\n",
    "            A_k, batch_rtgs = self.calculate_gae(batch_rews, batch_vals, batch_dones)\n",
    "            # ----------------------------------------------------------------------\n",
    "            \n",
    "            # Normalize Advantages (essential for stable training)\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "            \n",
    "            step = len(batch_obs)\n",
    "            inds = np.arange(step)\n",
    "            print(f\"Iteration {act_t}/{total_timesteps}, collected {np.sum(batch_lens)} steps\")\n",
    "            \n",
    "            # The 'assert' is no longer strictly necessary but good for sanity\n",
    "            # assert len(batch_rtgs) == len(batch_obs), f\"Return mismatch: {len(batch_rtgs)} vs {len(batch_obs)}\"\n",
    "            \n",
    "            for _ in range(self.n_updates):\n",
    "                np.random.shuffle(inds)\n",
    "                for start in range(0, step, self.mb_size):\n",
    "                    end = start + self.mb_size\n",
    "                    idx = inds[start:end]\n",
    "                    \n",
    "                    mini_obs = batch_obs[idx]\n",
    "                    mini_acts = batch_acts[idx]\n",
    "                    mini_log_probs = batch_log_probs[idx]\n",
    "                    mini_advantage = A_k[idx]\n",
    "                    mini_rtgs = batch_rtgs[idx] # Use the calculated GAE Return\n",
    "\n",
    "                    V, curr_log_probs, curr_entropy_loss = self.evaluate(mini_obs, mini_acts)\n",
    "                    \n",
    "                    # PPO Actor Loss\n",
    "                    ratios = torch.exp(curr_log_probs - mini_log_probs)\n",
    "                    surr1 = ratios * mini_advantage\n",
    "                    surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * mini_advantage\n",
    "                    actor_loss = (-torch.min(surr1, surr2)).mean() - self.entropy_coef*curr_entropy_loss\n",
    "                    \n",
    "                    # Critic Loss\n",
    "                    critic_loss = nn.MSELoss()(V, mini_rtgs) # V is trained to predict mini_rtgs\n",
    "                    \n",
    "                    # Update steps... (rest of the learn function is unchanged)\n",
    "                    self.actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.model_actor.parameters(), self.max_grad_norm)\n",
    "                    self.actor_optimizer.step()\n",
    "                    \n",
    "                    self.critic_optimizer.zero_grad() \n",
    "                    critic_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.model_critic.parameters(), self.max_grad_norm)\n",
    "                    self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "348c5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Enviroment to close\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('No Enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e187fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(*, game, state=None, max_episode_steps=4500, **kwargs):\n",
    "    if state is None:\n",
    "        state = retro.State.DEFAULT\n",
    "    env = retro.make(game, state, **kwargs)\n",
    "    env = ButtonActionWrapper(env)\n",
    "    #env = CustomRewardWrapper(env)\n",
    "    env = StochasticFrameSkip(env, n=num_frame_skip, stickprob=0.25)\n",
    "    if RESIZE_ENV:\n",
    "        input_shape = (num_stacked_frames*3, *new_size)\n",
    "        env = ResizeObservation(env, new_size)\n",
    "    else:\n",
    "        input_shape = (num_stacked_frames*3, 224, 320)\n",
    "    if max_episode_steps is not None:\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "    return env, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37280243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Iteration 2048/1800000, collected 2048 steps\n",
      "Iteration 4096/1800000, collected 2048 steps\n",
      "Iteration 6144/1800000, collected 2048 steps\n",
      "Iteration 8192/1800000, collected 2048 steps\n",
      "Iteration 10240/1800000, collected 2048 steps\n",
      "Iteration 12288/1800000, collected 2048 steps\n",
      "Iteration 14336/1800000, collected 2048 steps\n",
      "Iteration 16384/1800000, collected 2048 steps\n",
      "Iteration 18432/1800000, collected 2048 steps\n",
      "Iteration 20480/1800000, collected 2048 steps\n",
      "Iteration 22528/1800000, collected 2048 steps\n",
      "Iteration 24576/1800000, collected 2048 steps\n",
      "Iteration 26624/1800000, collected 2048 steps\n",
      "Iteration 28672/1800000, collected 2048 steps\n",
      "Iteration 30720/1800000, collected 2048 steps\n",
      "Iteration 32768/1800000, collected 2048 steps\n",
      "Iteration 34816/1800000, collected 2048 steps\n",
      "Iteration 36864/1800000, collected 2048 steps\n",
      "Iteration 38912/1800000, collected 2048 steps\n",
      "Iteration 40960/1800000, collected 2048 steps\n",
      "Iteration 43008/1800000, collected 2048 steps\n",
      "Iteration 45056/1800000, collected 2048 steps\n",
      "Iteration 47104/1800000, collected 2048 steps\n",
      "Iteration 49152/1800000, collected 2048 steps\n",
      "Iteration 51200/1800000, collected 2048 steps\n",
      "Iteration 53248/1800000, collected 2048 steps\n",
      "Iteration 55296/1800000, collected 2048 steps\n",
      "Iteration 57344/1800000, collected 2048 steps\n",
      "Iteration 59392/1800000, collected 2048 steps\n",
      "Iteration 61440/1800000, collected 2048 steps\n",
      "Iteration 63488/1800000, collected 2048 steps\n",
      "Iteration 65536/1800000, collected 2048 steps\n",
      "Iteration 67584/1800000, collected 2048 steps\n",
      "Iteration 69632/1800000, collected 2048 steps\n",
      "Iteration 71680/1800000, collected 2048 steps\n",
      "Iteration 73728/1800000, collected 2048 steps\n",
      "Iteration 75776/1800000, collected 2048 steps\n",
      "Iteration 77824/1800000, collected 2048 steps\n",
      "Iteration 79872/1800000, collected 2048 steps\n",
      "Iteration 81920/1800000, collected 2048 steps\n",
      "Iteration 83968/1800000, collected 2048 steps\n",
      "Iteration 86016/1800000, collected 2048 steps\n",
      "Iteration 88064/1800000, collected 2048 steps\n",
      "Iteration 90112/1800000, collected 2048 steps\n",
      "Iteration 92160/1800000, collected 2048 steps\n",
      "Iteration 94208/1800000, collected 2048 steps\n",
      "Iteration 96256/1800000, collected 2048 steps\n",
      "Iteration 98304/1800000, collected 2048 steps\n",
      "Iteration 100352/1800000, collected 2048 steps\n",
      "Iteration 102400/1800000, collected 2048 steps\n",
      "Iteration 104448/1800000, collected 2048 steps\n",
      "Iteration 106496/1800000, collected 2048 steps\n",
      "Iteration 108544/1800000, collected 2048 steps\n",
      "Iteration 110592/1800000, collected 2048 steps\n",
      "Iteration 112640/1800000, collected 2048 steps\n",
      "Iteration 114688/1800000, collected 2048 steps\n",
      "Iteration 116736/1800000, collected 2048 steps\n",
      "Iteration 118784/1800000, collected 2048 steps\n",
      "Iteration 120832/1800000, collected 2048 steps\n",
      "Iteration 122880/1800000, collected 2048 steps\n",
      "Iteration 124928/1800000, collected 2048 steps\n",
      "Iteration 126976/1800000, collected 2048 steps\n",
      "Iteration 129024/1800000, collected 2048 steps\n",
      "Iteration 131072/1800000, collected 2048 steps\n",
      "Iteration 133120/1800000, collected 2048 steps\n",
      "Iteration 135168/1800000, collected 2048 steps\n",
      "Iteration 137216/1800000, collected 2048 steps\n",
      "Iteration 139264/1800000, collected 2048 steps\n",
      "Iteration 141312/1800000, collected 2048 steps\n",
      "Iteration 143360/1800000, collected 2048 steps\n",
      "Iteration 145408/1800000, collected 2048 steps\n",
      "Iteration 147456/1800000, collected 2048 steps\n",
      "Iteration 149504/1800000, collected 2048 steps\n",
      "Iteration 151552/1800000, collected 2048 steps\n",
      "Iteration 153600/1800000, collected 2048 steps\n",
      "Iteration 155648/1800000, collected 2048 steps\n",
      "Iteration 157696/1800000, collected 2048 steps\n",
      "Iteration 159744/1800000, collected 2048 steps\n",
      "Iteration 161792/1800000, collected 2048 steps\n",
      "Iteration 163840/1800000, collected 2048 steps\n",
      "Iteration 165888/1800000, collected 2048 steps\n",
      "Iteration 167936/1800000, collected 2048 steps\n",
      "Iteration 169984/1800000, collected 2048 steps\n",
      "Iteration 172032/1800000, collected 2048 steps\n",
      "Iteration 174080/1800000, collected 2048 steps\n",
      "Iteration 176128/1800000, collected 2048 steps\n",
      "Iteration 178176/1800000, collected 2048 steps\n",
      "Iteration 180224/1800000, collected 2048 steps\n",
      "Iteration 182272/1800000, collected 2048 steps\n",
      "Iteration 184320/1800000, collected 2048 steps\n",
      "Iteration 186368/1800000, collected 2048 steps\n",
      "Iteration 188416/1800000, collected 2048 steps\n",
      "Iteration 190464/1800000, collected 2048 steps\n",
      "Iteration 192512/1800000, collected 2048 steps\n",
      "Iteration 194560/1800000, collected 2048 steps\n",
      "Iteration 196608/1800000, collected 2048 steps\n",
      "Iteration 198656/1800000, collected 2048 steps\n",
      "Iteration 200704/1800000, collected 2048 steps\n",
      "Iteration 202752/1800000, collected 2048 steps\n",
      "Iteration 204800/1800000, collected 2048 steps\n",
      "Iteration 206848/1800000, collected 2048 steps\n",
      "Iteration 208896/1800000, collected 2048 steps\n",
      "Iteration 210944/1800000, collected 2048 steps\n",
      "Iteration 212992/1800000, collected 2048 steps\n",
      "Iteration 215040/1800000, collected 2048 steps\n",
      "Iteration 217088/1800000, collected 2048 steps\n",
      "Iteration 219136/1800000, collected 2048 steps\n",
      "Iteration 221184/1800000, collected 2048 steps\n",
      "Iteration 223232/1800000, collected 2048 steps\n",
      "Iteration 225280/1800000, collected 2048 steps\n",
      "Iteration 227328/1800000, collected 2048 steps\n",
      "Iteration 229376/1800000, collected 2048 steps\n",
      "Iteration 231424/1800000, collected 2048 steps\n",
      "Iteration 233472/1800000, collected 2048 steps\n",
      "Iteration 235520/1800000, collected 2048 steps\n",
      "Iteration 237568/1800000, collected 2048 steps\n",
      "Iteration 239616/1800000, collected 2048 steps\n",
      "Iteration 241664/1800000, collected 2048 steps\n",
      "Iteration 243712/1800000, collected 2048 steps\n",
      "Iteration 245760/1800000, collected 2048 steps\n",
      "Iteration 247808/1800000, collected 2048 steps\n",
      "Iteration 249856/1800000, collected 2048 steps\n",
      "Iteration 251904/1800000, collected 2048 steps\n",
      "Iteration 253952/1800000, collected 2048 steps\n",
      "Iteration 256000/1800000, collected 2048 steps\n",
      "Iteration 258048/1800000, collected 2048 steps\n",
      "Iteration 260096/1800000, collected 2048 steps\n",
      "Iteration 262144/1800000, collected 2048 steps\n",
      "Iteration 264192/1800000, collected 2048 steps\n",
      "Iteration 266240/1800000, collected 2048 steps\n",
      "Iteration 268288/1800000, collected 2048 steps\n",
      "Iteration 270336/1800000, collected 2048 steps\n",
      "Iteration 272384/1800000, collected 2048 steps\n",
      "Iteration 274432/1800000, collected 2048 steps\n",
      "Iteration 276480/1800000, collected 2048 steps\n",
      "Iteration 278528/1800000, collected 2048 steps\n",
      "Iteration 280576/1800000, collected 2048 steps\n",
      "Iteration 282624/1800000, collected 2048 steps\n",
      "Iteration 284672/1800000, collected 2048 steps\n",
      "Iteration 286720/1800000, collected 2048 steps\n",
      "Iteration 288768/1800000, collected 2048 steps\n",
      "Iteration 290816/1800000, collected 2048 steps\n",
      "Iteration 292864/1800000, collected 2048 steps\n",
      "Iteration 294912/1800000, collected 2048 steps\n",
      "Iteration 296960/1800000, collected 2048 steps\n",
      "Iteration 299008/1800000, collected 2048 steps\n",
      "Iteration 301056/1800000, collected 2048 steps\n",
      "Iteration 303104/1800000, collected 2048 steps\n",
      "Iteration 305152/1800000, collected 2048 steps\n",
      "Iteration 307200/1800000, collected 2048 steps\n",
      "Iteration 309248/1800000, collected 2048 steps\n",
      "Iteration 311296/1800000, collected 2048 steps\n",
      "Iteration 313344/1800000, collected 2048 steps\n",
      "Iteration 315392/1800000, collected 2048 steps\n",
      "Iteration 317440/1800000, collected 2048 steps\n",
      "Iteration 319488/1800000, collected 2048 steps\n",
      "Iteration 321536/1800000, collected 2048 steps\n",
      "Iteration 323584/1800000, collected 2048 steps\n",
      "Iteration 325632/1800000, collected 2048 steps\n",
      "Iteration 327680/1800000, collected 2048 steps\n",
      "Iteration 329728/1800000, collected 2048 steps\n",
      "Iteration 331776/1800000, collected 2048 steps\n",
      "Iteration 333824/1800000, collected 2048 steps\n",
      "Iteration 335872/1800000, collected 2048 steps\n",
      "Iteration 337920/1800000, collected 2048 steps\n",
      "Iteration 339968/1800000, collected 2048 steps\n",
      "Iteration 342016/1800000, collected 2048 steps\n",
      "Iteration 344064/1800000, collected 2048 steps\n",
      "Iteration 346112/1800000, collected 2048 steps\n",
      "Iteration 348160/1800000, collected 2048 steps\n",
      "Iteration 350208/1800000, collected 2048 steps\n",
      "Iteration 352256/1800000, collected 2048 steps\n",
      "Iteration 354304/1800000, collected 2048 steps\n",
      "Iteration 356352/1800000, collected 2048 steps\n",
      "Iteration 358400/1800000, collected 2048 steps\n",
      "Iteration 360448/1800000, collected 2048 steps\n",
      "Iteration 362496/1800000, collected 2048 steps\n",
      "Iteration 364544/1800000, collected 2048 steps\n",
      "Iteration 366592/1800000, collected 2048 steps\n",
      "Iteration 368640/1800000, collected 2048 steps\n",
      "Iteration 370688/1800000, collected 2048 steps\n",
      "Iteration 372736/1800000, collected 2048 steps\n",
      "Iteration 374784/1800000, collected 2048 steps\n",
      "Iteration 376832/1800000, collected 2048 steps\n",
      "Iteration 378880/1800000, collected 2048 steps\n",
      "Iteration 380928/1800000, collected 2048 steps\n",
      "Iteration 382976/1800000, collected 2048 steps\n",
      "Iteration 385024/1800000, collected 2048 steps\n",
      "Iteration 387072/1800000, collected 2048 steps\n",
      "Iteration 389120/1800000, collected 2048 steps\n",
      "Iteration 391168/1800000, collected 2048 steps\n",
      "Iteration 393216/1800000, collected 2048 steps\n",
      "Iteration 395264/1800000, collected 2048 steps\n",
      "Iteration 397312/1800000, collected 2048 steps\n",
      "Iteration 399360/1800000, collected 2048 steps\n",
      "Iteration 401408/1800000, collected 2048 steps\n",
      "Iteration 403456/1800000, collected 2048 steps\n",
      "Iteration 405504/1800000, collected 2048 steps\n",
      "Iteration 407552/1800000, collected 2048 steps\n",
      "Iteration 409600/1800000, collected 2048 steps\n",
      "Iteration 411648/1800000, collected 2048 steps\n",
      "Iteration 413696/1800000, collected 2048 steps\n",
      "Iteration 415744/1800000, collected 2048 steps\n",
      "Iteration 417792/1800000, collected 2048 steps\n",
      "Iteration 419840/1800000, collected 2048 steps\n",
      "Iteration 421888/1800000, collected 2048 steps\n",
      "Iteration 423936/1800000, collected 2048 steps\n",
      "Iteration 425984/1800000, collected 2048 steps\n",
      "Iteration 428032/1800000, collected 2048 steps\n",
      "Iteration 430080/1800000, collected 2048 steps\n",
      "Iteration 432128/1800000, collected 2048 steps\n",
      "Iteration 434176/1800000, collected 2048 steps\n",
      "Iteration 436224/1800000, collected 2048 steps\n",
      "Iteration 438272/1800000, collected 2048 steps\n",
      "Iteration 440320/1800000, collected 2048 steps\n",
      "Iteration 442368/1800000, collected 2048 steps\n",
      "Iteration 444416/1800000, collected 2048 steps\n",
      "Iteration 446464/1800000, collected 2048 steps\n",
      "Iteration 448512/1800000, collected 2048 steps\n",
      "Iteration 450560/1800000, collected 2048 steps\n",
      "Iteration 452608/1800000, collected 2048 steps\n",
      "Iteration 454656/1800000, collected 2048 steps\n",
      "Iteration 456704/1800000, collected 2048 steps\n",
      "Iteration 458752/1800000, collected 2048 steps\n",
      "Iteration 460800/1800000, collected 2048 steps\n",
      "Iteration 462848/1800000, collected 2048 steps\n",
      "Iteration 464896/1800000, collected 2048 steps\n",
      "Iteration 466944/1800000, collected 2048 steps\n",
      "Iteration 468992/1800000, collected 2048 steps\n",
      "Iteration 471040/1800000, collected 2048 steps\n",
      "Iteration 473088/1800000, collected 2048 steps\n",
      "Iteration 475136/1800000, collected 2048 steps\n",
      "Iteration 477184/1800000, collected 2048 steps\n",
      "Iteration 479232/1800000, collected 2048 steps\n",
      "Iteration 481280/1800000, collected 2048 steps\n",
      "Iteration 483328/1800000, collected 2048 steps\n",
      "Iteration 485376/1800000, collected 2048 steps\n",
      "Iteration 487424/1800000, collected 2048 steps\n",
      "Iteration 489472/1800000, collected 2048 steps\n",
      "Iteration 491520/1800000, collected 2048 steps\n",
      "Iteration 493568/1800000, collected 2048 steps\n",
      "Iteration 495616/1800000, collected 2048 steps\n",
      "Iteration 497664/1800000, collected 2048 steps\n",
      "Iteration 499712/1800000, collected 2048 steps\n",
      "Iteration 501760/1800000, collected 2048 steps\n",
      "Iteration 503808/1800000, collected 2048 steps\n",
      "Iteration 505856/1800000, collected 2048 steps\n",
      "Iteration 507904/1800000, collected 2048 steps\n",
      "Iteration 509952/1800000, collected 2048 steps\n",
      "Iteration 512000/1800000, collected 2048 steps\n",
      "Iteration 514048/1800000, collected 2048 steps\n",
      "Iteration 516096/1800000, collected 2048 steps\n",
      "Iteration 518144/1800000, collected 2048 steps\n",
      "Iteration 520192/1800000, collected 2048 steps\n",
      "Iteration 522240/1800000, collected 2048 steps\n",
      "Iteration 524288/1800000, collected 2048 steps\n",
      "Iteration 526336/1800000, collected 2048 steps\n",
      "Iteration 528384/1800000, collected 2048 steps\n",
      "Iteration 530432/1800000, collected 2048 steps\n",
      "Iteration 532480/1800000, collected 2048 steps\n",
      "Iteration 534528/1800000, collected 2048 steps\n",
      "Iteration 536576/1800000, collected 2048 steps\n",
      "Iteration 538624/1800000, collected 2048 steps\n",
      "Iteration 540672/1800000, collected 2048 steps\n",
      "Iteration 542720/1800000, collected 2048 steps\n",
      "Iteration 544768/1800000, collected 2048 steps\n",
      "Iteration 546816/1800000, collected 2048 steps\n",
      "Iteration 548864/1800000, collected 2048 steps\n",
      "Iteration 550912/1800000, collected 2048 steps\n",
      "Iteration 552960/1800000, collected 2048 steps\n",
      "Iteration 555008/1800000, collected 2048 steps\n",
      "Iteration 557056/1800000, collected 2048 steps\n",
      "Iteration 559104/1800000, collected 2048 steps\n",
      "Iteration 561152/1800000, collected 2048 steps\n",
      "Iteration 563200/1800000, collected 2048 steps\n",
      "Iteration 565248/1800000, collected 2048 steps\n",
      "Iteration 567296/1800000, collected 2048 steps\n",
      "Iteration 569344/1800000, collected 2048 steps\n",
      "Iteration 571392/1800000, collected 2048 steps\n",
      "Iteration 573440/1800000, collected 2048 steps\n",
      "Iteration 575488/1800000, collected 2048 steps\n",
      "Iteration 577536/1800000, collected 2048 steps\n",
      "Iteration 579584/1800000, collected 2048 steps\n",
      "Iteration 581632/1800000, collected 2048 steps\n",
      "Iteration 583680/1800000, collected 2048 steps\n",
      "Iteration 585728/1800000, collected 2048 steps\n",
      "Iteration 587776/1800000, collected 2048 steps\n",
      "Iteration 589824/1800000, collected 2048 steps\n",
      "Iteration 591872/1800000, collected 2048 steps\n",
      "Iteration 593920/1800000, collected 2048 steps\n",
      "Iteration 595968/1800000, collected 2048 steps\n",
      "Iteration 598016/1800000, collected 2048 steps\n",
      "Iteration 600064/1800000, collected 2048 steps\n",
      "Iteration 602112/1800000, collected 2048 steps\n",
      "Iteration 604160/1800000, collected 2048 steps\n",
      "Iteration 606208/1800000, collected 2048 steps\n",
      "Iteration 608256/1800000, collected 2048 steps\n",
      "Iteration 610304/1800000, collected 2048 steps\n",
      "Iteration 612352/1800000, collected 2048 steps\n",
      "Iteration 614400/1800000, collected 2048 steps\n",
      "Iteration 616448/1800000, collected 2048 steps\n",
      "Iteration 618496/1800000, collected 2048 steps\n",
      "Iteration 620544/1800000, collected 2048 steps\n",
      "Iteration 622592/1800000, collected 2048 steps\n",
      "Iteration 624640/1800000, collected 2048 steps\n",
      "Iteration 626688/1800000, collected 2048 steps\n",
      "Iteration 628736/1800000, collected 2048 steps\n",
      "Iteration 630784/1800000, collected 2048 steps\n",
      "Iteration 632832/1800000, collected 2048 steps\n",
      "Iteration 634880/1800000, collected 2048 steps\n",
      "Iteration 636928/1800000, collected 2048 steps\n",
      "Iteration 638976/1800000, collected 2048 steps\n",
      "Iteration 641024/1800000, collected 2048 steps\n",
      "Iteration 643072/1800000, collected 2048 steps\n",
      "Iteration 645120/1800000, collected 2048 steps\n",
      "Iteration 647168/1800000, collected 2048 steps\n",
      "Iteration 649216/1800000, collected 2048 steps\n",
      "Iteration 651264/1800000, collected 2048 steps\n",
      "Iteration 653312/1800000, collected 2048 steps\n",
      "Iteration 655360/1800000, collected 2048 steps\n",
      "Iteration 657408/1800000, collected 2048 steps\n",
      "Iteration 659456/1800000, collected 2048 steps\n",
      "Iteration 661504/1800000, collected 2048 steps\n",
      "Iteration 663552/1800000, collected 2048 steps\n",
      "Iteration 665600/1800000, collected 2048 steps\n",
      "Iteration 667648/1800000, collected 2048 steps\n",
      "Iteration 669696/1800000, collected 2048 steps\n",
      "Iteration 671744/1800000, collected 2048 steps\n",
      "Iteration 673792/1800000, collected 2048 steps\n",
      "Iteration 675840/1800000, collected 2048 steps\n",
      "Iteration 677888/1800000, collected 2048 steps\n",
      "Iteration 679936/1800000, collected 2048 steps\n",
      "Iteration 681984/1800000, collected 2048 steps\n",
      "Iteration 684032/1800000, collected 2048 steps\n",
      "Iteration 686080/1800000, collected 2048 steps\n",
      "Iteration 688128/1800000, collected 2048 steps\n",
      "Iteration 690176/1800000, collected 2048 steps\n",
      "Iteration 692224/1800000, collected 2048 steps\n",
      "Iteration 694272/1800000, collected 2048 steps\n",
      "Iteration 696320/1800000, collected 2048 steps\n",
      "Iteration 698368/1800000, collected 2048 steps\n",
      "Iteration 700416/1800000, collected 2048 steps\n",
      "Iteration 702464/1800000, collected 2048 steps\n",
      "Iteration 704512/1800000, collected 2048 steps\n",
      "Iteration 706560/1800000, collected 2048 steps\n",
      "Iteration 708608/1800000, collected 2048 steps\n",
      "Iteration 710656/1800000, collected 2048 steps\n",
      "Iteration 712704/1800000, collected 2048 steps\n",
      "Iteration 714752/1800000, collected 2048 steps\n",
      "Iteration 716800/1800000, collected 2048 steps\n",
      "Iteration 718848/1800000, collected 2048 steps\n",
      "Iteration 720896/1800000, collected 2048 steps\n",
      "Iteration 722944/1800000, collected 2048 steps\n",
      "Iteration 724992/1800000, collected 2048 steps\n",
      "Iteration 727040/1800000, collected 2048 steps\n",
      "Iteration 729088/1800000, collected 2048 steps\n",
      "Iteration 731136/1800000, collected 2048 steps\n",
      "Iteration 733184/1800000, collected 2048 steps\n",
      "Iteration 735232/1800000, collected 2048 steps\n",
      "Iteration 737280/1800000, collected 2048 steps\n",
      "Iteration 739328/1800000, collected 2048 steps\n",
      "Iteration 741376/1800000, collected 2048 steps\n",
      "Iteration 743424/1800000, collected 2048 steps\n",
      "Iteration 745472/1800000, collected 2048 steps\n",
      "Iteration 747520/1800000, collected 2048 steps\n",
      "Iteration 749568/1800000, collected 2048 steps\n",
      "Iteration 751616/1800000, collected 2048 steps\n",
      "Iteration 753664/1800000, collected 2048 steps\n",
      "Iteration 755712/1800000, collected 2048 steps\n",
      "Iteration 757760/1800000, collected 2048 steps\n",
      "Iteration 759808/1800000, collected 2048 steps\n",
      "Iteration 761856/1800000, collected 2048 steps\n",
      "Iteration 763904/1800000, collected 2048 steps\n",
      "Iteration 765952/1800000, collected 2048 steps\n",
      "Iteration 768000/1800000, collected 2048 steps\n",
      "Iteration 770048/1800000, collected 2048 steps\n",
      "Iteration 772096/1800000, collected 2048 steps\n",
      "Iteration 774144/1800000, collected 2048 steps\n",
      "Iteration 776192/1800000, collected 2048 steps\n",
      "Iteration 778240/1800000, collected 2048 steps\n",
      "Iteration 780288/1800000, collected 2048 steps\n",
      "Iteration 782336/1800000, collected 2048 steps\n",
      "Iteration 784384/1800000, collected 2048 steps\n",
      "Iteration 786432/1800000, collected 2048 steps\n",
      "Iteration 788480/1800000, collected 2048 steps\n",
      "Iteration 790528/1800000, collected 2048 steps\n",
      "Iteration 792576/1800000, collected 2048 steps\n",
      "Iteration 794624/1800000, collected 2048 steps\n",
      "Iteration 796672/1800000, collected 2048 steps\n",
      "Iteration 798720/1800000, collected 2048 steps\n",
      "Iteration 800768/1800000, collected 2048 steps\n",
      "Iteration 802816/1800000, collected 2048 steps\n",
      "Iteration 804864/1800000, collected 2048 steps\n",
      "Iteration 806912/1800000, collected 2048 steps\n",
      "Iteration 808960/1800000, collected 2048 steps\n",
      "Iteration 811008/1800000, collected 2048 steps\n",
      "Iteration 813056/1800000, collected 2048 steps\n",
      "Iteration 815104/1800000, collected 2048 steps\n",
      "Iteration 817152/1800000, collected 2048 steps\n",
      "Iteration 819200/1800000, collected 2048 steps\n",
      "Iteration 821248/1800000, collected 2048 steps\n",
      "Iteration 823296/1800000, collected 2048 steps\n",
      "Iteration 825344/1800000, collected 2048 steps\n",
      "Iteration 827392/1800000, collected 2048 steps\n",
      "Iteration 829440/1800000, collected 2048 steps\n",
      "Iteration 831488/1800000, collected 2048 steps\n",
      "Iteration 833536/1800000, collected 2048 steps\n",
      "Iteration 835584/1800000, collected 2048 steps\n",
      "Iteration 837632/1800000, collected 2048 steps\n",
      "Iteration 839680/1800000, collected 2048 steps\n",
      "Iteration 841728/1800000, collected 2048 steps\n",
      "Iteration 843776/1800000, collected 2048 steps\n",
      "Iteration 845824/1800000, collected 2048 steps\n",
      "Iteration 847872/1800000, collected 2048 steps\n",
      "Iteration 849920/1800000, collected 2048 steps\n",
      "Iteration 851968/1800000, collected 2048 steps\n",
      "Iteration 854016/1800000, collected 2048 steps\n",
      "Iteration 856064/1800000, collected 2048 steps\n",
      "Iteration 858112/1800000, collected 2048 steps\n",
      "Iteration 860160/1800000, collected 2048 steps\n",
      "Iteration 862208/1800000, collected 2048 steps\n",
      "Iteration 864256/1800000, collected 2048 steps\n",
      "Iteration 866304/1800000, collected 2048 steps\n",
      "Iteration 868352/1800000, collected 2048 steps\n",
      "Iteration 870400/1800000, collected 2048 steps\n",
      "Iteration 872448/1800000, collected 2048 steps\n",
      "Iteration 874496/1800000, collected 2048 steps\n",
      "Iteration 876544/1800000, collected 2048 steps\n",
      "Iteration 878592/1800000, collected 2048 steps\n",
      "Iteration 880640/1800000, collected 2048 steps\n",
      "Iteration 882688/1800000, collected 2048 steps\n",
      "Iteration 884736/1800000, collected 2048 steps\n",
      "Iteration 886784/1800000, collected 2048 steps\n",
      "Iteration 888832/1800000, collected 2048 steps\n",
      "Iteration 890880/1800000, collected 2048 steps\n",
      "Iteration 892928/1800000, collected 2048 steps\n",
      "Iteration 894976/1800000, collected 2048 steps\n",
      "Iteration 897024/1800000, collected 2048 steps\n",
      "Iteration 899072/1800000, collected 2048 steps\n",
      "Iteration 901120/1800000, collected 2048 steps\n",
      "Iteration 903168/1800000, collected 2048 steps\n",
      "Iteration 905216/1800000, collected 2048 steps\n",
      "Iteration 907264/1800000, collected 2048 steps\n",
      "Iteration 909312/1800000, collected 2048 steps\n",
      "Iteration 911360/1800000, collected 2048 steps\n",
      "Iteration 913408/1800000, collected 2048 steps\n",
      "Iteration 915456/1800000, collected 2048 steps\n",
      "Iteration 917504/1800000, collected 2048 steps\n",
      "Iteration 919552/1800000, collected 2048 steps\n",
      "Iteration 921600/1800000, collected 2048 steps\n",
      "Iteration 923648/1800000, collected 2048 steps\n",
      "Iteration 925696/1800000, collected 2048 steps\n",
      "Iteration 927744/1800000, collected 2048 steps\n",
      "Iteration 929792/1800000, collected 2048 steps\n",
      "Iteration 931840/1800000, collected 2048 steps\n",
      "Iteration 933888/1800000, collected 2048 steps\n",
      "Iteration 935936/1800000, collected 2048 steps\n",
      "Iteration 937984/1800000, collected 2048 steps\n",
      "Iteration 940032/1800000, collected 2048 steps\n",
      "Iteration 942080/1800000, collected 2048 steps\n",
      "Iteration 944128/1800000, collected 2048 steps\n",
      "Iteration 946176/1800000, collected 2048 steps\n",
      "Iteration 948224/1800000, collected 2048 steps\n",
      "Iteration 950272/1800000, collected 2048 steps\n",
      "Iteration 952320/1800000, collected 2048 steps\n",
      "Iteration 954368/1800000, collected 2048 steps\n",
      "Iteration 956416/1800000, collected 2048 steps\n",
      "Iteration 958464/1800000, collected 2048 steps\n",
      "Iteration 960512/1800000, collected 2048 steps\n",
      "Iteration 962560/1800000, collected 2048 steps\n",
      "Iteration 964608/1800000, collected 2048 steps\n",
      "Iteration 966656/1800000, collected 2048 steps\n",
      "Iteration 968704/1800000, collected 2048 steps\n",
      "Iteration 970752/1800000, collected 2048 steps\n",
      "Iteration 972800/1800000, collected 2048 steps\n",
      "Iteration 974848/1800000, collected 2048 steps\n",
      "Iteration 976896/1800000, collected 2048 steps\n",
      "Iteration 978944/1800000, collected 2048 steps\n",
      "Iteration 980992/1800000, collected 2048 steps\n",
      "Iteration 983040/1800000, collected 2048 steps\n",
      "Iteration 985088/1800000, collected 2048 steps\n",
      "Iteration 987136/1800000, collected 2048 steps\n",
      "Iteration 989184/1800000, collected 2048 steps\n",
      "Iteration 991232/1800000, collected 2048 steps\n",
      "Iteration 993280/1800000, collected 2048 steps\n",
      "Iteration 995328/1800000, collected 2048 steps\n",
      "Iteration 997376/1800000, collected 2048 steps\n",
      "Iteration 999424/1800000, collected 2048 steps\n",
      "Iteration 1001472/1800000, collected 2048 steps\n",
      "Iteration 1003520/1800000, collected 2048 steps\n",
      "Iteration 1005568/1800000, collected 2048 steps\n",
      "Iteration 1007616/1800000, collected 2048 steps\n",
      "Iteration 1009664/1800000, collected 2048 steps\n",
      "Iteration 1011712/1800000, collected 2048 steps\n",
      "Iteration 1013760/1800000, collected 2048 steps\n",
      "Iteration 1015808/1800000, collected 2048 steps\n",
      "Iteration 1017856/1800000, collected 2048 steps\n",
      "Iteration 1019904/1800000, collected 2048 steps\n",
      "Iteration 1021952/1800000, collected 2048 steps\n",
      "Iteration 1024000/1800000, collected 2048 steps\n",
      "Iteration 1026048/1800000, collected 2048 steps\n",
      "Iteration 1028096/1800000, collected 2048 steps\n",
      "Iteration 1030144/1800000, collected 2048 steps\n",
      "Iteration 1032192/1800000, collected 2048 steps\n",
      "Iteration 1034240/1800000, collected 2048 steps\n",
      "Iteration 1036288/1800000, collected 2048 steps\n",
      "Iteration 1038336/1800000, collected 2048 steps\n",
      "Iteration 1040384/1800000, collected 2048 steps\n",
      "Iteration 1042432/1800000, collected 2048 steps\n",
      "Iteration 1044480/1800000, collected 2048 steps\n",
      "Iteration 1046528/1800000, collected 2048 steps\n",
      "Iteration 1048576/1800000, collected 2048 steps\n",
      "Iteration 1050624/1800000, collected 2048 steps\n",
      "Iteration 1052672/1800000, collected 2048 steps\n",
      "Iteration 1054720/1800000, collected 2048 steps\n",
      "Iteration 1056768/1800000, collected 2048 steps\n",
      "Iteration 1058816/1800000, collected 2048 steps\n",
      "Iteration 1060864/1800000, collected 2048 steps\n",
      "Iteration 1062912/1800000, collected 2048 steps\n",
      "Iteration 1064960/1800000, collected 2048 steps\n",
      "Iteration 1067008/1800000, collected 2048 steps\n",
      "Iteration 1069056/1800000, collected 2048 steps\n",
      "Iteration 1071104/1800000, collected 2048 steps\n",
      "Iteration 1073152/1800000, collected 2048 steps\n",
      "Iteration 1075200/1800000, collected 2048 steps\n",
      "Iteration 1077248/1800000, collected 2048 steps\n",
      "Iteration 1079296/1800000, collected 2048 steps\n",
      "Iteration 1081344/1800000, collected 2048 steps\n",
      "Iteration 1083392/1800000, collected 2048 steps\n",
      "Iteration 1085440/1800000, collected 2048 steps\n",
      "Iteration 1087488/1800000, collected 2048 steps\n",
      "Iteration 1089536/1800000, collected 2048 steps\n",
      "Iteration 1091584/1800000, collected 2048 steps\n",
      "Iteration 1093632/1800000, collected 2048 steps\n",
      "Iteration 1095680/1800000, collected 2048 steps\n",
      "Iteration 1097728/1800000, collected 2048 steps\n",
      "Iteration 1099776/1800000, collected 2048 steps\n",
      "Iteration 1101824/1800000, collected 2048 steps\n",
      "Iteration 1103872/1800000, collected 2048 steps\n",
      "Iteration 1105920/1800000, collected 2048 steps\n",
      "Iteration 1107968/1800000, collected 2048 steps\n",
      "Iteration 1110016/1800000, collected 2048 steps\n",
      "Iteration 1112064/1800000, collected 2048 steps\n",
      "Iteration 1114112/1800000, collected 2048 steps\n",
      "Iteration 1116160/1800000, collected 2048 steps\n",
      "Iteration 1118208/1800000, collected 2048 steps\n",
      "Iteration 1120256/1800000, collected 2048 steps\n",
      "Iteration 1122304/1800000, collected 2048 steps\n",
      "Iteration 1124352/1800000, collected 2048 steps\n",
      "Iteration 1126400/1800000, collected 2048 steps\n",
      "Iteration 1128448/1800000, collected 2048 steps\n",
      "Iteration 1130496/1800000, collected 2048 steps\n",
      "Iteration 1132544/1800000, collected 2048 steps\n",
      "Iteration 1134592/1800000, collected 2048 steps\n",
      "Iteration 1136640/1800000, collected 2048 steps\n",
      "Iteration 1138688/1800000, collected 2048 steps\n",
      "Iteration 1140736/1800000, collected 2048 steps\n",
      "Iteration 1142784/1800000, collected 2048 steps\n",
      "Iteration 1144832/1800000, collected 2048 steps\n",
      "Iteration 1146880/1800000, collected 2048 steps\n",
      "Iteration 1148928/1800000, collected 2048 steps\n",
      "Iteration 1150976/1800000, collected 2048 steps\n",
      "Iteration 1153024/1800000, collected 2048 steps\n",
      "Iteration 1155072/1800000, collected 2048 steps\n",
      "Iteration 1157120/1800000, collected 2048 steps\n",
      "Iteration 1159168/1800000, collected 2048 steps\n",
      "Iteration 1161216/1800000, collected 2048 steps\n",
      "Iteration 1163264/1800000, collected 2048 steps\n",
      "Iteration 1165312/1800000, collected 2048 steps\n",
      "Iteration 1167360/1800000, collected 2048 steps\n",
      "Iteration 1169408/1800000, collected 2048 steps\n",
      "Iteration 1171456/1800000, collected 2048 steps\n",
      "Iteration 1173504/1800000, collected 2048 steps\n",
      "Iteration 1175552/1800000, collected 2048 steps\n",
      "Iteration 1177600/1800000, collected 2048 steps\n",
      "Iteration 1179648/1800000, collected 2048 steps\n",
      "Iteration 1181696/1800000, collected 2048 steps\n",
      "Iteration 1183744/1800000, collected 2048 steps\n",
      "Iteration 1185792/1800000, collected 2048 steps\n",
      "Iteration 1187840/1800000, collected 2048 steps\n",
      "Iteration 1189888/1800000, collected 2048 steps\n",
      "Iteration 1191936/1800000, collected 2048 steps\n",
      "Iteration 1193984/1800000, collected 2048 steps\n",
      "Iteration 1196032/1800000, collected 2048 steps\n",
      "Iteration 1198080/1800000, collected 2048 steps\n",
      "Iteration 1200128/1800000, collected 2048 steps\n",
      "Iteration 1202176/1800000, collected 2048 steps\n",
      "Iteration 1204224/1800000, collected 2048 steps\n",
      "Iteration 1206272/1800000, collected 2048 steps\n",
      "Iteration 1208320/1800000, collected 2048 steps\n",
      "Iteration 1210368/1800000, collected 2048 steps\n",
      "Iteration 1212416/1800000, collected 2048 steps\n",
      "Iteration 1214464/1800000, collected 2048 steps\n",
      "Iteration 1216512/1800000, collected 2048 steps\n",
      "Iteration 1218560/1800000, collected 2048 steps\n",
      "Iteration 1220608/1800000, collected 2048 steps\n",
      "Iteration 1222656/1800000, collected 2048 steps\n",
      "Iteration 1224704/1800000, collected 2048 steps\n",
      "Iteration 1226752/1800000, collected 2048 steps\n",
      "Iteration 1228800/1800000, collected 2048 steps\n",
      "Iteration 1230848/1800000, collected 2048 steps\n",
      "Iteration 1232896/1800000, collected 2048 steps\n",
      "Iteration 1234944/1800000, collected 2048 steps\n",
      "Iteration 1236992/1800000, collected 2048 steps\n",
      "Iteration 1239040/1800000, collected 2048 steps\n",
      "Iteration 1241088/1800000, collected 2048 steps\n",
      "Iteration 1243136/1800000, collected 2048 steps\n",
      "Iteration 1245184/1800000, collected 2048 steps\n",
      "Iteration 1247232/1800000, collected 2048 steps\n",
      "Iteration 1249280/1800000, collected 2048 steps\n",
      "Iteration 1251328/1800000, collected 2048 steps\n",
      "Iteration 1253376/1800000, collected 2048 steps\n",
      "Iteration 1255424/1800000, collected 2048 steps\n",
      "Iteration 1257472/1800000, collected 2048 steps\n",
      "Iteration 1259520/1800000, collected 2048 steps\n",
      "Iteration 1261568/1800000, collected 2048 steps\n",
      "Iteration 1263616/1800000, collected 2048 steps\n",
      "Iteration 1265664/1800000, collected 2048 steps\n",
      "Iteration 1267712/1800000, collected 2048 steps\n",
      "Iteration 1269760/1800000, collected 2048 steps\n",
      "Iteration 1271808/1800000, collected 2048 steps\n",
      "Iteration 1273856/1800000, collected 2048 steps\n",
      "Iteration 1275904/1800000, collected 2048 steps\n",
      "Iteration 1277952/1800000, collected 2048 steps\n",
      "Iteration 1280000/1800000, collected 2048 steps\n",
      "Iteration 1282048/1800000, collected 2048 steps\n",
      "Iteration 1284096/1800000, collected 2048 steps\n",
      "Iteration 1286144/1800000, collected 2048 steps\n",
      "Iteration 1288192/1800000, collected 2048 steps\n",
      "Iteration 1290240/1800000, collected 2048 steps\n",
      "Iteration 1292288/1800000, collected 2048 steps\n",
      "Iteration 1294336/1800000, collected 2048 steps\n",
      "Iteration 1296384/1800000, collected 2048 steps\n",
      "Iteration 1298432/1800000, collected 2048 steps\n",
      "Iteration 1300480/1800000, collected 2048 steps\n",
      "Iteration 1302528/1800000, collected 2048 steps\n",
      "Iteration 1304576/1800000, collected 2048 steps\n",
      "Iteration 1306624/1800000, collected 2048 steps\n",
      "Iteration 1308672/1800000, collected 2048 steps\n",
      "Iteration 1310720/1800000, collected 2048 steps\n",
      "Iteration 1312768/1800000, collected 2048 steps\n",
      "Iteration 1314816/1800000, collected 2048 steps\n",
      "Iteration 1316864/1800000, collected 2048 steps\n",
      "Iteration 1318912/1800000, collected 2048 steps\n",
      "Iteration 1320960/1800000, collected 2048 steps\n",
      "Iteration 1323008/1800000, collected 2048 steps\n",
      "Iteration 1325056/1800000, collected 2048 steps\n",
      "Iteration 1327104/1800000, collected 2048 steps\n",
      "Iteration 1329152/1800000, collected 2048 steps\n",
      "Iteration 1331200/1800000, collected 2048 steps\n",
      "Iteration 1333248/1800000, collected 2048 steps\n",
      "Iteration 1335296/1800000, collected 2048 steps\n",
      "Iteration 1337344/1800000, collected 2048 steps\n",
      "Iteration 1339392/1800000, collected 2048 steps\n",
      "Iteration 1341440/1800000, collected 2048 steps\n",
      "Iteration 1343488/1800000, collected 2048 steps\n",
      "Iteration 1345536/1800000, collected 2048 steps\n",
      "Iteration 1347584/1800000, collected 2048 steps\n",
      "Iteration 1349632/1800000, collected 2048 steps\n",
      "Iteration 1351680/1800000, collected 2048 steps\n",
      "Iteration 1353728/1800000, collected 2048 steps\n",
      "Iteration 1355776/1800000, collected 2048 steps\n",
      "Iteration 1357824/1800000, collected 2048 steps\n",
      "Iteration 1359872/1800000, collected 2048 steps\n",
      "Iteration 1361920/1800000, collected 2048 steps\n",
      "Iteration 1363968/1800000, collected 2048 steps\n",
      "Iteration 1366016/1800000, collected 2048 steps\n",
      "Iteration 1368064/1800000, collected 2048 steps\n",
      "Iteration 1370112/1800000, collected 2048 steps\n",
      "Iteration 1372160/1800000, collected 2048 steps\n",
      "Iteration 1374208/1800000, collected 2048 steps\n",
      "Iteration 1376256/1800000, collected 2048 steps\n",
      "Iteration 1378304/1800000, collected 2048 steps\n",
      "Iteration 1380352/1800000, collected 2048 steps\n",
      "Iteration 1382400/1800000, collected 2048 steps\n",
      "Iteration 1384448/1800000, collected 2048 steps\n",
      "Iteration 1386496/1800000, collected 2048 steps\n",
      "Iteration 1388544/1800000, collected 2048 steps\n",
      "Iteration 1390592/1800000, collected 2048 steps\n",
      "Iteration 1392640/1800000, collected 2048 steps\n",
      "Iteration 1394688/1800000, collected 2048 steps\n",
      "Iteration 1396736/1800000, collected 2048 steps\n",
      "Iteration 1398784/1800000, collected 2048 steps\n",
      "Iteration 1400832/1800000, collected 2048 steps\n",
      "Iteration 1402880/1800000, collected 2048 steps\n",
      "Iteration 1404928/1800000, collected 2048 steps\n",
      "Iteration 1406976/1800000, collected 2048 steps\n",
      "Iteration 1409024/1800000, collected 2048 steps\n",
      "Iteration 1411072/1800000, collected 2048 steps\n",
      "Iteration 1413120/1800000, collected 2048 steps\n",
      "Iteration 1415168/1800000, collected 2048 steps\n",
      "Iteration 1417216/1800000, collected 2048 steps\n",
      "Iteration 1419264/1800000, collected 2048 steps\n",
      "Iteration 1421312/1800000, collected 2048 steps\n",
      "Iteration 1423360/1800000, collected 2048 steps\n",
      "Iteration 1425408/1800000, collected 2048 steps\n",
      "Iteration 1427456/1800000, collected 2048 steps\n",
      "Iteration 1429504/1800000, collected 2048 steps\n",
      "Iteration 1431552/1800000, collected 2048 steps\n",
      "Iteration 1433600/1800000, collected 2048 steps\n",
      "Iteration 1435648/1800000, collected 2048 steps\n",
      "Iteration 1437696/1800000, collected 2048 steps\n",
      "Iteration 1439744/1800000, collected 2048 steps\n",
      "Iteration 1441792/1800000, collected 2048 steps\n",
      "Iteration 1443840/1800000, collected 2048 steps\n",
      "Iteration 1445888/1800000, collected 2048 steps\n",
      "Iteration 1447936/1800000, collected 2048 steps\n",
      "Iteration 1449984/1800000, collected 2048 steps\n",
      "Iteration 1452032/1800000, collected 2048 steps\n",
      "Iteration 1454080/1800000, collected 2048 steps\n",
      "Iteration 1456128/1800000, collected 2048 steps\n",
      "Iteration 1458176/1800000, collected 2048 steps\n",
      "Iteration 1460224/1800000, collected 2048 steps\n",
      "Iteration 1462272/1800000, collected 2048 steps\n",
      "Iteration 1464320/1800000, collected 2048 steps\n",
      "Iteration 1466368/1800000, collected 2048 steps\n",
      "Iteration 1468416/1800000, collected 2048 steps\n",
      "Iteration 1470464/1800000, collected 2048 steps\n",
      "Iteration 1472512/1800000, collected 2048 steps\n",
      "Iteration 1474560/1800000, collected 2048 steps\n",
      "Iteration 1476608/1800000, collected 2048 steps\n",
      "Iteration 1478656/1800000, collected 2048 steps\n",
      "Iteration 1480704/1800000, collected 2048 steps\n",
      "Iteration 1482752/1800000, collected 2048 steps\n",
      "Iteration 1484800/1800000, collected 2048 steps\n",
      "Iteration 1486848/1800000, collected 2048 steps\n",
      "Iteration 1488896/1800000, collected 2048 steps\n",
      "Iteration 1490944/1800000, collected 2048 steps\n",
      "Iteration 1492992/1800000, collected 2048 steps\n",
      "Iteration 1495040/1800000, collected 2048 steps\n",
      "Iteration 1497088/1800000, collected 2048 steps\n",
      "Iteration 1499136/1800000, collected 2048 steps\n",
      "Iteration 1501184/1800000, collected 2048 steps\n",
      "Iteration 1503232/1800000, collected 2048 steps\n",
      "Iteration 1505280/1800000, collected 2048 steps\n",
      "Iteration 1507328/1800000, collected 2048 steps\n",
      "Iteration 1509376/1800000, collected 2048 steps\n",
      "Iteration 1511424/1800000, collected 2048 steps\n",
      "Iteration 1513472/1800000, collected 2048 steps\n",
      "Iteration 1515520/1800000, collected 2048 steps\n",
      "Iteration 1517568/1800000, collected 2048 steps\n",
      "Iteration 1519616/1800000, collected 2048 steps\n",
      "Iteration 1521664/1800000, collected 2048 steps\n",
      "Iteration 1523712/1800000, collected 2048 steps\n",
      "Iteration 1525760/1800000, collected 2048 steps\n",
      "Iteration 1527808/1800000, collected 2048 steps\n",
      "Iteration 1529856/1800000, collected 2048 steps\n",
      "Iteration 1531904/1800000, collected 2048 steps\n",
      "Iteration 1533952/1800000, collected 2048 steps\n",
      "Iteration 1536000/1800000, collected 2048 steps\n",
      "Iteration 1538048/1800000, collected 2048 steps\n",
      "Iteration 1540096/1800000, collected 2048 steps\n",
      "Iteration 1542144/1800000, collected 2048 steps\n",
      "Iteration 1544192/1800000, collected 2048 steps\n",
      "Iteration 1546240/1800000, collected 2048 steps\n",
      "Iteration 1548288/1800000, collected 2048 steps\n",
      "Iteration 1550336/1800000, collected 2048 steps\n",
      "Iteration 1552384/1800000, collected 2048 steps\n",
      "Iteration 1554432/1800000, collected 2048 steps\n",
      "Iteration 1556480/1800000, collected 2048 steps\n",
      "Iteration 1558528/1800000, collected 2048 steps\n",
      "Iteration 1560576/1800000, collected 2048 steps\n",
      "Iteration 1562624/1800000, collected 2048 steps\n",
      "Iteration 1564672/1800000, collected 2048 steps\n",
      "Iteration 1566720/1800000, collected 2048 steps\n",
      "Iteration 1568768/1800000, collected 2048 steps\n",
      "Iteration 1570816/1800000, collected 2048 steps\n",
      "Iteration 1572864/1800000, collected 2048 steps\n",
      "Iteration 1574912/1800000, collected 2048 steps\n",
      "Iteration 1576960/1800000, collected 2048 steps\n",
      "Iteration 1579008/1800000, collected 2048 steps\n",
      "Iteration 1581056/1800000, collected 2048 steps\n",
      "Iteration 1583104/1800000, collected 2048 steps\n",
      "Iteration 1585152/1800000, collected 2048 steps\n",
      "Iteration 1587200/1800000, collected 2048 steps\n",
      "Iteration 1589248/1800000, collected 2048 steps\n",
      "Iteration 1591296/1800000, collected 2048 steps\n",
      "Iteration 1593344/1800000, collected 2048 steps\n",
      "Iteration 1595392/1800000, collected 2048 steps\n",
      "Iteration 1597440/1800000, collected 2048 steps\n",
      "Iteration 1599488/1800000, collected 2048 steps\n",
      "Iteration 1601536/1800000, collected 2048 steps\n",
      "Iteration 1603584/1800000, collected 2048 steps\n",
      "Iteration 1605632/1800000, collected 2048 steps\n",
      "Iteration 1607680/1800000, collected 2048 steps\n",
      "Iteration 1609728/1800000, collected 2048 steps\n",
      "Iteration 1611776/1800000, collected 2048 steps\n",
      "Iteration 1613824/1800000, collected 2048 steps\n",
      "Iteration 1615872/1800000, collected 2048 steps\n",
      "Iteration 1617920/1800000, collected 2048 steps\n",
      "Iteration 1619968/1800000, collected 2048 steps\n",
      "Iteration 1622016/1800000, collected 2048 steps\n",
      "Iteration 1624064/1800000, collected 2048 steps\n",
      "Iteration 1626112/1800000, collected 2048 steps\n",
      "Iteration 1628160/1800000, collected 2048 steps\n",
      "Iteration 1630208/1800000, collected 2048 steps\n",
      "Iteration 1632256/1800000, collected 2048 steps\n",
      "Iteration 1634304/1800000, collected 2048 steps\n",
      "Iteration 1636352/1800000, collected 2048 steps\n",
      "Iteration 1638400/1800000, collected 2048 steps\n",
      "Iteration 1640448/1800000, collected 2048 steps\n",
      "Iteration 1642496/1800000, collected 2048 steps\n",
      "Iteration 1644544/1800000, collected 2048 steps\n",
      "Iteration 1646592/1800000, collected 2048 steps\n",
      "Iteration 1648640/1800000, collected 2048 steps\n",
      "Iteration 1650688/1800000, collected 2048 steps\n",
      "Iteration 1652736/1800000, collected 2048 steps\n",
      "Iteration 1654784/1800000, collected 2048 steps\n",
      "Iteration 1656832/1800000, collected 2048 steps\n",
      "Iteration 1658880/1800000, collected 2048 steps\n",
      "Iteration 1660928/1800000, collected 2048 steps\n",
      "Iteration 1662976/1800000, collected 2048 steps\n",
      "Iteration 1665024/1800000, collected 2048 steps\n",
      "Iteration 1667072/1800000, collected 2048 steps\n",
      "Iteration 1669120/1800000, collected 2048 steps\n",
      "Iteration 1671168/1800000, collected 2048 steps\n",
      "Iteration 1673216/1800000, collected 2048 steps\n",
      "Iteration 1675264/1800000, collected 2048 steps\n",
      "Iteration 1677312/1800000, collected 2048 steps\n",
      "Iteration 1679360/1800000, collected 2048 steps\n",
      "Iteration 1681408/1800000, collected 2048 steps\n",
      "Iteration 1683456/1800000, collected 2048 steps\n",
      "Iteration 1685504/1800000, collected 2048 steps\n",
      "Iteration 1687552/1800000, collected 2048 steps\n",
      "Iteration 1689600/1800000, collected 2048 steps\n",
      "Iteration 1691648/1800000, collected 2048 steps\n",
      "Iteration 1693696/1800000, collected 2048 steps\n",
      "Iteration 1695744/1800000, collected 2048 steps\n",
      "Iteration 1697792/1800000, collected 2048 steps\n",
      "Iteration 1699840/1800000, collected 2048 steps\n",
      "Iteration 1701888/1800000, collected 2048 steps\n",
      "Iteration 1703936/1800000, collected 2048 steps\n",
      "Iteration 1705984/1800000, collected 2048 steps\n",
      "Iteration 1708032/1800000, collected 2048 steps\n",
      "Iteration 1710080/1800000, collected 2048 steps\n",
      "Iteration 1712128/1800000, collected 2048 steps\n",
      "Iteration 1714176/1800000, collected 2048 steps\n",
      "Iteration 1716224/1800000, collected 2048 steps\n",
      "Iteration 1718272/1800000, collected 2048 steps\n",
      "Iteration 1720320/1800000, collected 2048 steps\n",
      "Iteration 1722368/1800000, collected 2048 steps\n",
      "Iteration 1724416/1800000, collected 2048 steps\n",
      "Iteration 1726464/1800000, collected 2048 steps\n",
      "Iteration 1728512/1800000, collected 2048 steps\n",
      "Iteration 1730560/1800000, collected 2048 steps\n",
      "Iteration 1732608/1800000, collected 2048 steps\n",
      "Iteration 1734656/1800000, collected 2048 steps\n",
      "Iteration 1736704/1800000, collected 2048 steps\n",
      "Iteration 1738752/1800000, collected 2048 steps\n",
      "Iteration 1740800/1800000, collected 2048 steps\n",
      "Iteration 1742848/1800000, collected 2048 steps\n",
      "Iteration 1744896/1800000, collected 2048 steps\n",
      "Iteration 1746944/1800000, collected 2048 steps\n",
      "Iteration 1748992/1800000, collected 2048 steps\n",
      "Iteration 1751040/1800000, collected 2048 steps\n",
      "Iteration 1753088/1800000, collected 2048 steps\n",
      "Iteration 1755136/1800000, collected 2048 steps\n",
      "Iteration 1757184/1800000, collected 2048 steps\n",
      "Iteration 1759232/1800000, collected 2048 steps\n",
      "Iteration 1761280/1800000, collected 2048 steps\n",
      "Iteration 1763328/1800000, collected 2048 steps\n",
      "Iteration 1765376/1800000, collected 2048 steps\n",
      "Iteration 1767424/1800000, collected 2048 steps\n",
      "Iteration 1769472/1800000, collected 2048 steps\n",
      "Iteration 1771520/1800000, collected 2048 steps\n",
      "Iteration 1773568/1800000, collected 2048 steps\n",
      "Iteration 1775616/1800000, collected 2048 steps\n",
      "Iteration 1777664/1800000, collected 2048 steps\n",
      "Iteration 1779712/1800000, collected 2048 steps\n",
      "Iteration 1781760/1800000, collected 2048 steps\n",
      "Iteration 1783808/1800000, collected 2048 steps\n",
      "Iteration 1785856/1800000, collected 2048 steps\n",
      "Iteration 1787904/1800000, collected 2048 steps\n",
      "Iteration 1789952/1800000, collected 2048 steps\n",
      "Iteration 1792000/1800000, collected 2048 steps\n",
      "Iteration 1794048/1800000, collected 2048 steps\n",
      "Iteration 1796096/1800000, collected 2048 steps\n",
      "Iteration 1798144/1800000, collected 2048 steps\n",
      "Iteration 1800192/1800000, collected 2048 steps\n",
      "Modelo exitosamente guardado en ../Saved_Models/PPO\n"
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array, scenario = 'contest'\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "#venv = VecTransposeImage(VecFrameStack(SubprocVecEnv([make_env] * 8), n_stack=4))\n",
    "if Model == \"DQN\":\n",
    "  agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"D3QN\":\n",
    "  agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE, update_target_freq=UPDATE_TARGET_FREQ)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model_online.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"PPO\":\n",
    "  agent = ConvPPOAgent(env=env, input_shape=input_shape, learning_rate=LR, gamma=GAMMA, n_steps=N_STEPS, clip=CLIP, n_updates_per_iteration=N_UPDATES_PER_ITERATION, entropy_coef=ENTROPY_COEF)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "\n",
    "agent.learn(num_episodes*max_steps_per_episode)\n",
    "save_model(agent, num_episodes)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d648480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/seba/Documentos/AI/RL/Video/PPO folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120459/304922075.py:23: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  agent.model_actor.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
      "/tmp/ipykernel_120459/304922075.py:26: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  agent.model_critic.state_dict(torch.load(model_load_path, map_location=agent.device))\n"
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=f'../Video/{Model}',    # Folder to save videos\n",
    "    name_prefix=f'eval-V{version}-S{max_steps_per_episode*num_episodes}',               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: True    # Record every episode\n",
    ")\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "if Model == \"DQN\":\n",
    "    model_load_path = get_last_modified_file(target_directory)\n",
    "    agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"D3QN\":  \n",
    "    model_load_path = get_last_modified_file(target_directory)\n",
    "    agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "    agent.model_online.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"PPO\":\n",
    "    agent = ConvPPOAgent(env=env, input_shape=input_shape)\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Actor\")\n",
    "    agent.model_actor.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_actor.eval()\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Critic\")\n",
    "    agent.model_critic.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_critic.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e22ae8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 522.6602030992508\n",
      "Episode: 1 Reward: 553.9629039764404\n",
      "Episode: 2 Reward: 229.5531206727028\n",
      "Episode: 3 Reward: -6.556510925292969e-07\n",
      "Episode: 4 Reward: 265.59865218400955\n",
      "Episode: 5 Reward: 1.1920928955078125e-07\n",
      "Episode: 6 Reward: 113.82799369096756\n",
      "Episode: 7 Reward: -4.172325134277344e-07\n",
      "Episode: 8 Reward: 1.7881393432617188e-07\n",
      "Episode: 9 Reward: 219.11888718605042\n"
     ]
    }
   ],
   "source": [
    "episode = 10\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if Model == \"PPO\":\n",
    "            obs = agent.preprocess_wv(state=obs)\n",
    "            action = agent.get_action(obs = obs)[0]\n",
    "        else:\n",
    "            action = agent.act(state = obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff4ac12-8cfa-41b2-bbe0-70719551fa67",
   "metadata": {},
   "source": [
    "###### env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", state='GreenHillZone.Act1', render_mode='human', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "episode = 10\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = int(input('input:'))\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        print(reward)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0ef33-6430-4ff5-8e37-eeb8f536021f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
