{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cbe9c13-8df3-4c42-b865-e731851135be",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.distributions import MultivariateNormal, Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation, GrayscaleObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "import io\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b7406-06cd-4f78-8b7d-e8b0222c38a5",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc92308",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = False\n",
    "LOAD_MODEL = False\n",
    "RGB = False\n",
    "FRAMESTACK = True\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "num_episodes = 200\n",
    "max_steps_per_episode = 1800\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 4\n",
    "Model = \"D3QN\"\n",
    "save_interval = 100\n",
    "episode_p_interval = 4\n",
    "rew_p_interval = 900\n",
    "\n",
    "#Version\n",
    "version = 8\n",
    "\n",
    "#Hiperparametros\n",
    "LR = 2e-5\n",
    "GAMMA = 0.999\n",
    "\n",
    "#DQN y D3QN Params\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.01#0.99 Exponential / 0.01 Reciprocal\n",
    "BUFFER_SIZE = 20000\n",
    "batch_size = 256\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "#D3QN\n",
    "UPDATE_TARGET_FREQ = 10000\n",
    "\n",
    "#PPO Params\n",
    "N_STEPS = 1024\n",
    "N_UPDATES_PER_ITERATION = 5\n",
    "CLIP = 0.1\n",
    "ENTROPY_COEF = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a252dd-f567-4a6a-96cc-a015f3459327",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a9764-302a-4fc9-8a2c-dd62f8201411",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c752692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_modified_file(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
    "        return None\n",
    "    files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=os.path.getmtime, reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "target_directory = f\"../Saved_Models/{Model}\" \n",
    "model_load_path = get_last_modified_file(target_directory)\n",
    "\n",
    "if model_load_path:\n",
    "    print(f\"The last modified file is: {model_load_path}\")\n",
    "else:\n",
    "    print(\"No files found in the directory or directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"../Saved_Models\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Actor\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Critic\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "try:\n",
    "    os.mkdir(\"../Video/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Logs/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "try:\n",
    "    os.mkdir(\"../Logs/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Logs/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/{Model}' #ppt para jit, pth para statedict\n",
    "    model_file_name = f'/{Model}-Sonic-V{version}-E{episode}-S{max_steps_per_episode}.pth'\n",
    "    try:\n",
    "        if Model == \"PPO\":\n",
    "            torch.save(agent.model_actor.state_dict(), model_save_path+\"/Actor\"+model_file_name)\n",
    "            torch.save(agent.model_critic.state_dict(), model_save_path+\"/Critic\"+model_file_name)\n",
    "        else:\n",
    "            torch.save(agent.model_online.state_dict(), model_save_path+model_file_name)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym-retro environment and make it use discrete\n",
    "    actions for the Sonic game.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ButtonActionWrapper, self).__init__(env)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],\n",
    "                   ['DOWN', 'B'], ['B']]\n",
    "        self._actions = []\n",
    "        for action in actions:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in action:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._actions.append(arr)\n",
    "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
    "\n",
    "    def action(self, a):\n",
    "        return self._actions[a].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a2d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, mov_rew=1, score_rew=1, hp_rew=1, ring_rew=0.5, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= self.mov_rew\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticFrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, n, stickprob):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n = n\n",
    "        self.stickprob = stickprob\n",
    "        self.curac = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.supports_want_render = hasattr(env, \"supports_want_render\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.curac = None\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, ac):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        totrew = 0\n",
    "        for i in range(self.n):\n",
    "            if self.curac is None:\n",
    "                self.curac = ac\n",
    "            elif i == 0:\n",
    "                if self.rng.rand() > self.stickprob:\n",
    "                    self.curac = ac\n",
    "            elif i == 1:\n",
    "                self.curac = ac\n",
    "            if self.supports_want_render and i < self.n - 1:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(\n",
    "                    self.curac,\n",
    "                    want_render=False,\n",
    "                )\n",
    "            else:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(self.curac)\n",
    "            totrew += rew\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        return ob, totrew, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e187fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(*, game, state=None, max_episode_steps=4500,log_mon = False, **kwargs):\n",
    "    if state is None:\n",
    "        state = retro.State.DEFAULT\n",
    "    env = retro.make(game, state, **kwargs)\n",
    "    env = ButtonActionWrapper(env)\n",
    "    #env = CustomRewardWrapper(env)\n",
    "    env = StochasticFrameSkip(env, n=num_frame_skip, stickprob=0.25)\n",
    "    if RGB:\n",
    "        channels = 3\n",
    "    else:\n",
    "        env = GrayscaleObservation(env)\n",
    "        channels = 1\n",
    "    if RESIZE_ENV:\n",
    "        input_shape = (num_stacked_frames*channels, *new_size)\n",
    "        env = ResizeObservation(env, new_size)\n",
    "    else:\n",
    "        input_shape = (num_stacked_frames*channels, 224, 320)\n",
    "    if FRAMESTACK:\n",
    "        env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "    if max_episode_steps is not None:\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    if log_mon:\n",
    "        env = Monitor(env, f'../Logs/{Model}/{Model}_V{version}_S{num_episodes*max_steps_per_episode}.csv')\n",
    "    return env, input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b01301-3ab5-4626-a718-dfdb3b7c604b",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff4f99-d49d-4875-ad76-a8885b6e961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    def add(self, experience):\n",
    "        e = self.experience(*experience)\n",
    "        self.memory.append(e)\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "        minibatch_compatible = [[e] for e in experiences]\n",
    "        tree_indices = None \n",
    "        is_weights = None\n",
    "        return tree_indices, minibatch_compatible, is_weights\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc36ece-aa1d-4039-a9e0-574535ccaa2c",
   "metadata": {},
   "source": [
    "## PrioritizedExperienceReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5419f093-e0a6-4b8c-bfa7-9142bf8e0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    data_pointer = 0\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity \n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.current_size = 0\n",
    "        \n",
    "    def add(self, priority, data):\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update (tree_index, priority)\n",
    "        if self.current_size < self.capacity:\n",
    "            self.current_size += 1\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  \n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    def update(self, tree_index, priority):\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        while tree_index != 0: \n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "            \n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "        while True: \n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else: \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "        \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f1a8b-961d-4314-ae83-094c7d661a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedExperienceReplay:\n",
    "    PER_e = 0.01  \n",
    "    PER_a = 0.6  \n",
    "    PER_b = 0.4  \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    absolute_error_upper = 1.  \n",
    "        \n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        self.tree.add(max_priority, experience)   \n",
    "        \n",
    "    def sample(self, n):\n",
    "        memory_b = []\n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        priority_segment = self.tree.total_priority / n \n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling]) \n",
    "        \n",
    "        filled_priorities = self.tree.tree[-self.tree.capacity:][:self.tree.current_size]\n",
    "        if len(filled_priorities) > 0 and self.tree.total_priority > 0:\n",
    "            p_min = np.min(filled_priorities) / self.tree.total_priority\n",
    "        else:\n",
    "            p_min = 1e-8\n",
    "        \n",
    "        if p_min == 0 or p_min * n == 0:\n",
    "            p_min = 1e-8\n",
    "            \n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "            b_idx[i]= index\n",
    "            experience = [data]\n",
    "            memory_b.append(experience)\n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "        \n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)\n",
    "    def __len__(self):\n",
    "        return self.tree.current_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6eed1d-f3c2-4f7e-a6e9-226975ec164b",
   "metadata": {},
   "source": [
    "# Models & Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a22dd-7ff2-4a35-afa6-988f0474d218",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2246aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, buffer_class, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000, update_target_freq=10000, epsilon_end=0.01):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.st = epsilon_end-((epsilon-epsilon_end)*(100.0/(1.0+epsilon_decay*180000.0)))\n",
    "        self.memory = buffer_class(buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model_online = ConvDQN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target = ConvDQN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model_online.parameters(), lr=lr)\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.step_counter = 0\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_start = epsilon\n",
    "        self.decay_steps = 0\n",
    "        self.scaler = GradScaler(self.device)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device) / 255.0\n",
    "        if RGB and FRAMESTACK:\n",
    "            state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "            C_out = self.input_shape[0]\n",
    "            H_out = self.input_shape[1] \n",
    "            W_out = self.input_shape[2] \n",
    "            state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model_online(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        if self.step_counter % self.update_target_freq == 0:\n",
    "            self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        tree_indices, minibatch, IS_weights = self.memory.sample(batch_size)\n",
    "        experiences = [item[0] for item in minibatch]\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        if IS_weights is not None:\n",
    "            IS_weights_tensor = torch.tensor(IS_weights, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values_target = self.model_target(next_states_tensor)\n",
    "            max_Q_next = torch.max(next_q_values_target, dim=1).values\n",
    "            target_q_values = rewards_tensor + self.gamma * max_Q_next * (~dones_tensor)\n",
    "        \n",
    "        with autocast(device_type=self.device):\n",
    "            current_q_values = self.model_online(states_tensor)\n",
    "            current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "            if IS_weights is not None:\n",
    "                loss = (IS_weights_tensor.squeeze() * nn.MSELoss(reduction='none')(current_q_for_actions, target_q_values)).mean()\n",
    "            else:\n",
    "                loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        \n",
    "        td_error = torch.abs(target_q_values - current_q_for_actions).detach().cpu().numpy()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(self.model_online.parameters(), max_norm=10.0)\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        self.memory.batch_update(tree_indices, td_error)\n",
    "        \n",
    "        self.step_counter += 1\n",
    "        self.update_target_network()\n",
    "        \n",
    "        self.decay()\n",
    "        \n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                if frame_count % 4 == 0:\n",
    "                    #for _ in range(4):\n",
    "                    self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")\n",
    "    def decay(self):\n",
    "        self.decay_steps += 1\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            if self.epsilon_decay > 0.5:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            else:\n",
    "                self.epsilon = max(self.epsilon_end,self.st+((self.epsilon_start-self.epsilon_end)*(100.0/(1.0+self.epsilon_decay*self.decay_steps))))\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a27117-1886-4b15-8749-644f6ca4d73c",
   "metadata": {},
   "source": [
    "## D3QN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d750b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvD3QN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.advance_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        advantages = self.advance_stream(conv_out)\n",
    "        value = self.value_stream(conv_out)\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6de7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, buffer_class, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000, update_target_freq=10000, epsilon_end = 0.01):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.st = epsilon_end-((epsilon-epsilon_end)*(100.0/(1.0+epsilon_decay*180000.0)))\n",
    "        self.memory = buffer_class(buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model_online = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model_online.parameters(), lr=lr)\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.step_counter = 0\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_start = epsilon\n",
    "        self.decay_steps = 0\n",
    "        self.scaler = GradScaler(self.device)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        if RGB and FRAMESTACK:\n",
    "            state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "            C_out = self.input_shape[0]\n",
    "            H_out = self.input_shape[1] \n",
    "            W_out = self.input_shape[2] \n",
    "            state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model_online(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        tree_indices, minibatch, IS_weights = self.memory.sample(batch_size)\n",
    "        experiences = [item[0] for item in minibatch]\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        if IS_weights is not None:\n",
    "            IS_weights_tensor = torch.tensor(IS_weights, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values_online = self.model_online(next_states_tensor)\n",
    "            best_action_online_indices = torch.argmax(next_q_values_online, dim=1).unsqueeze(1)\n",
    "            max_Q_next = self.model_target(next_states_tensor).gather(1, best_action_online_indices).squeeze()\n",
    "        target_q_values = rewards_tensor + self.gamma * max_Q_next * (~dones_tensor)\n",
    "\n",
    "        with autocast(device_type=self.device):\n",
    "            current_q_values = self.model_online(states_tensor)\n",
    "            current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "            if IS_weights is not None:\n",
    "                loss = (IS_weights_tensor.squeeze() * nn.MSELoss(reduction='none')(current_q_for_actions, target_q_values)).mean()\n",
    "            else:\n",
    "                loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "                \n",
    "        td_error = torch.abs(target_q_values - current_q_for_actions).detach().cpu().numpy()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(self.model_online.parameters(), max_norm=10.0)\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        self.memory.batch_update(tree_indices, td_error)\n",
    "        \n",
    "        self.step_counter += 1\n",
    "        self.update_target_network()\n",
    "        \n",
    "        self.decay()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        if self.step_counter % self.update_target_freq == 0:\n",
    "            self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                if frame_count % 4 == 0:\n",
    "                    #for _ in range(4):\n",
    "                    self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")\n",
    "            \n",
    "    def decay(self):\n",
    "        self.decay_steps += 1\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            if self.epsilon_decay > 0.5:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            else:\n",
    "                self.epsilon = max(self.epsilon_end,self.st+((self.epsilon_start-self.epsilon_end)*(100.0/(1.0+self.epsilon_decay*self.decay_steps))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45787692-a026-4b6d-8aa3-f365b6760c87",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPPOActor(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvPPOActor, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "            #nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        conv_out = self.fc_layers(conv_out)\n",
    "        dist = Categorical(logits=conv_out)\n",
    "        return dist\n",
    "    \n",
    "class ConvPPOCritic(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(ConvPPOCritic, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPPOAgent:\n",
    "    def __init__(self, env, input_shape, clip=0.2, learning_rate=1e-4, gamma=0.99, n_steps=2048, n_updates_per_iteration=5, entropy_coef=0.01, minibatch_size = 64, max_grad_norm=0.5, lam = 0.95):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.n_steps = n_steps\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_actor = ConvPPOActor(input_shape, self.num_actions).to(self.device)\n",
    "        #self.model_actor.half()\n",
    "        self.model_critic = ConvPPOCritic(input_shape).to(self.device)\n",
    "        #self.model_critic.half()\n",
    "        self.actor_optimizer = optim.Adam(self.model_actor.parameters(), lr=self.lr)\n",
    "        self.critic_optimizer = optim.Adam(self.model_critic.parameters(), lr=self.lr)\n",
    "        self.n_updates = n_updates_per_iteration\n",
    "        self.clip = clip\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.mb_size = minibatch_size\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.lam = lam\n",
    "        self.csv_file = f'../Logs/{Model}/PPO.csv'\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        dist = self.model_actor(obs.unsqueeze(0))\n",
    "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.detach().squeeze()\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        if RGB and FRAMESTACK:\n",
    "            state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "            C_out = self.input_shape[0]\n",
    "            H_out = self.input_shape[1] \n",
    "            W_out = self.input_shape[2] \n",
    "            state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def rollout(self):\n",
    "        batch_obs = []      \n",
    "        batch_acts = []            \n",
    "        batch_log_probs = []       \n",
    "        batch_rews = []          \n",
    "        batch_lens = []     \n",
    "        batch_vals = []\n",
    "        batch_dones = []\n",
    "        ep_rews = []\n",
    "        ep_vals = []\n",
    "        ep_dones = []\n",
    "        t = 0 \n",
    "        if not hasattr(self, 'current_obs'):\n",
    "            obs, _ =self.env.reset()\n",
    "            self.current_obs = obs\n",
    "        obs = self.current_obs\n",
    "        with torch.no_grad():\n",
    "            while t < self.n_steps:\n",
    "                t+=1\n",
    "                obs_tensor = self.preprocess_wv(obs)\n",
    "                batch_obs.append(obs_tensor)\n",
    "                action, log_prob = self.get_action(obs_tensor)\n",
    "                val = self.model_critic(obs_tensor.unsqueeze(0)).detach().cpu().item()#.numpy().flatten()[0]\n",
    "                obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                ep_dones.append(done)\n",
    "                ep_rews.append(reward)\n",
    "                ep_vals.append(val)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                if done:\n",
    "                    batch_lens.append(len(ep_rews))\n",
    "                    batch_rews.append(ep_rews)\n",
    "                    batch_vals.append(ep_vals)\n",
    "                    batch_dones.append(ep_dones)\n",
    "                    obs, _ = self.env.reset()\n",
    "                    ep_rews = []\n",
    "                    ep_vals = []\n",
    "                    ep_dones = []\n",
    "                    self.current_obs = obs\n",
    "        if len(ep_rews) > 0:\n",
    "            batch_lens.append(len(ep_rews))\n",
    "            batch_rews.append(ep_rews)\n",
    "            batch_vals.append(ep_vals)\n",
    "            batch_dones.append(ep_dones)\n",
    "        final_obs_tensor = self.preprocess_wv(obs)\n",
    "        with torch.no_grad():\n",
    "            self.final_val = self.model_critic(final_obs_tensor.unsqueeze(0)).squeeze().cpu().item()\n",
    "\n",
    "        self.current_obs = obs\n",
    "        batch_obs = torch.stack(batch_obs)\n",
    "        #batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.long, device=self.device)\n",
    "        batch_log_probs = torch.stack(batch_log_probs).float()\n",
    "        #batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float32, device=self.device)\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens, batch_vals, batch_dones\n",
    "    \n",
    "    # def compute_rtgs(self, batch_rews):\n",
    "    #     batch_rtgs = []\n",
    "    #     for ep_rews in reversed(batch_rews):\n",
    "    #         discounted_reward = 0 \n",
    "    #         for rew in reversed(ep_rews):\n",
    "    #             discounted_reward = rew + discounted_reward * self.gamma\n",
    "    #             batch_rtgs.insert(0, discounted_reward)\n",
    "    #     batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float, device=self.device)\n",
    "    #     return batch_rtgs\n",
    "\n",
    "    def calculate_gae(self, rewards, values, dones):\n",
    "        batch_advantages = []\n",
    "        for i, (ep_rews, ep_vals, ep_dones) in enumerate(zip(rewards, values, dones)):\n",
    "            is_last_segment = (i == len(rewards) - 1)\n",
    "            advantages = []\n",
    "            last_advantage = 0\n",
    "            for t in reversed(range(len(ep_rews))):\n",
    "                is_terminal = ep_dones[t]\n",
    "                if is_terminal:\n",
    "                    next_val = 0\n",
    "                elif t == len(ep_rews) - 1 and is_last_segment:\n",
    "                    next_val = self.final_val\n",
    "                else:\n",
    "                    next_val = ep_vals[t+1]\n",
    "                delta = ep_rews[t] + self.gamma * next_val - ep_vals[t]\n",
    "                advantage = delta + self.gamma * self.lam * (1 - is_terminal) * last_advantage\n",
    "                last_advantage = advantage\n",
    "                advantages.insert(0, advantage)\n",
    "            batch_advantages.extend(advantages)\n",
    "        return torch.tensor(batch_advantages, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        V = self.model_critic(batch_obs).view(-1) #.squeeze()\n",
    "        dist = self.model_actor(batch_obs)\n",
    "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        entropy_loss = dist.entropy().mean()\n",
    "        return V, log_probs, entropy_loss\n",
    "    \n",
    "    def learn(self, total_timesteps):\n",
    "        act_t = 0\n",
    "        while act_t < total_timesteps:\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens, batch_vals, batch_dones = self.rollout()\n",
    "            act_t += np.sum(batch_lens)\n",
    "            \n",
    "            batch_obs = batch_obs.cpu()\n",
    "            batch_acts = batch_acts.cpu()\n",
    "            batch_log_probs = batch_log_probs.cpu()\n",
    "            \n",
    "            V_old_flat = [val for ep_vals in batch_vals for val in ep_vals]\n",
    "            V_old_flat = torch.tensor(V_old_flat, dtype=torch.float32).detach() #, device=self.device\n",
    "            A_k = self.calculate_gae(batch_rews, batch_vals, batch_dones)\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "            A_k = A_k.cpu()\n",
    "            # batch_acts = torch.tensor(batch_acts, dtype=torch.long, device=self.device)\n",
    "            # batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float, device=self.device)\n",
    "            batch_rtgs = A_k + V_old_flat\n",
    "            step = len(batch_obs)\n",
    "            inds = np.arange(step)\n",
    "            print(f\"Iteration {act_t}/{total_timesteps}, collected {np.sum(batch_lens)} steps\")\n",
    "            assert len(V_old_flat) == len(batch_obs), f\"Value mismatch: {len(V_old_flat)} vs {len(batch_obs)}\"\n",
    "            for _ in range(self.n_updates):\n",
    "                np.random.shuffle(inds)\n",
    "                for start in range(0, step, self.mb_size):\n",
    "                    end = start + self.mb_size\n",
    "                    idx = inds[start:end]\n",
    "                    mini_obs = batch_obs[idx].to(self.device)\n",
    "                    mini_acts = batch_acts[idx].to(self.device)\n",
    "                    mini_log_probs = batch_log_probs[idx].to(self.device)\n",
    "                    mini_advantage = A_k[idx].to(self.device)\n",
    "                    mini_rtgs = batch_rtgs[idx].to(self.device)\n",
    "                    V, curr_log_probs, curr_entropy_loss = self.evaluate(mini_obs, mini_acts)\n",
    "                    ratios = torch.exp(curr_log_probs - mini_log_probs)\n",
    "                    surr1 = ratios * mini_advantage\n",
    "                    surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * mini_advantage\n",
    "                    actor_loss = (-torch.min(surr1, surr2)).mean() - self.entropy_coef*curr_entropy_loss\n",
    "                    critic_loss = nn.MSELoss()(V, mini_rtgs)\n",
    "                    self.actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.model_actor.parameters(), self.max_grad_norm)\n",
    "                    self.actor_optimizer.step()\n",
    "                    self.critic_optimizer.zero_grad()    \n",
    "                    critic_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.model_critic.parameters(), self.max_grad_norm)\n",
    "                    self.critic_optimizer.step()\n",
    "            del batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens\n",
    "            del batch_vals, batch_dones, V_old_flat, A_k, batch_rtgs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d2dce-2735-4094-b5c5-4726d4b5baa6",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('No Enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37280243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', log_mon = True, max_episode_steps=max_steps_per_episode) #rgb_array, scenario = 'contest'\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "#venv = VecTransposeImage(VecFrameStack(SubprocVecEnv([make_env] * 8), n_stack=4))\n",
    "if Model == \"DQN\":\n",
    "  agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, buffer_class=PrioritizedExperienceReplay, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE, update_target_freq=UPDATE_TARGET_FREQ)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"D3QN\":\n",
    "  agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, buffer_class=PrioritizedExperienceReplay, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE, update_target_freq=UPDATE_TARGET_FREQ)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model_online.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"PPO\":\n",
    "  agent = ConvPPOAgent(env=env, input_shape=input_shape, learning_rate=LR, gamma=GAMMA, n_steps=N_STEPS, clip=CLIP, n_updates_per_iteration=N_UPDATES_PER_ITERATION, entropy_coef=ENTROPY_COEF)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "\n",
    "agent.learn(num_episodes*max_steps_per_episode)\n",
    "save_model(agent, num_episodes)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf030d1-f77e-4cf8-aecb-cd92c64d7a02",
   "metadata": {},
   "source": [
    "# Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c0991-dab6-4d4d-b340-d3a541136e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('No Enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=f'../Video/{Model}',   \n",
    "    name_prefix=f'eval-V{version}-S{max_steps_per_episode*num_episodes}',   \n",
    "    episode_trigger=lambda x: True  \n",
    ")\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "target_directory = f\"../Saved_Models/{Model}\" \n",
    "if Model == \"PPO\":\n",
    "    agent = ConvPPOAgent(env=env, input_shape=input_shape)\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Actor\")\n",
    "    agent.model_actor.load_state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_actor.eval()\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Critic\")\n",
    "    agent.model_critic.load_state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_critic.eval()\n",
    "else:  \n",
    "    model_load_path = get_last_modified_file(target_directory)\n",
    "    if Model == \"D3QN\":\n",
    "        agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, buffer_class=PrioritizedExperienceReplay, lr=LR, gamma=GAMMA, epsilon=0.05, epsilon_decay=0.9955, buffer_size=20000)\n",
    "    else:\n",
    "        agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, buffer_class=PrioritizedExperienceReplay, lr=LR, gamma=GAMMA, epsilon=0.05, epsilon_decay=0.9955, buffer_size=20000)\n",
    "    agent.model_online.load_state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.load_state_dict(torch.load(model_load_path, map_location=agent.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ae8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 10\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if Model == \"PPO\":\n",
    "            obs = agent.preprocess_wv(state=obs)\n",
    "            action = agent.get_action(obs = obs)[0]\n",
    "        else:\n",
    "            action = agent.act(state = obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf7e07-5e51-4ada-8dd7-cb360ec03e66",
   "metadata": {},
   "source": [
    "# Graficar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e5726-82bb-411f-9d2e-e41bbe37a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "monitor_dir = f'../Logs/{Model}/{Model}_V{version}_S{num_episodes*max_steps_per_episode}.csv.monitor.csv'\n",
    "df = pd.read_csv(monitor_dir, header=1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['r'], label='Episode Reward', alpha=0.5)\n",
    "\n",
    "# Calculate smooth reward (e.g., using a rolling mean)\n",
    "window_size = 50  # You can adjust this window size\n",
    "df['smooth_reward'] = df['r'].rolling(window=window_size).mean()\n",
    "plt.plot(df['smooth_reward'], label=f'Smooth Reward', linestyle='--')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Graph')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f651bf8-ac1b-4760-ae0d-ca30ac0f703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(monitor_dir, header=1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['l'], label='Episode Length', alpha=0.5)\n",
    "\n",
    "# Calculate smooth reward (e.g., using a rolling mean)\n",
    "window_size = 50  # You can adjust this window size\n",
    "df['smooth_reward'] = df['l'].rolling(window=window_size).mean()\n",
    "plt.plot(df['smooth_reward'], label=f'Smooth Length', linestyle='--')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length')\n",
    "plt.title('Length Graph')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
