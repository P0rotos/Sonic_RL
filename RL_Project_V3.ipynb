{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce02c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbc92308",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = True\n",
    "LOAD_MODEL = False\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "batch_size = 64\n",
    "num_episodes = 200\n",
    "max_steps_per_episode = 1800\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 2\n",
    "version = 1\n",
    "Model = \"D3QN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c752692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Directory '../Saved_Models/D3QN' does not exist.\n",
      "No files found in the directory or directory does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_last_modified_file(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
    "        return None\n",
    "    files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=os.path.getmtime, reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "model_load_path = get_last_modified_file(target_directory)\n",
    "\n",
    "if model_load_path:\n",
    "    print(f\"The last modified file is: {model_load_path}\")\n",
    "else:\n",
    "    print(\"No files found in the directory or directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6229fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym-retro environment and make it use discrete\n",
    "    actions for the Sonic game.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ButtonActionWrapper, self).__init__(env)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],\n",
    "                   ['DOWN', 'B'], ['B']]\n",
    "        self._actions = []\n",
    "        for action in actions:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in action:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._actions.append(arr)\n",
    "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
    "\n",
    "    def action(self, a): # pylint: disable=W0221\n",
    "        return self._actions[a].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91a2d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, mov_rew=0.01, score_rew=0.05, hp_rew=1, ring_rew=0.1, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= self.mov_rew\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f4e5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticFrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, n, stickprob):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n = n\n",
    "        self.stickprob = stickprob\n",
    "        self.curac = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.supports_want_render = hasattr(env, \"supports_want_render\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.curac = None\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, ac):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        totrew = 0\n",
    "        for i in range(self.n):\n",
    "            # First step after reset, use action\n",
    "            if self.curac is None:\n",
    "                self.curac = ac\n",
    "            # First substep, delay with probability=stickprob\n",
    "            elif i == 0:\n",
    "                if self.rng.rand() > self.stickprob:\n",
    "                    self.curac = ac\n",
    "            # Second substep, new action definitely kicks in\n",
    "            elif i == 1:\n",
    "                self.curac = ac\n",
    "            if self.supports_want_render and i < self.n - 1:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(\n",
    "                    self.curac,\n",
    "                    want_render=False,\n",
    "                )\n",
    "            else:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(self.curac)\n",
    "            totrew += rew\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        return ob, totrew, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d2246aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f3cce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, input_shape, num_actions, lr, gamma, epsilon, epsilon_decay, buffer_size):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model = ConvDQN(input_shape, num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        transform = T.Lambda(lambda x: x.permute(0,3,1,2).reshape(-1, self.input_shape[1], self.input_shape[2]))\n",
    "        return transform(state)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = self.preprocess_wv(next_state)\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state.unsqueeze(0))).item()\n",
    "            state = self.preprocess_wv(state)\n",
    "            target_f = self.model(state.unsqueeze(0)).to(\"cpu\").detach().numpy()\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(torch.tensor(target_f).to(self.device), self.model(state.unsqueeze(0)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states_tensor)\n",
    "            max_next_q = torch.max(next_q_values, dim=1)[0]\n",
    "        target_q_values = rewards_tensor + self.gamma * max_next_q * (~dones_tensor)\n",
    "        current_q_values = self.model(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.epsilon > 0.05:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23d750b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvD3QN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.advance_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        advantages = self.advance_stream(conv_out)\n",
    "        value = self.value_stream(conv_out)\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6de7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QNAgent:\n",
    "    def __init__(self, input_shape, num_actions, lr, gamma, epsilon, epsilon_decay, buffer_size):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model_online = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model_online.parameters(), lr=lr)\n",
    "        self.update_target_freq = 10000\n",
    "        self.step_counter = 0\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        transform = T.Lambda(lambda x: x.permute(0,3,1,2).reshape(-1, self.input_shape[1], self.input_shape[2]))\n",
    "        return transform(state)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model_online(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values_online = self.model_online(next_states_tensor)\n",
    "            best_action_online_indices = torch.argmax(next_q_values_online, dim=1).unsqueeze(1)\n",
    "            max_Q_next = self.model_target(next_states_tensor).gather(1, best_action_online_indices).squeeze()\n",
    "        target_q_values = rewards_tensor + self.gamma * max_Q_next * (~dones_tensor)\n",
    "        current_q_values = self.model_online(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.step_counter += 1\n",
    "        self.update_target_network()\n",
    "        if self.epsilon > 0.05:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    def update_target_network(self):\n",
    "        if self.step_counter % self.update_target_freq == 0:\n",
    "            self.model_target.load_state_dict(self.model_online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50a63e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/DQN/DQN-Sonic-V{version}-E{episode}-S{max_steps_per_episode}.pth' #ppt para jit, pth para statedict\n",
    "    try:\n",
    "        torch.save(agent.model.state_dict(), model_save_path)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "348c5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('No Enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8e187fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(*, game, state=None, max_episode_steps=4500, **kwargs):\n",
    "    if state is None:\n",
    "        state = retro.State.DEFAULT\n",
    "    env = retro.make(game, state, **kwargs)\n",
    "    env = ButtonActionWrapper(env)\n",
    "    #env = CustomRewardWrapper(env)\n",
    "    env = StochasticFrameSkip(env, n=num_frame_skip, stickprob=0.25)\n",
    "    if RESIZE_ENV:\n",
    "        input_shape = (num_stacked_frames*3, *new_size)\n",
    "        env = ResizeObservation(env, new_size)\n",
    "    else:\n",
    "        input_shape = (num_stacked_frames*3, 224, 320)\n",
    "    if max_episode_steps is not None:\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "    return env, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37280243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "step n=5 with reward 0.0\n",
      "step n=10 with reward 0.0\n",
      "step n=15 with reward 0.0\n",
      "step n=20 with reward 1.897133231163025\n",
      "step n=25 with reward 5.691399693489075\n",
      "step n=30 with reward 11.38279938697815\n",
      "step n=35 with reward 20.868465423583984\n",
      "step n=40 with reward 27.508431315422058\n",
      "step n=45 with reward 27.508431315422058\n",
      "step n=50 with reward 24.662731647491455\n",
      "step n=55 with reward 22.76559853553772\n",
      "step n=60 with reward 19.919898867607117\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.nda",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with reward \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m     temp_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 28\u001b[0m   \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_vect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (episode\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mstep n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(frame_count\u001b[38;5;241m-\u001b[39mframe_count_prev)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mreward \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_reward\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 57\u001b[0m, in \u001b[0;36mConvD3QNAgent.replay_vect\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m     next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_online(next_states_tensor)\n\u001b[1;32m     56\u001b[0m     max_next_q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(next_q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 57\u001b[0m     max_Q_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, max_next_q)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     58\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m rewards_tensor \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m max_Q_next \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m~\u001b[39mdones_tensor)\n\u001b[1;32m     59\u001b[0m current_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_online(states_tensor)\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[37], line 29\u001b[0m, in \u001b[0;36mConvD3QN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 29\u001b[0m     conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m     advantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance_stream(conv_out)\n\u001b[1;32m     31\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_stream(conv_out)\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.nda"
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "#venv = VecTransposeImage(VecFrameStack(SubprocVecEnv([make_env] * 8), n_stack=4))\n",
    "agent = ConvD3QNAgent(input_shape, action_dim, lr=0.001, gamma=0.99, epsilon=0.05, epsilon_decay=0.99, buffer_size=10000)\n",
    "if LOAD_MODEL:\n",
    "  agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "\n",
    "temp_reward = 0\n",
    "frame_count_prev = 0\n",
    "frame_count = 0\n",
    "for episode in range(num_episodes):\n",
    "  state, info = env.reset()\n",
    "  total_reward = 0\n",
    "  done = False\n",
    "  while not done:\n",
    "    frame_count += 1\n",
    "    action = agent.act(state = state)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    agent.remember(state, action, reward, observation, done)\n",
    "    state = observation\n",
    "    total_reward += reward\n",
    "    temp_reward += reward\n",
    "    if frame_count % 5 == 0:\n",
    "      print(f'step n={frame_count} with reward {temp_reward}')\n",
    "      temp_reward = 0\n",
    "    agent.replay_vect(batch_size)\n",
    "\n",
    "  if (episode+1) % 2 == 0:\n",
    "    print(f'Episode {episode+1} \\nstep n={(frame_count-frame_count_prev)/4}\\nreward {temp_reward/4}\\n')\n",
    "    temp_reward = 0\n",
    "    frame_count_prev=frame_count\n",
    "  if (episode+1) % 10 == 0:\n",
    "    save_model(agent, episode)\n",
    "env.close()\n",
    "print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee4d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/seba/Documentos/AI/RL/Video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3676/3828671753.py:13: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv_layers.0.weight',\n",
       "              tensor([[[[ 2.4869e-03, -1.7304e-02,  1.2540e-02,  ...,  2.9088e-03,\n",
       "                         -1.7215e-02,  3.4750e-02],\n",
       "                        [-1.0479e-02, -2.5742e-02, -2.7437e-02,  ...,  3.3137e-02,\n",
       "                          1.7395e-02,  3.2747e-02],\n",
       "                        [ 9.4973e-03, -3.0751e-02, -3.5344e-02,  ...,  1.7550e-03,\n",
       "                          1.7423e-02, -3.2122e-02],\n",
       "                        ...,\n",
       "                        [ 1.0678e-03,  3.5334e-02, -1.3512e-02,  ...,  4.0269e-03,\n",
       "                         -3.2342e-02, -3.5242e-02],\n",
       "                        [-6.4955e-03, -2.3027e-02, -2.9597e-02,  ...,  1.9529e-02,\n",
       "                          2.0288e-02,  1.2679e-02],\n",
       "                        [-3.1549e-02,  7.4906e-03,  2.9042e-02,  ...,  1.2957e-02,\n",
       "                          8.9327e-03,  3.3879e-04]],\n",
       "              \n",
       "                       [[-1.9579e-02, -9.2094e-03,  2.2475e-02,  ..., -3.4654e-03,\n",
       "                         -7.4001e-03, -4.8176e-03],\n",
       "                        [-3.0289e-02, -3.1477e-02, -2.5110e-02,  ...,  1.5048e-02,\n",
       "                          1.2629e-02,  2.7573e-02],\n",
       "                        [-2.0699e-02,  3.5545e-03,  2.2930e-02,  ...,  9.3519e-03,\n",
       "                          5.3810e-03,  1.2089e-02],\n",
       "                        ...,\n",
       "                        [ 3.2972e-02, -1.9119e-02, -2.3284e-02,  ..., -1.0059e-02,\n",
       "                          9.5553e-03, -9.9675e-03],\n",
       "                        [-1.6420e-02,  1.6367e-03,  2.9639e-02,  ...,  2.9250e-02,\n",
       "                         -9.7481e-03, -2.1963e-02],\n",
       "                        [-2.4727e-02, -3.4393e-04, -1.4653e-02,  ...,  4.0571e-03,\n",
       "                          1.1706e-02,  6.2693e-03]],\n",
       "              \n",
       "                       [[-1.4790e-02,  2.4467e-02, -1.1263e-02,  ..., -4.4884e-03,\n",
       "                         -4.3871e-03, -2.4252e-02],\n",
       "                        [ 1.2180e-02,  1.0221e-02, -3.1885e-02,  ..., -1.8705e-03,\n",
       "                          3.3310e-04, -1.0916e-02],\n",
       "                        [ 7.2919e-03,  2.9886e-02, -3.0693e-02,  ..., -2.0235e-02,\n",
       "                         -3.1958e-02,  2.5743e-02],\n",
       "                        ...,\n",
       "                        [-2.1881e-03,  2.6054e-02,  1.4920e-02,  ..., -7.8105e-03,\n",
       "                          1.0064e-02, -2.7851e-03],\n",
       "                        [ 3.1283e-02, -3.3173e-02, -1.7785e-02,  ..., -7.6612e-03,\n",
       "                         -1.5842e-02,  1.5807e-02],\n",
       "                        [ 1.3035e-02, -2.6906e-02, -3.1756e-02,  ...,  2.6457e-04,\n",
       "                         -2.4745e-02, -2.8798e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.1221e-04,  2.0677e-02, -2.8612e-02,  ...,  2.6688e-02,\n",
       "                         -3.9222e-03, -1.0467e-03],\n",
       "                        [-3.7692e-03, -2.5017e-02, -4.8611e-03,  ...,  3.1064e-03,\n",
       "                          5.6476e-03, -2.5664e-02],\n",
       "                        [ 2.6739e-02, -1.4775e-02,  2.0029e-02,  ..., -2.2123e-02,\n",
       "                         -1.1488e-02,  3.2267e-02],\n",
       "                        ...,\n",
       "                        [ 4.9566e-03,  2.1852e-02,  1.5409e-02,  ...,  2.2445e-02,\n",
       "                         -1.3876e-02, -3.7428e-03],\n",
       "                        [-6.9345e-03,  2.3799e-02,  1.3355e-02,  ...,  1.5394e-02,\n",
       "                         -2.9672e-02,  2.7879e-02],\n",
       "                        [-1.1091e-02,  2.8526e-02, -2.8809e-02,  ..., -3.5874e-02,\n",
       "                         -8.3120e-03,  5.1485e-03]],\n",
       "              \n",
       "                       [[-1.4276e-02,  9.3993e-03,  3.3004e-02,  ...,  1.7355e-02,\n",
       "                          3.3002e-02, -5.8478e-03],\n",
       "                        [ 1.9118e-02,  8.1695e-03,  3.2877e-02,  ...,  8.9090e-03,\n",
       "                          3.3411e-02, -2.4617e-02],\n",
       "                        [-1.4633e-02,  1.0160e-02,  1.4981e-02,  ..., -3.4418e-02,\n",
       "                          1.7096e-02,  4.8020e-03],\n",
       "                        ...,\n",
       "                        [ 3.1604e-02, -3.4066e-02, -1.4486e-02,  ...,  3.3332e-02,\n",
       "                         -2.2855e-02,  2.4385e-02],\n",
       "                        [ 1.9622e-02, -3.2386e-02, -6.2376e-03,  ...,  5.8371e-03,\n",
       "                         -2.0601e-02, -2.6460e-02],\n",
       "                        [ 2.2802e-02, -1.2204e-02,  1.0281e-02,  ..., -9.9367e-03,\n",
       "                          1.6571e-02,  2.5914e-02]],\n",
       "              \n",
       "                       [[ 2.3009e-02,  1.2974e-02, -2.5753e-02,  ...,  6.8866e-03,\n",
       "                          3.5372e-02, -3.3113e-02],\n",
       "                        [ 1.8685e-02, -1.6297e-02, -2.8341e-02,  ...,  1.0406e-02,\n",
       "                          1.6612e-02, -2.8807e-02],\n",
       "                        [-7.9357e-03,  6.2275e-03,  9.9420e-03,  ..., -2.5294e-02,\n",
       "                          2.6028e-02,  8.9572e-03],\n",
       "                        ...,\n",
       "                        [ 2.3025e-02, -2.6035e-02,  2.4939e-02,  ..., -1.0637e-02,\n",
       "                         -1.8326e-02, -5.1996e-03],\n",
       "                        [ 2.5312e-02, -1.8400e-02, -2.3351e-02,  ..., -3.3103e-02,\n",
       "                          3.2268e-02,  2.4318e-02],\n",
       "                        [-2.6392e-02, -3.3741e-02,  2.3377e-02,  ..., -1.7956e-04,\n",
       "                         -2.5135e-02, -1.3332e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.7374e-03, -6.1391e-03, -3.2799e-02,  ...,  7.4891e-03,\n",
       "                         -2.0936e-02, -2.4344e-02],\n",
       "                        [ 9.4250e-04,  3.0419e-03, -4.9319e-03,  ..., -6.3244e-04,\n",
       "                         -3.0628e-02, -3.5655e-02],\n",
       "                        [-9.6662e-03, -8.1552e-03, -2.6708e-02,  ..., -3.5179e-04,\n",
       "                         -1.4440e-02, -5.0663e-04],\n",
       "                        ...,\n",
       "                        [ 3.3150e-02, -1.1983e-02,  1.0088e-04,  ...,  3.5839e-03,\n",
       "                         -7.7770e-03, -2.4012e-02],\n",
       "                        [-1.9898e-02, -1.3717e-03,  1.6433e-02,  ...,  9.1813e-03,\n",
       "                         -2.2755e-02, -6.4920e-03],\n",
       "                        [-2.9875e-02,  3.3555e-02,  2.9738e-02,  ..., -3.3142e-02,\n",
       "                          2.2555e-03,  1.9948e-02]],\n",
       "              \n",
       "                       [[-3.1662e-02, -1.2033e-02,  1.6304e-02,  ...,  2.2667e-02,\n",
       "                         -1.4985e-02, -1.1188e-03],\n",
       "                        [ 5.8021e-04, -3.3215e-02, -3.2645e-02,  ..., -1.5348e-02,\n",
       "                          1.5256e-02, -2.5210e-02],\n",
       "                        [ 2.7830e-02,  3.1647e-02,  2.9248e-02,  ...,  2.0647e-03,\n",
       "                          1.6219e-02,  8.4292e-03],\n",
       "                        ...,\n",
       "                        [-1.6544e-02, -5.8644e-03, -1.3508e-02,  ..., -3.5116e-02,\n",
       "                         -3.0430e-02, -2.1904e-02],\n",
       "                        [-1.0336e-02, -9.5093e-03, -3.3765e-02,  ...,  4.5723e-03,\n",
       "                         -1.9634e-02, -9.2398e-03],\n",
       "                        [-1.7894e-02, -2.7770e-02,  1.1892e-03,  ...,  1.8031e-02,\n",
       "                          1.2905e-03,  2.3499e-02]],\n",
       "              \n",
       "                       [[ 5.8412e-03, -2.0230e-02,  3.5351e-02,  ...,  1.4346e-02,\n",
       "                          3.4851e-02,  3.1754e-02],\n",
       "                        [-9.1771e-03, -3.0192e-02,  1.1127e-02,  ..., -2.4061e-02,\n",
       "                          6.4528e-04, -1.6840e-02],\n",
       "                        [-1.0554e-02,  2.1599e-02,  3.5922e-02,  ..., -1.8743e-02,\n",
       "                          3.5272e-02,  2.1802e-02],\n",
       "                        ...,\n",
       "                        [ 2.5086e-02, -2.9229e-02,  2.6698e-02,  ..., -2.6632e-02,\n",
       "                          6.0013e-03, -1.8475e-02],\n",
       "                        [ 2.0684e-02, -3.3485e-02, -4.8028e-03,  ..., -2.6020e-02,\n",
       "                         -2.2796e-02,  2.3473e-02],\n",
       "                        [ 6.5025e-03, -5.5118e-03,  2.5428e-02,  ...,  1.6428e-02,\n",
       "                          3.5692e-02,  1.9225e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.9221e-02,  1.2791e-02,  1.5223e-02,  ...,  6.6700e-03,\n",
       "                         -9.6202e-03,  3.4698e-02],\n",
       "                        [-1.3312e-02,  3.3477e-02,  3.6043e-02,  ...,  3.5626e-02,\n",
       "                          3.6764e-03, -7.6444e-03],\n",
       "                        [ 8.3626e-03, -2.1318e-02, -1.4127e-02,  ...,  1.6002e-02,\n",
       "                          5.2122e-03,  9.6777e-03],\n",
       "                        ...,\n",
       "                        [ 3.3378e-03,  9.9635e-03,  2.2701e-02,  ...,  2.9724e-03,\n",
       "                         -2.3389e-02,  1.2943e-02],\n",
       "                        [ 4.4557e-04, -1.8490e-02, -2.8921e-02,  ...,  3.0850e-02,\n",
       "                         -3.5279e-02,  3.4687e-02],\n",
       "                        [-1.7933e-02, -3.5804e-02, -1.8669e-02,  ...,  2.4220e-02,\n",
       "                          2.7973e-02,  2.8770e-02]],\n",
       "              \n",
       "                       [[ 3.0938e-02,  3.5802e-02, -1.9065e-02,  ..., -1.2631e-02,\n",
       "                         -2.6520e-02, -2.4820e-02],\n",
       "                        [ 3.7604e-03,  4.0238e-03, -2.1018e-04,  ..., -2.0499e-02,\n",
       "                         -4.3840e-03, -2.4510e-02],\n",
       "                        [ 3.1126e-02,  3.3939e-02,  1.4753e-02,  ...,  1.7959e-02,\n",
       "                          8.4796e-03, -3.2627e-02],\n",
       "                        ...,\n",
       "                        [ 2.6777e-02, -3.3201e-02, -1.1072e-02,  ...,  6.5362e-03,\n",
       "                          1.2538e-02,  2.8086e-02],\n",
       "                        [ 3.4691e-02, -2.9765e-02, -2.3030e-02,  ...,  2.3694e-02,\n",
       "                          2.2333e-02,  4.2674e-03],\n",
       "                        [-1.0406e-02,  3.4111e-02, -4.4400e-04,  ...,  2.7745e-02,\n",
       "                         -8.2327e-03,  3.0977e-02]],\n",
       "              \n",
       "                       [[-3.4200e-02, -2.0958e-02,  5.5765e-04,  ...,  2.8515e-02,\n",
       "                         -2.3042e-02, -2.5133e-03],\n",
       "                        [ 1.3740e-02, -1.9983e-02, -1.6752e-02,  ...,  2.4921e-02,\n",
       "                         -2.3999e-02, -3.0242e-02],\n",
       "                        [-1.3383e-02, -2.5916e-02,  2.4693e-02,  ..., -1.3943e-02,\n",
       "                          2.6917e-02,  1.8967e-02],\n",
       "                        ...,\n",
       "                        [-1.1544e-02, -2.6957e-02, -2.5909e-02,  ...,  2.5861e-02,\n",
       "                          2.2936e-02,  2.9306e-02],\n",
       "                        [ 1.4081e-02,  2.0675e-02,  7.3275e-03,  ...,  3.0479e-02,\n",
       "                          2.4448e-02, -7.0546e-03],\n",
       "                        [ 5.5511e-03,  1.2144e-03, -5.9217e-04,  ...,  2.3956e-02,\n",
       "                          2.8923e-02,  1.1628e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.2480e-02,  1.1207e-02,  2.0882e-02,  ...,  1.3017e-02,\n",
       "                         -3.1294e-02, -1.6907e-03],\n",
       "                        [ 1.6474e-02,  2.2187e-02, -1.7648e-02,  ..., -2.5664e-02,\n",
       "                          1.0667e-02,  1.4332e-02],\n",
       "                        [ 3.2340e-02, -1.0125e-02,  1.9078e-02,  ...,  1.1028e-03,\n",
       "                          2.1839e-02,  1.2208e-03],\n",
       "                        ...,\n",
       "                        [-1.0913e-02, -1.2510e-02,  2.6204e-02,  ...,  6.5689e-03,\n",
       "                          9.4773e-05, -3.4358e-02],\n",
       "                        [-2.8833e-03,  3.1157e-02, -1.3052e-02,  ...,  7.5390e-03,\n",
       "                         -8.1419e-03, -7.0975e-03],\n",
       "                        [-3.9034e-03,  1.5389e-04,  4.1149e-03,  ...,  1.1603e-02,\n",
       "                          1.3702e-02, -9.1862e-03]],\n",
       "              \n",
       "                       [[ 3.4023e-02, -3.1524e-02, -8.8893e-03,  ...,  2.0344e-02,\n",
       "                         -2.5478e-02, -1.3773e-03],\n",
       "                        [ 3.3628e-02,  2.0425e-03, -1.2336e-02,  ..., -2.1690e-02,\n",
       "                          2.5906e-02,  1.3500e-02],\n",
       "                        [-1.3887e-02, -9.5642e-03, -2.7958e-02,  ..., -3.1923e-02,\n",
       "                         -2.6587e-02, -1.8597e-02],\n",
       "                        ...,\n",
       "                        [-2.1028e-02,  4.7196e-04,  1.3654e-02,  ...,  3.0337e-02,\n",
       "                         -2.2962e-02,  2.7527e-02],\n",
       "                        [ 3.5827e-02,  1.3387e-02,  1.8394e-02,  ...,  1.8479e-02,\n",
       "                         -3.3396e-03,  3.4392e-02],\n",
       "                        [-9.0275e-03,  8.3507e-03,  5.0049e-03,  ...,  7.9759e-03,\n",
       "                         -2.2244e-02, -1.3455e-02]],\n",
       "              \n",
       "                       [[-3.3470e-02,  2.6700e-02, -2.8560e-02,  ...,  6.2062e-03,\n",
       "                         -6.8888e-03,  3.9823e-03],\n",
       "                        [-8.8751e-03, -1.2653e-02,  2.4728e-02,  ..., -3.5264e-02,\n",
       "                          2.2491e-02,  3.1078e-02],\n",
       "                        [-1.0941e-02, -3.1313e-02, -1.2583e-02,  ..., -1.0178e-02,\n",
       "                         -1.7580e-02,  6.9932e-03],\n",
       "                        ...,\n",
       "                        [ 3.6663e-03, -1.9319e-02, -1.6205e-02,  ...,  2.6604e-02,\n",
       "                         -1.1530e-02,  2.6510e-02],\n",
       "                        [-3.0445e-04,  3.4838e-02, -1.1105e-03,  ...,  2.3061e-02,\n",
       "                          1.3128e-02,  3.4923e-02],\n",
       "                        [-2.4529e-02, -5.7253e-04, -9.9248e-03,  ..., -3.5096e-02,\n",
       "                         -1.7555e-02,  5.5280e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4279e-02,  9.4375e-03,  6.0211e-03,  ...,  2.3173e-02,\n",
       "                          8.2173e-03, -1.3116e-02],\n",
       "                        [-1.8012e-02,  3.5159e-02,  2.3963e-02,  ..., -3.8580e-03,\n",
       "                          1.6631e-02, -2.2284e-02],\n",
       "                        [ 4.9008e-03,  1.2515e-02,  1.8637e-02,  ...,  2.3776e-02,\n",
       "                         -6.7916e-03,  3.2280e-02],\n",
       "                        ...,\n",
       "                        [ 3.3096e-02, -1.7067e-02, -1.6127e-02,  ..., -3.2774e-02,\n",
       "                         -2.6043e-02, -2.1617e-02],\n",
       "                        [ 8.0013e-03,  3.6006e-03, -1.3029e-02,  ..., -2.0259e-02,\n",
       "                          7.8752e-03, -1.3894e-02],\n",
       "                        [ 1.5113e-02, -3.5559e-02, -3.5999e-02,  ...,  2.5164e-02,\n",
       "                         -2.5778e-02, -5.1966e-03]],\n",
       "              \n",
       "                       [[-2.2169e-02, -2.4288e-03,  2.8993e-03,  ..., -3.4366e-02,\n",
       "                          1.8295e-02,  2.4935e-02],\n",
       "                        [ 3.2894e-02,  3.8189e-03,  2.1953e-02,  ..., -2.2138e-02,\n",
       "                          1.9878e-02, -1.1589e-02],\n",
       "                        [-2.2045e-02, -1.1961e-02,  1.3411e-02,  ..., -1.9536e-02,\n",
       "                         -1.7921e-03, -2.4327e-02],\n",
       "                        ...,\n",
       "                        [ 1.8048e-02,  2.9065e-02, -2.7888e-02,  ..., -2.7265e-03,\n",
       "                          3.4606e-03,  3.0448e-02],\n",
       "                        [-2.9824e-02,  3.1282e-02, -4.9576e-03,  ..., -2.3500e-02,\n",
       "                          1.9877e-02,  1.7532e-02],\n",
       "                        [-9.3945e-03, -2.4438e-02, -1.9956e-02,  ...,  1.5120e-02,\n",
       "                          3.2489e-02,  1.8644e-02]],\n",
       "              \n",
       "                       [[-1.6062e-02,  3.4560e-02,  2.4414e-02,  ...,  3.3373e-02,\n",
       "                          1.7937e-02, -2.6010e-02],\n",
       "                        [-4.1084e-03, -3.5412e-02,  2.4486e-02,  ...,  9.0781e-03,\n",
       "                         -1.3705e-02, -1.5899e-02],\n",
       "                        [-4.0851e-03, -9.1054e-03,  1.2951e-02,  ..., -2.8125e-02,\n",
       "                         -5.1640e-03,  2.1002e-02],\n",
       "                        ...,\n",
       "                        [-2.2252e-02, -3.2583e-02, -2.4971e-02,  ...,  2.6763e-02,\n",
       "                          2.4291e-02,  3.3413e-02],\n",
       "                        [ 2.4977e-03, -3.2473e-02, -1.9937e-02,  ...,  2.1591e-02,\n",
       "                          5.7161e-03, -5.1786e-03],\n",
       "                        [-5.8470e-03,  3.2643e-02, -2.5552e-04,  ..., -1.2504e-02,\n",
       "                         -2.2639e-02,  2.3934e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 2.2508e-02, -3.0658e-02,  1.6965e-02,  ..., -1.5669e-03,\n",
       "                         -2.7823e-02, -3.4858e-02],\n",
       "                        [ 3.5296e-02,  5.0355e-04, -1.9063e-02,  ..., -3.0142e-02,\n",
       "                          1.4594e-02,  1.4681e-02],\n",
       "                        [ 1.8656e-02,  3.5809e-02, -2.4696e-02,  ..., -3.0207e-02,\n",
       "                         -1.6125e-02,  1.6497e-02],\n",
       "                        ...,\n",
       "                        [-1.0844e-03, -2.4942e-02,  2.0596e-02,  ...,  2.8139e-02,\n",
       "                         -3.6057e-02,  6.8325e-03],\n",
       "                        [-2.6681e-02, -1.8837e-02,  1.3072e-02,  ...,  3.4849e-02,\n",
       "                         -1.3446e-02,  2.9997e-02],\n",
       "                        [-1.3854e-02,  1.0892e-03, -7.7214e-03,  ...,  1.6582e-02,\n",
       "                          1.4732e-02,  3.6009e-02]],\n",
       "              \n",
       "                       [[ 2.4022e-02, -1.4965e-02, -2.5532e-02,  ..., -3.0453e-02,\n",
       "                         -1.1134e-02, -2.4456e-02],\n",
       "                        [ 3.1362e-02, -1.2239e-03,  7.6856e-03,  ...,  3.5783e-02,\n",
       "                         -3.3265e-02, -8.1276e-03],\n",
       "                        [-3.5116e-02, -3.1768e-02, -2.7653e-02,  ..., -3.0784e-02,\n",
       "                         -3.2402e-02, -6.0233e-03],\n",
       "                        ...,\n",
       "                        [ 9.3252e-03, -2.3022e-02,  1.2216e-02,  ..., -7.4327e-03,\n",
       "                         -2.1339e-02,  1.1934e-02],\n",
       "                        [-3.5454e-02,  3.5501e-02, -3.5293e-02,  ...,  1.9186e-02,\n",
       "                          4.3255e-03,  4.6109e-03],\n",
       "                        [-2.2994e-02, -3.0045e-02, -1.3635e-02,  ...,  1.7213e-02,\n",
       "                         -7.2851e-03,  1.4313e-02]],\n",
       "              \n",
       "                       [[ 2.2375e-03,  4.7533e-05, -8.8232e-03,  ...,  1.7264e-02,\n",
       "                         -2.0003e-02, -3.5179e-02],\n",
       "                        [-1.6112e-02, -1.8604e-03,  6.6070e-03,  ..., -2.2331e-02,\n",
       "                         -9.5843e-03,  1.4088e-02],\n",
       "                        [ 9.3734e-03, -2.9669e-02, -1.9453e-02,  ..., -1.1564e-02,\n",
       "                          3.0673e-02,  2.1336e-02],\n",
       "                        ...,\n",
       "                        [ 2.8993e-03, -6.0495e-04,  2.0821e-02,  ...,  3.9555e-04,\n",
       "                          1.7247e-02, -1.0286e-02],\n",
       "                        [-2.5106e-02,  5.8028e-03, -2.3269e-02,  ..., -3.4807e-03,\n",
       "                          4.7297e-03,  3.2512e-02],\n",
       "                        [-1.7460e-02, -1.3757e-02,  1.9971e-02,  ..., -2.0941e-02,\n",
       "                         -1.7991e-02, -1.0103e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-8.9521e-03, -2.3753e-02, -2.9319e-02,  ...,  9.2343e-03,\n",
       "                          2.6987e-02,  2.0598e-02],\n",
       "                        [ 2.2799e-02, -2.2165e-02, -3.5248e-03,  ..., -3.1989e-02,\n",
       "                         -3.4679e-02,  3.2080e-02],\n",
       "                        [-2.1590e-02, -1.5777e-04,  2.7349e-02,  ..., -7.8014e-03,\n",
       "                         -2.2713e-02,  2.0009e-02],\n",
       "                        ...,\n",
       "                        [ 2.3473e-02,  3.0516e-02,  1.4102e-02,  ...,  2.2588e-02,\n",
       "                          1.6038e-02,  2.6507e-02],\n",
       "                        [ 6.4447e-03,  3.5229e-02, -1.4568e-03,  ..., -3.5767e-02,\n",
       "                         -2.4154e-02, -2.9085e-02],\n",
       "                        [ 1.7440e-02, -1.5406e-02, -9.6481e-03,  ...,  1.0724e-02,\n",
       "                         -3.3850e-02,  2.5116e-02]],\n",
       "              \n",
       "                       [[ 2.9815e-02,  2.3063e-02, -2.3264e-02,  ...,  3.5878e-02,\n",
       "                         -2.4478e-02,  2.5775e-02],\n",
       "                        [-3.3799e-03, -5.9855e-03,  2.5894e-02,  ..., -2.4705e-02,\n",
       "                          1.4715e-03,  1.4901e-02],\n",
       "                        [-3.5022e-02, -2.8908e-02, -2.7418e-02,  ...,  4.2331e-03,\n",
       "                          5.2165e-03, -2.5397e-02],\n",
       "                        ...,\n",
       "                        [ 1.9031e-02,  3.3499e-03, -2.5893e-02,  ...,  2.2579e-02,\n",
       "                         -1.5001e-02,  2.7381e-02],\n",
       "                        [ 2.7185e-02, -1.6040e-02,  2.8380e-02,  ..., -2.2591e-02,\n",
       "                         -3.0712e-02, -1.7905e-02],\n",
       "                        [ 1.7906e-02,  9.2091e-03, -1.9838e-03,  ...,  3.4041e-03,\n",
       "                          1.2924e-02,  2.8157e-03]],\n",
       "              \n",
       "                       [[-2.6590e-02, -3.1181e-02, -2.4747e-02,  ..., -2.2821e-02,\n",
       "                         -1.5582e-02,  2.6338e-02],\n",
       "                        [ 9.7225e-03, -2.4967e-02,  2.2827e-02,  ..., -1.9706e-02,\n",
       "                          2.4704e-02,  2.9173e-02],\n",
       "                        [ 3.1752e-02,  7.2445e-03,  8.0875e-03,  ...,  1.4651e-02,\n",
       "                          1.7407e-02,  1.9383e-02],\n",
       "                        ...,\n",
       "                        [ 1.1249e-02,  7.3848e-03,  6.5656e-03,  ..., -4.5359e-03,\n",
       "                          2.6298e-02, -3.1926e-05],\n",
       "                        [ 1.7644e-02,  3.4942e-02, -1.5329e-02,  ...,  4.3686e-03,\n",
       "                         -2.7926e-02,  1.5004e-02],\n",
       "                        [-9.3772e-03,  4.5864e-03, -2.5903e-02,  ..., -1.7953e-04,\n",
       "                         -5.6333e-03, -2.8355e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.0058e-03,  2.3950e-02,  6.4427e-03,  ..., -1.8577e-02,\n",
       "                          8.7242e-03, -7.6551e-03],\n",
       "                        [-1.1130e-02,  2.3790e-02, -3.1548e-02,  ...,  2.5586e-02,\n",
       "                          1.2530e-02,  2.3491e-02],\n",
       "                        [ 3.5115e-02, -1.4682e-02, -1.8404e-02,  ...,  3.0210e-02,\n",
       "                          9.9488e-03,  1.0232e-04],\n",
       "                        ...,\n",
       "                        [-1.9865e-02, -1.2138e-02,  2.0768e-02,  ...,  2.0502e-02,\n",
       "                          6.1630e-03,  3.4589e-02],\n",
       "                        [-1.4895e-02, -2.0951e-02, -2.7174e-02,  ..., -1.8801e-02,\n",
       "                          1.7658e-02, -1.6487e-02],\n",
       "                        [-6.7592e-03,  1.7056e-02,  2.5507e-02,  ...,  1.6943e-02,\n",
       "                          1.6289e-02, -2.3124e-02]],\n",
       "              \n",
       "                       [[-2.4524e-02,  2.0058e-02, -1.5957e-02,  ..., -1.7377e-02,\n",
       "                          1.6012e-03, -3.3439e-02],\n",
       "                        [-6.8720e-03, -2.2942e-02,  6.1473e-03,  ...,  2.3069e-02,\n",
       "                          1.6871e-02,  3.1474e-02],\n",
       "                        [-1.8293e-02, -1.1454e-02, -2.3260e-02,  ...,  9.8560e-03,\n",
       "                         -3.6582e-03, -2.9800e-02],\n",
       "                        ...,\n",
       "                        [ 1.6336e-02, -1.0352e-02,  3.0839e-02,  ..., -1.3763e-02,\n",
       "                         -1.4355e-02, -1.5521e-02],\n",
       "                        [-1.7217e-03,  1.6422e-02,  1.7817e-04,  ..., -1.5519e-02,\n",
       "                          3.6325e-03,  3.5255e-02],\n",
       "                        [-1.3463e-02, -2.3755e-02, -1.4932e-02,  ..., -2.1030e-02,\n",
       "                         -2.2707e-02,  1.6693e-02]],\n",
       "              \n",
       "                       [[-1.3298e-02,  3.5934e-03,  1.1943e-02,  ...,  1.4703e-02,\n",
       "                         -2.6676e-02,  1.2783e-02],\n",
       "                        [-1.0372e-02, -3.0770e-02, -1.6373e-02,  ...,  6.2786e-04,\n",
       "                         -9.3979e-03,  2.2969e-02],\n",
       "                        [-1.3408e-02, -2.9644e-02, -1.8884e-02,  ...,  3.1838e-02,\n",
       "                         -3.7533e-03,  3.2929e-02],\n",
       "                        ...,\n",
       "                        [ 5.4838e-03, -1.3456e-02,  3.4066e-02,  ...,  3.2077e-03,\n",
       "                          2.1858e-02, -5.7396e-03],\n",
       "                        [-2.8038e-02,  1.6149e-02, -2.4941e-02,  ...,  1.8488e-02,\n",
       "                          1.5347e-02,  9.7469e-03],\n",
       "                        [-5.5660e-03, -3.1592e-02, -3.3992e-02,  ...,  2.7974e-02,\n",
       "                          3.4024e-02, -8.0902e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.6714e-02,  4.5014e-04, -1.6581e-02,  ...,  4.7467e-03,\n",
       "                          7.7756e-03,  3.1855e-02],\n",
       "                        [-2.2212e-02, -1.2949e-02, -1.6207e-03,  ...,  1.6131e-02,\n",
       "                          3.1174e-02,  3.0702e-02],\n",
       "                        [ 2.1571e-02,  1.7807e-02, -3.5833e-02,  ...,  2.9651e-02,\n",
       "                          1.5422e-03, -2.5679e-02],\n",
       "                        ...,\n",
       "                        [-1.9346e-02, -2.8098e-02,  3.1390e-02,  ..., -1.0595e-02,\n",
       "                          1.4621e-02,  2.9297e-02],\n",
       "                        [ 2.4745e-02, -3.2209e-03,  2.9843e-02,  ...,  3.4597e-02,\n",
       "                          1.5309e-02,  3.2447e-02],\n",
       "                        [-4.6507e-03,  1.7458e-02,  7.3582e-03,  ..., -5.9568e-03,\n",
       "                         -4.8756e-04, -3.5843e-02]],\n",
       "              \n",
       "                       [[-1.8520e-02, -1.2891e-02, -6.6745e-03,  ..., -8.8699e-03,\n",
       "                          3.2107e-02,  2.0689e-02],\n",
       "                        [ 1.6243e-02, -1.8100e-02,  4.1363e-03,  ...,  2.9096e-02,\n",
       "                          1.2504e-02, -2.7223e-02],\n",
       "                        [-1.2975e-02, -1.1064e-02, -2.4329e-02,  ..., -1.8901e-02,\n",
       "                         -2.7738e-02, -1.3285e-02],\n",
       "                        ...,\n",
       "                        [-1.4864e-02, -3.1541e-02,  1.3256e-02,  ..., -2.8113e-02,\n",
       "                         -3.4213e-02, -2.7619e-02],\n",
       "                        [ 2.6840e-02, -1.7602e-02,  2.5867e-02,  ..., -1.8799e-02,\n",
       "                         -2.2497e-02,  4.3170e-03],\n",
       "                        [ 1.1413e-02,  1.6658e-03,  2.3681e-02,  ...,  2.6456e-02,\n",
       "                          2.4035e-02,  9.0809e-03]],\n",
       "              \n",
       "                       [[-5.6550e-03, -1.6278e-03, -4.4249e-03,  ...,  2.6044e-02,\n",
       "                         -7.8775e-03,  8.5241e-03],\n",
       "                        [-5.6785e-03, -2.4397e-02, -3.4379e-02,  ...,  3.1422e-03,\n",
       "                         -2.8681e-03, -3.1218e-02],\n",
       "                        [ 2.8276e-03, -1.4060e-02,  1.5544e-02,  ...,  2.8412e-02,\n",
       "                         -3.5160e-02,  2.6805e-02],\n",
       "                        ...,\n",
       "                        [-2.8926e-02, -1.0194e-02, -2.1778e-02,  ..., -2.0419e-03,\n",
       "                          1.5318e-02, -3.5505e-02],\n",
       "                        [-2.1464e-02, -4.4429e-03,  3.2729e-02,  ...,  1.7045e-02,\n",
       "                         -9.7715e-04,  2.1916e-02],\n",
       "                        [-2.6397e-02, -2.3144e-02,  4.6992e-03,  ..., -1.5331e-02,\n",
       "                         -1.5761e-02, -1.5295e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.3201e-03, -1.9530e-02,  1.9156e-02,  ..., -5.7473e-03,\n",
       "                          2.5034e-02, -5.3507e-03],\n",
       "                        [-2.1640e-02, -2.2146e-03, -2.4209e-02,  ..., -5.9038e-03,\n",
       "                          3.4553e-03,  2.5312e-02],\n",
       "                        [ 3.1002e-02, -2.8551e-02,  2.2646e-02,  ..., -2.9103e-02,\n",
       "                          2.6599e-02,  2.2775e-02],\n",
       "                        ...,\n",
       "                        [ 2.2605e-02,  1.3794e-02, -9.8773e-03,  ..., -1.9735e-02,\n",
       "                          2.0206e-02,  3.3044e-02],\n",
       "                        [-5.2061e-03,  1.1946e-02,  2.7195e-03,  ...,  2.1436e-02,\n",
       "                          2.1114e-02, -2.2134e-03],\n",
       "                        [ 2.4405e-03, -9.1089e-04,  1.0781e-02,  ...,  1.8888e-02,\n",
       "                          2.1927e-03, -1.1007e-02]],\n",
       "              \n",
       "                       [[ 3.5094e-02,  2.1490e-02,  6.2723e-03,  ..., -2.6477e-02,\n",
       "                          1.2904e-02, -2.4326e-02],\n",
       "                        [-1.2683e-02, -2.2723e-02,  1.2909e-02,  ...,  1.4769e-02,\n",
       "                         -1.3111e-02, -3.1829e-02],\n",
       "                        [ 5.4422e-03,  2.5303e-02,  7.4841e-03,  ..., -2.4700e-02,\n",
       "                          3.1167e-02, -4.1173e-03],\n",
       "                        ...,\n",
       "                        [-1.0379e-02, -1.6922e-02, -8.0039e-04,  ...,  8.1815e-03,\n",
       "                         -3.0745e-02, -1.8517e-02],\n",
       "                        [ 7.1726e-04,  1.5977e-02,  1.0004e-02,  ..., -1.3464e-03,\n",
       "                         -2.0638e-02, -2.0914e-02],\n",
       "                        [ 2.9317e-02,  3.3381e-02,  3.6040e-02,  ...,  3.1733e-02,\n",
       "                          8.5084e-03, -1.8207e-02]],\n",
       "              \n",
       "                       [[-1.7232e-02, -2.7040e-02, -8.9015e-03,  ..., -2.1428e-02,\n",
       "                         -1.5066e-02, -2.4417e-02],\n",
       "                        [ 2.0288e-02,  3.2709e-02, -8.0745e-04,  ..., -2.9956e-02,\n",
       "                          9.8156e-03, -9.2477e-03],\n",
       "                        [-1.1280e-02,  2.1666e-02, -2.1620e-02,  ...,  2.1307e-02,\n",
       "                         -2.0785e-02,  2.3453e-03],\n",
       "                        ...,\n",
       "                        [-3.3861e-02, -2.7166e-02, -1.7243e-02,  ...,  1.8471e-02,\n",
       "                         -3.0215e-03, -1.8236e-02],\n",
       "                        [ 2.3965e-02, -2.3801e-02,  2.2904e-02,  ...,  8.7141e-03,\n",
       "                          3.2221e-03,  2.7119e-02],\n",
       "                        [ 2.7717e-02, -1.4007e-02, -8.5043e-04,  ..., -2.1173e-02,\n",
       "                         -7.8453e-03,  2.6568e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.9426e-02, -3.7163e-03, -7.9142e-03,  ..., -3.1665e-02,\n",
       "                          2.3423e-02, -9.2287e-03],\n",
       "                        [-1.6218e-02,  1.1732e-02,  1.5309e-02,  ...,  2.4003e-02,\n",
       "                          8.5185e-03,  1.2491e-02],\n",
       "                        [-1.0585e-02, -2.3673e-02,  2.3462e-02,  ...,  1.3839e-02,\n",
       "                         -1.0240e-02,  1.6460e-02],\n",
       "                        ...,\n",
       "                        [-3.5564e-02, -1.6166e-02,  8.0117e-03,  ..., -7.3419e-03,\n",
       "                          3.4584e-02, -3.3839e-02],\n",
       "                        [-2.5155e-02, -2.3876e-02, -8.3906e-03,  ...,  1.0330e-02,\n",
       "                         -1.9553e-02,  1.7855e-02],\n",
       "                        [ 4.9935e-03,  5.2071e-03, -4.0833e-03,  ..., -1.5347e-02,\n",
       "                         -1.7257e-02,  2.3254e-02]],\n",
       "              \n",
       "                       [[ 4.7056e-03, -5.7704e-03, -3.5170e-02,  ..., -7.7272e-03,\n",
       "                         -2.1440e-02, -3.5449e-02],\n",
       "                        [ 2.1992e-02,  1.2708e-02, -1.7355e-02,  ..., -3.2887e-02,\n",
       "                          2.1387e-02,  2.8908e-02],\n",
       "                        [-2.4693e-02,  3.6020e-02,  1.8277e-02,  ..., -2.0280e-02,\n",
       "                         -2.9369e-02,  1.8100e-02],\n",
       "                        ...,\n",
       "                        [ 2.8008e-02,  1.0586e-02, -1.2339e-02,  ..., -5.9457e-03,\n",
       "                         -9.8250e-03, -1.7181e-02],\n",
       "                        [-8.5845e-03, -1.9171e-02,  1.6716e-02,  ..., -3.8175e-03,\n",
       "                          1.2867e-02, -3.1720e-02],\n",
       "                        [ 9.4398e-03,  9.2261e-03,  2.8348e-02,  ..., -1.3281e-03,\n",
       "                         -2.1618e-02,  2.6984e-03]],\n",
       "              \n",
       "                       [[-2.7379e-02, -1.5342e-02, -2.0382e-02,  ...,  1.0081e-02,\n",
       "                          2.0567e-02, -3.4112e-02],\n",
       "                        [ 1.8380e-04,  1.5770e-03,  4.7503e-03,  ...,  1.7106e-02,\n",
       "                          2.4606e-02, -3.5901e-02],\n",
       "                        [ 3.5471e-02, -2.4489e-02,  2.1225e-02,  ..., -4.0168e-03,\n",
       "                          1.6030e-02,  2.7199e-02],\n",
       "                        ...,\n",
       "                        [ 3.4545e-02,  4.2934e-03, -3.1301e-02,  ..., -3.5948e-02,\n",
       "                          5.7445e-04,  2.3420e-03],\n",
       "                        [ 2.7187e-02,  3.5602e-02,  2.0869e-02,  ...,  6.7668e-05,\n",
       "                         -3.3819e-02,  3.3314e-02],\n",
       "                        [-9.5271e-03,  3.1184e-02, -1.6777e-02,  ..., -1.8370e-02,\n",
       "                          2.9195e-02,  3.3673e-02]]]], device='cuda:0')),\n",
       "             ('conv_layers.0.bias',\n",
       "              tensor([ 0.0011, -0.0023,  0.0103,  0.0232,  0.0115, -0.0304,  0.0124,  0.0079,\n",
       "                       0.0237,  0.0059,  0.0174, -0.0186, -0.0334, -0.0259,  0.0281, -0.0295,\n",
       "                       0.0019, -0.0231, -0.0276,  0.0085,  0.0010,  0.0121,  0.0222, -0.0297,\n",
       "                      -0.0119, -0.0288, -0.0124, -0.0128, -0.0096,  0.0307,  0.0260, -0.0133],\n",
       "                     device='cuda:0')),\n",
       "             ('conv_layers.2.weight',\n",
       "              tensor([[[[-0.0031,  0.0044,  0.0317, -0.0103],\n",
       "                        [-0.0045, -0.0016, -0.0239,  0.0267],\n",
       "                        [-0.0071,  0.0057,  0.0233,  0.0331],\n",
       "                        [ 0.0324, -0.0038, -0.0378,  0.0146]],\n",
       "              \n",
       "                       [[-0.0188,  0.0422,  0.0375, -0.0058],\n",
       "                        [ 0.0367,  0.0289,  0.0012, -0.0097],\n",
       "                        [-0.0316, -0.0178, -0.0052,  0.0042],\n",
       "                        [ 0.0183,  0.0271,  0.0238, -0.0131]],\n",
       "              \n",
       "                       [[ 0.0286, -0.0062, -0.0102, -0.0359],\n",
       "                        [ 0.0314,  0.0031,  0.0286, -0.0213],\n",
       "                        [ 0.0390, -0.0276,  0.0313,  0.0242],\n",
       "                        [ 0.0133,  0.0097,  0.0119,  0.0221]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0240, -0.0196,  0.0154, -0.0073],\n",
       "                        [-0.0428, -0.0202,  0.0353,  0.0415],\n",
       "                        [-0.0371, -0.0312, -0.0159, -0.0232],\n",
       "                        [-0.0183,  0.0434,  0.0306, -0.0067]],\n",
       "              \n",
       "                       [[-0.0349, -0.0062,  0.0193,  0.0319],\n",
       "                        [ 0.0357,  0.0413,  0.0118, -0.0137],\n",
       "                        [-0.0105,  0.0016, -0.0201,  0.0287],\n",
       "                        [ 0.0275, -0.0337, -0.0238,  0.0101]],\n",
       "              \n",
       "                       [[-0.0427,  0.0044,  0.0061, -0.0415],\n",
       "                        [-0.0220,  0.0074, -0.0328, -0.0382],\n",
       "                        [ 0.0261, -0.0360,  0.0142, -0.0149],\n",
       "                        [ 0.0236, -0.0251, -0.0061,  0.0049]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0022,  0.0017, -0.0345,  0.0030],\n",
       "                        [-0.0075,  0.0426, -0.0195, -0.0321],\n",
       "                        [ 0.0090,  0.0388,  0.0193,  0.0002],\n",
       "                        [ 0.0222, -0.0363, -0.0119, -0.0107]],\n",
       "              \n",
       "                       [[-0.0088, -0.0360, -0.0248,  0.0238],\n",
       "                        [-0.0435, -0.0164, -0.0018, -0.0330],\n",
       "                        [-0.0051, -0.0087,  0.0016, -0.0413],\n",
       "                        [ 0.0051, -0.0282, -0.0281,  0.0023]],\n",
       "              \n",
       "                       [[ 0.0435,  0.0033, -0.0140, -0.0292],\n",
       "                        [-0.0186, -0.0392,  0.0205, -0.0400],\n",
       "                        [ 0.0427, -0.0058, -0.0245,  0.0289],\n",
       "                        [-0.0197,  0.0223,  0.0121,  0.0005]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0174,  0.0189, -0.0117,  0.0039],\n",
       "                        [ 0.0263,  0.0021,  0.0046, -0.0282],\n",
       "                        [-0.0176,  0.0167, -0.0225, -0.0030],\n",
       "                        [ 0.0424, -0.0432, -0.0394, -0.0386]],\n",
       "              \n",
       "                       [[ 0.0029,  0.0414,  0.0268, -0.0066],\n",
       "                        [-0.0098,  0.0387,  0.0313, -0.0313],\n",
       "                        [ 0.0424,  0.0067, -0.0389, -0.0173],\n",
       "                        [ 0.0116, -0.0278, -0.0299,  0.0102]],\n",
       "              \n",
       "                       [[ 0.0328,  0.0353, -0.0180, -0.0301],\n",
       "                        [ 0.0398,  0.0291, -0.0006,  0.0184],\n",
       "                        [-0.0019, -0.0187,  0.0254,  0.0054],\n",
       "                        [ 0.0103, -0.0112,  0.0325, -0.0171]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0230,  0.0394,  0.0043,  0.0429],\n",
       "                        [-0.0258,  0.0022, -0.0194,  0.0399],\n",
       "                        [ 0.0242,  0.0343, -0.0344, -0.0154],\n",
       "                        [-0.0287,  0.0076,  0.0432, -0.0437]],\n",
       "              \n",
       "                       [[-0.0043,  0.0335,  0.0428,  0.0192],\n",
       "                        [ 0.0188,  0.0237, -0.0237, -0.0115],\n",
       "                        [ 0.0185, -0.0413, -0.0183, -0.0391],\n",
       "                        [-0.0351,  0.0358,  0.0157,  0.0100]],\n",
       "              \n",
       "                       [[-0.0419, -0.0402,  0.0123,  0.0044],\n",
       "                        [ 0.0119,  0.0403,  0.0428,  0.0436],\n",
       "                        [ 0.0235,  0.0101,  0.0252,  0.0346],\n",
       "                        [ 0.0284,  0.0430,  0.0344, -0.0276]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0005,  0.0392, -0.0165, -0.0337],\n",
       "                        [ 0.0048, -0.0271,  0.0133,  0.0131],\n",
       "                        [-0.0404,  0.0323,  0.0205, -0.0422],\n",
       "                        [-0.0034, -0.0241,  0.0278,  0.0266]],\n",
       "              \n",
       "                       [[ 0.0175, -0.0141, -0.0052,  0.0107],\n",
       "                        [ 0.0439,  0.0025,  0.0091, -0.0086],\n",
       "                        [ 0.0008,  0.0136,  0.0327, -0.0069],\n",
       "                        [ 0.0089,  0.0136, -0.0190, -0.0247]],\n",
       "              \n",
       "                       [[-0.0405, -0.0200,  0.0206,  0.0369],\n",
       "                        [ 0.0440,  0.0069,  0.0239,  0.0153],\n",
       "                        [ 0.0045, -0.0098,  0.0077, -0.0282],\n",
       "                        [ 0.0080, -0.0139,  0.0343,  0.0234]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0169, -0.0391, -0.0384, -0.0361],\n",
       "                        [ 0.0111,  0.0271,  0.0047,  0.0086],\n",
       "                        [ 0.0361,  0.0432,  0.0138,  0.0199],\n",
       "                        [ 0.0279, -0.0159,  0.0387, -0.0251]],\n",
       "              \n",
       "                       [[-0.0138, -0.0272, -0.0384,  0.0426],\n",
       "                        [ 0.0277,  0.0431, -0.0300, -0.0238],\n",
       "                        [-0.0243,  0.0157,  0.0021,  0.0397],\n",
       "                        [ 0.0308,  0.0335, -0.0341, -0.0271]],\n",
       "              \n",
       "                       [[ 0.0291, -0.0135, -0.0044, -0.0042],\n",
       "                        [ 0.0129, -0.0126, -0.0439, -0.0369],\n",
       "                        [ 0.0327, -0.0278,  0.0406, -0.0435],\n",
       "                        [ 0.0175, -0.0158,  0.0352,  0.0105]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0393,  0.0326, -0.0230,  0.0358],\n",
       "                        [-0.0163, -0.0201, -0.0150, -0.0057],\n",
       "                        [ 0.0323, -0.0417, -0.0216, -0.0326],\n",
       "                        [-0.0412, -0.0010, -0.0390, -0.0220]],\n",
       "              \n",
       "                       [[ 0.0233, -0.0292,  0.0190,  0.0419],\n",
       "                        [-0.0251,  0.0004, -0.0391, -0.0193],\n",
       "                        [ 0.0179,  0.0432, -0.0018,  0.0102],\n",
       "                        [-0.0238, -0.0139, -0.0049,  0.0090]],\n",
       "              \n",
       "                       [[ 0.0084,  0.0233,  0.0188,  0.0284],\n",
       "                        [ 0.0219, -0.0297, -0.0355,  0.0117],\n",
       "                        [-0.0139, -0.0378, -0.0243, -0.0052],\n",
       "                        [ 0.0405, -0.0075, -0.0139,  0.0020]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0396, -0.0362,  0.0001,  0.0097],\n",
       "                        [-0.0284,  0.0183,  0.0218, -0.0115],\n",
       "                        [-0.0138,  0.0082,  0.0018, -0.0168],\n",
       "                        [-0.0407, -0.0356, -0.0437, -0.0417]],\n",
       "              \n",
       "                       [[ 0.0250, -0.0158, -0.0030,  0.0354],\n",
       "                        [-0.0067, -0.0399,  0.0262,  0.0256],\n",
       "                        [-0.0371,  0.0383, -0.0161, -0.0362],\n",
       "                        [ 0.0436, -0.0244,  0.0407,  0.0091]],\n",
       "              \n",
       "                       [[ 0.0158,  0.0281, -0.0327,  0.0152],\n",
       "                        [-0.0134,  0.0436,  0.0089,  0.0058],\n",
       "                        [ 0.0375, -0.0357,  0.0330,  0.0159],\n",
       "                        [-0.0194,  0.0433,  0.0181,  0.0349]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0097, -0.0172,  0.0396,  0.0105],\n",
       "                        [-0.0376,  0.0429,  0.0296, -0.0333],\n",
       "                        [ 0.0368, -0.0134, -0.0236,  0.0360],\n",
       "                        [-0.0062,  0.0102, -0.0123, -0.0316]],\n",
       "              \n",
       "                       [[-0.0341, -0.0391, -0.0281,  0.0307],\n",
       "                        [-0.0052,  0.0228, -0.0201, -0.0226],\n",
       "                        [ 0.0078,  0.0090,  0.0373,  0.0255],\n",
       "                        [ 0.0365,  0.0029,  0.0177, -0.0276]],\n",
       "              \n",
       "                       [[ 0.0011,  0.0299,  0.0351,  0.0146],\n",
       "                        [-0.0083,  0.0378, -0.0233,  0.0303],\n",
       "                        [ 0.0352,  0.0206, -0.0062, -0.0341],\n",
       "                        [-0.0392, -0.0247,  0.0086,  0.0397]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0111,  0.0062, -0.0359,  0.0202],\n",
       "                        [-0.0420, -0.0157, -0.0121,  0.0163],\n",
       "                        [-0.0048,  0.0115,  0.0035, -0.0090],\n",
       "                        [-0.0429,  0.0386,  0.0133, -0.0392]],\n",
       "              \n",
       "                       [[-0.0323, -0.0383, -0.0341,  0.0118],\n",
       "                        [-0.0437,  0.0038, -0.0269,  0.0140],\n",
       "                        [-0.0035, -0.0289, -0.0063,  0.0137],\n",
       "                        [ 0.0012, -0.0238,  0.0176,  0.0141]],\n",
       "              \n",
       "                       [[ 0.0151, -0.0357,  0.0128, -0.0424],\n",
       "                        [-0.0279, -0.0209, -0.0210,  0.0318],\n",
       "                        [-0.0364, -0.0045,  0.0101, -0.0436],\n",
       "                        [ 0.0175, -0.0300,  0.0053,  0.0414]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0118,  0.0335,  0.0157, -0.0204],\n",
       "                        [ 0.0168, -0.0337,  0.0210,  0.0113],\n",
       "                        [-0.0027,  0.0070, -0.0080, -0.0295],\n",
       "                        [-0.0182,  0.0185, -0.0038,  0.0364]],\n",
       "              \n",
       "                       [[ 0.0248,  0.0036, -0.0173,  0.0324],\n",
       "                        [-0.0413,  0.0187, -0.0315, -0.0406],\n",
       "                        [ 0.0095, -0.0412,  0.0289,  0.0171],\n",
       "                        [-0.0065, -0.0029, -0.0369,  0.0416]],\n",
       "              \n",
       "                       [[-0.0066,  0.0237,  0.0311, -0.0037],\n",
       "                        [-0.0147, -0.0328,  0.0181, -0.0251],\n",
       "                        [-0.0349,  0.0214, -0.0161,  0.0434],\n",
       "                        [-0.0220,  0.0422,  0.0150,  0.0395]]]], device='cuda:0')),\n",
       "             ('conv_layers.2.bias',\n",
       "              tensor([ 0.0208,  0.0075,  0.0244,  0.0181, -0.0056,  0.0203,  0.0424,  0.0126,\n",
       "                      -0.0343, -0.0120,  0.0440,  0.0166,  0.0324, -0.0237,  0.0031, -0.0011,\n",
       "                      -0.0227,  0.0169,  0.0294,  0.0083,  0.0286,  0.0229, -0.0173, -0.0334,\n",
       "                      -0.0275, -0.0149,  0.0375,  0.0065,  0.0316, -0.0360, -0.0241,  0.0039,\n",
       "                      -0.0027, -0.0432, -0.0370, -0.0387, -0.0128,  0.0418, -0.0068, -0.0135,\n",
       "                      -0.0432, -0.0305, -0.0439,  0.0139, -0.0085, -0.0324, -0.0018,  0.0255,\n",
       "                       0.0251,  0.0236,  0.0012, -0.0313,  0.0253,  0.0240, -0.0152,  0.0295,\n",
       "                       0.0007, -0.0410, -0.0009, -0.0320, -0.0413,  0.0071, -0.0059, -0.0061],\n",
       "                     device='cuda:0')),\n",
       "             ('conv_layers.4.weight',\n",
       "              tensor([[[[ 0.0127, -0.0504],\n",
       "                        [ 0.0578,  0.0478]],\n",
       "              \n",
       "                       [[-0.0092, -0.0128],\n",
       "                        [ 0.0100, -0.0030]],\n",
       "              \n",
       "                       [[-0.0147,  0.0372],\n",
       "                        [-0.0573,  0.0274]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0302, -0.0214],\n",
       "                        [-0.0083, -0.0383]],\n",
       "              \n",
       "                       [[ 0.0326, -0.0161],\n",
       "                        [ 0.0356,  0.0264]],\n",
       "              \n",
       "                       [[-0.0169,  0.0038],\n",
       "                        [-0.0412,  0.0333]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0080,  0.0249],\n",
       "                        [ 0.0622, -0.0123]],\n",
       "              \n",
       "                       [[-0.0619, -0.0501],\n",
       "                        [-0.0223, -0.0277]],\n",
       "              \n",
       "                       [[-0.0524,  0.0048],\n",
       "                        [-0.0605, -0.0371]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0295,  0.0499],\n",
       "                        [ 0.0197, -0.0024]],\n",
       "              \n",
       "                       [[ 0.0128,  0.0431],\n",
       "                        [-0.0568, -0.0161]],\n",
       "              \n",
       "                       [[-0.0027, -0.0359],\n",
       "                        [ 0.0183,  0.0451]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0118, -0.0024],\n",
       "                        [ 0.0166,  0.0437]],\n",
       "              \n",
       "                       [[ 0.0622,  0.0281],\n",
       "                        [-0.0210,  0.0556]],\n",
       "              \n",
       "                       [[ 0.0105,  0.0262],\n",
       "                        [ 0.0068,  0.0155]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0167, -0.0574],\n",
       "                        [-0.0248,  0.0494]],\n",
       "              \n",
       "                       [[-0.0511,  0.0144],\n",
       "                        [-0.0362, -0.0474]],\n",
       "              \n",
       "                       [[ 0.0174, -0.0322],\n",
       "                        [-0.0545, -0.0062]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0282, -0.0313],\n",
       "                        [ 0.0448,  0.0299]],\n",
       "              \n",
       "                       [[ 0.0541,  0.0597],\n",
       "                        [-0.0143,  0.0379]],\n",
       "              \n",
       "                       [[-0.0388, -0.0077],\n",
       "                        [ 0.0473, -0.0092]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0231, -0.0073],\n",
       "                        [-0.0015,  0.0402]],\n",
       "              \n",
       "                       [[-0.0314, -0.0155],\n",
       "                        [-0.0526, -0.0385]],\n",
       "              \n",
       "                       [[ 0.0049, -0.0378],\n",
       "                        [-0.0302,  0.0218]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0459, -0.0258],\n",
       "                        [ 0.0148, -0.0579]],\n",
       "              \n",
       "                       [[ 0.0539, -0.0316],\n",
       "                        [ 0.0437,  0.0086]],\n",
       "              \n",
       "                       [[ 0.0216, -0.0345],\n",
       "                        [ 0.0196,  0.0618]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0317, -0.0465],\n",
       "                        [-0.0345,  0.0041]],\n",
       "              \n",
       "                       [[ 0.0347,  0.0480],\n",
       "                        [ 0.0035, -0.0209]],\n",
       "              \n",
       "                       [[ 0.0106, -0.0037],\n",
       "                        [-0.0528, -0.0235]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0395, -0.0492],\n",
       "                        [ 0.0326, -0.0224]],\n",
       "              \n",
       "                       [[-0.0078, -0.0594],\n",
       "                        [ 0.0419,  0.0057]],\n",
       "              \n",
       "                       [[ 0.0221,  0.0585],\n",
       "                        [-0.0027, -0.0453]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0320, -0.0581],\n",
       "                        [ 0.0613, -0.0254]],\n",
       "              \n",
       "                       [[-0.0524,  0.0470],\n",
       "                        [-0.0160,  0.0510]],\n",
       "              \n",
       "                       [[ 0.0368,  0.0139],\n",
       "                        [ 0.0080, -0.0429]]]], device='cuda:0')),\n",
       "             ('conv_layers.4.bias',\n",
       "              tensor([-0.0473,  0.0034, -0.0222,  0.0080, -0.0313,  0.0333, -0.0397,  0.0299,\n",
       "                      -0.0260,  0.0284, -0.0617,  0.0575,  0.0477, -0.0433,  0.0360, -0.0538,\n",
       "                       0.0007,  0.0430, -0.0573,  0.0080,  0.0413,  0.0288,  0.0257, -0.0596,\n",
       "                       0.0326, -0.0241, -0.0418, -0.0233, -0.0110, -0.0523,  0.0573, -0.0220,\n",
       "                       0.0146,  0.0242,  0.0469, -0.0231, -0.0355,  0.0381,  0.0131, -0.0067,\n",
       "                       0.0061,  0.0053, -0.0384, -0.0565, -0.0160,  0.0408,  0.0019, -0.0602,\n",
       "                       0.0179, -0.0284,  0.0577, -0.0350,  0.0586, -0.0027,  0.0419, -0.0374,\n",
       "                      -0.0334,  0.0378,  0.0220,  0.0073, -0.0225,  0.0016, -0.0219,  0.0561],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_layers.0.weight',\n",
       "              tensor([[-4.6582e-03, -2.5799e-03, -1.1695e-02,  ..., -2.0023e-03,\n",
       "                       -1.2714e-02, -5.8222e-03],\n",
       "                      [ 5.0213e-03,  7.5831e-03, -8.1748e-04,  ...,  1.2268e-02,\n",
       "                        8.5097e-03, -3.5702e-03],\n",
       "                      [-1.0524e-02,  1.2419e-02,  4.7217e-03,  ...,  8.9738e-03,\n",
       "                       -6.4416e-03,  4.6077e-03],\n",
       "                      ...,\n",
       "                      [ 9.7241e-03,  1.0443e-02, -8.7317e-04,  ..., -8.1486e-03,\n",
       "                        1.0990e-02, -8.3124e-03],\n",
       "                      [-1.1223e-02, -2.1759e-05, -5.1267e-04,  ..., -9.7595e-04,\n",
       "                       -7.9739e-03,  7.5067e-03],\n",
       "                      [ 1.0305e-02, -7.2679e-03,  5.0682e-03,  ...,  1.0072e-02,\n",
       "                        6.1625e-03, -1.2269e-03]], device='cuda:0')),\n",
       "             ('fc_layers.0.bias',\n",
       "              tensor([-7.4137e-03,  7.2780e-03,  9.1046e-03, -1.2042e-02,  5.6340e-03,\n",
       "                       8.3389e-03, -8.2886e-03, -4.3384e-03, -5.6225e-03, -4.4699e-03,\n",
       "                       5.2084e-03, -1.2752e-02,  5.2584e-03, -3.8126e-03, -1.0849e-03,\n",
       "                       1.2158e-02, -3.0748e-04, -7.2475e-03,  3.9116e-03, -6.7740e-03,\n",
       "                      -7.2690e-03,  7.7725e-03,  2.6054e-03, -1.2629e-02,  7.8040e-03,\n",
       "                      -3.5768e-03, -2.3134e-03,  1.1636e-03,  4.1736e-03, -3.8592e-03,\n",
       "                       8.7789e-03, -9.2759e-03,  3.8211e-03,  7.8357e-03,  6.8282e-03,\n",
       "                      -2.4499e-03, -5.4641e-03,  1.5748e-03,  1.6625e-03, -1.8484e-03,\n",
       "                       1.0884e-02,  2.7742e-03,  9.7486e-03,  7.2933e-03,  1.0864e-02,\n",
       "                      -2.9333e-03,  7.0494e-03,  1.0819e-02, -5.3390e-03,  1.2523e-02,\n",
       "                       2.1023e-03,  6.2291e-03, -4.7492e-03,  6.5560e-03, -1.2419e-02,\n",
       "                       4.1272e-03,  6.7233e-03,  1.8664e-03,  5.6215e-04,  3.1992e-03,\n",
       "                      -1.1372e-02,  1.2506e-02, -8.2497e-03,  5.9433e-04,  1.0273e-02,\n",
       "                      -3.2577e-03, -1.3075e-03,  1.9495e-04,  2.3924e-04,  1.0677e-02,\n",
       "                      -6.5040e-03, -3.2303e-03, -2.5545e-04,  1.2071e-02, -1.2365e-02,\n",
       "                       2.7458e-03,  3.7992e-03,  9.3132e-03,  3.3746e-03,  4.6125e-03,\n",
       "                      -4.7206e-03, -3.8999e-03, -2.0757e-03, -9.4404e-03, -3.0918e-03,\n",
       "                       5.6781e-03,  6.4560e-03, -7.0363e-03,  4.6627e-03,  1.0755e-02,\n",
       "                      -1.1913e-02,  3.2311e-03,  4.3485e-03, -1.0235e-03,  4.8566e-03,\n",
       "                      -1.1866e-02,  4.9723e-03,  1.1485e-02, -7.2219e-03, -1.0265e-04,\n",
       "                      -4.7636e-03, -4.9936e-03, -7.0463e-03,  2.2159e-03,  6.5533e-03,\n",
       "                      -3.4249e-03, -4.5978e-03,  1.1010e-02, -8.1739e-03,  5.1413e-04,\n",
       "                       6.7419e-04,  2.2898e-05,  2.6968e-03, -9.7180e-05, -1.0335e-02,\n",
       "                      -3.2331e-03, -1.1439e-02, -1.0869e-02,  6.9356e-03, -2.7279e-05,\n",
       "                      -1.2324e-02, -1.1155e-03, -7.9393e-03,  9.4512e-03,  6.2232e-03,\n",
       "                       3.7953e-03, -7.3723e-05, -9.2780e-03, -1.1967e-04, -8.0390e-03,\n",
       "                      -1.0334e-02,  1.1901e-02,  4.5655e-03, -7.2757e-03, -1.1352e-02,\n",
       "                      -1.0255e-02,  6.0316e-03, -1.0352e-02, -7.1740e-04,  2.7009e-03,\n",
       "                      -1.0294e-02, -6.7233e-03,  8.6603e-03, -1.1312e-02,  1.2297e-03,\n",
       "                       2.6209e-04,  9.9348e-03,  2.6917e-03, -1.2278e-02,  1.9871e-03,\n",
       "                      -8.7658e-03, -1.0405e-02, -4.9891e-03, -4.6785e-03,  7.6440e-03,\n",
       "                      -8.1663e-03,  7.1082e-03, -9.1818e-03, -2.8485e-03,  1.9927e-04,\n",
       "                       2.2278e-03, -1.0409e-02,  3.1318e-04, -9.7528e-03, -1.8114e-03,\n",
       "                       7.2029e-03, -1.0669e-02,  1.1864e-02, -4.6555e-04,  1.2535e-02,\n",
       "                       6.2441e-03,  3.7522e-03, -6.9754e-03, -7.8320e-03,  4.1632e-03,\n",
       "                       1.2096e-02,  1.2280e-02,  1.9027e-03,  5.7200e-03, -8.9906e-03,\n",
       "                      -5.4578e-03,  5.2399e-03,  1.0352e-02,  1.0278e-02,  9.3618e-03,\n",
       "                       1.0913e-02, -1.2255e-02,  6.1153e-03,  6.6600e-03,  5.3557e-03,\n",
       "                      -3.0598e-03, -8.7319e-03,  9.2196e-03, -5.5065e-03, -1.1063e-02,\n",
       "                      -4.2116e-03,  6.0811e-03,  9.0528e-05,  6.6568e-03, -5.9205e-03,\n",
       "                       1.1674e-02, -2.5654e-03, -1.8408e-03, -7.9211e-03,  1.7299e-03,\n",
       "                      -8.1879e-03, -1.2501e-02, -7.1205e-03, -5.5209e-03, -4.0197e-03,\n",
       "                      -2.6719e-03, -3.2521e-03, -1.0151e-02,  1.0318e-02,  2.0779e-03,\n",
       "                       3.4887e-03,  2.7682e-03, -9.8415e-03,  4.0876e-03, -4.9178e-04,\n",
       "                       6.5406e-03, -1.8851e-03,  1.1974e-02,  1.1454e-02, -1.2360e-02,\n",
       "                      -4.3677e-05,  9.3410e-03, -1.8651e-03, -3.5523e-03, -1.0669e-02,\n",
       "                       2.5998e-03, -1.7475e-03,  3.6814e-04,  3.1269e-03,  2.5870e-03,\n",
       "                      -3.1941e-03,  4.1990e-03,  1.3609e-03, -1.1485e-02,  7.8581e-03,\n",
       "                       8.0009e-03,  1.0170e-02, -1.0936e-02,  4.0555e-03,  3.6105e-03,\n",
       "                       6.9634e-03,  1.9622e-03, -3.9787e-03, -1.1285e-02, -4.5545e-03,\n",
       "                       1.2424e-02, -9.2656e-03,  8.8346e-03,  1.2577e-02,  2.4093e-03,\n",
       "                       2.2080e-03,  1.1722e-02,  7.1264e-03, -8.7719e-03,  1.8699e-03,\n",
       "                      -1.1685e-02, -7.2033e-03,  8.1699e-03,  3.4008e-03,  5.4719e-03,\n",
       "                       2.3151e-03,  7.6741e-03, -7.0163e-03, -4.0752e-03, -1.1695e-02,\n",
       "                      -2.5398e-04,  1.1235e-02, -9.7408e-03,  7.1920e-03, -1.4823e-03,\n",
       "                       5.7248e-03, -1.0812e-02,  6.2425e-03,  5.3897e-03, -2.7316e-03,\n",
       "                       1.0397e-02, -1.1012e-02, -1.2228e-02, -1.2153e-02, -5.2770e-03,\n",
       "                       4.6810e-03, -3.7529e-03,  1.0636e-02, -1.7654e-03,  1.4932e-04,\n",
       "                      -6.2887e-03,  5.6573e-03, -5.1054e-03,  9.6250e-03, -8.8062e-03,\n",
       "                      -4.3710e-03, -6.9744e-03,  6.8520e-04,  4.2865e-03, -2.5784e-04,\n",
       "                       9.8181e-03, -4.1769e-03,  8.9394e-03,  6.7646e-03,  1.0639e-02,\n",
       "                       9.2491e-03,  2.6768e-04,  1.0273e-04,  1.0111e-02, -9.9028e-03,\n",
       "                      -4.9339e-03,  1.5565e-03, -7.5036e-03,  6.5729e-03,  8.3719e-03,\n",
       "                      -2.4605e-03, -2.6759e-03,  2.9599e-03,  1.0413e-02,  6.2336e-03,\n",
       "                      -1.7618e-03, -3.4852e-04, -7.5963e-04, -6.7314e-03,  2.7382e-03,\n",
       "                      -5.6056e-03,  2.8339e-03,  1.0820e-02,  6.7337e-03, -1.9855e-03,\n",
       "                      -7.7875e-03,  5.3360e-03, -1.1143e-02, -9.2005e-03, -9.0206e-03,\n",
       "                       5.3798e-03,  7.3006e-03, -1.1536e-02, -2.8704e-03,  4.2503e-03,\n",
       "                       9.3460e-03, -4.9655e-03,  3.6912e-03, -9.3364e-03,  7.9412e-03,\n",
       "                       4.4751e-03, -1.2749e-03, -1.5286e-03,  8.1124e-03,  6.2790e-04,\n",
       "                       3.1592e-03,  6.3959e-03, -7.0560e-04,  4.7733e-03,  1.0118e-02,\n",
       "                      -7.4803e-03,  9.0805e-03,  6.4826e-03,  1.0926e-02,  1.0809e-02,\n",
       "                      -8.4847e-03, -1.3576e-03,  1.0761e-02, -1.0691e-02,  8.2149e-03,\n",
       "                       1.0679e-02, -8.7840e-03,  1.0867e-02,  4.0749e-03, -1.2713e-02,\n",
       "                      -2.4596e-03, -7.3306e-03, -9.5425e-03,  4.8438e-03,  9.5378e-03,\n",
       "                      -2.9879e-03,  4.6400e-03,  4.4342e-03, -5.9674e-03,  1.2272e-02,\n",
       "                      -4.1630e-03,  8.7363e-03,  2.7020e-04, -1.2101e-02, -3.7360e-03,\n",
       "                       4.4019e-03, -4.8414e-03,  4.5162e-03, -6.5885e-03,  7.6135e-03,\n",
       "                       1.1582e-02, -3.4360e-03, -1.2229e-02, -4.4193e-03,  8.0636e-04,\n",
       "                       5.8472e-03,  8.1564e-03,  4.9224e-03, -8.0667e-03, -1.2872e-03,\n",
       "                      -1.1924e-02,  1.0240e-02, -7.3434e-03, -1.0486e-02,  7.0836e-03,\n",
       "                      -2.2154e-03, -1.1761e-02,  4.6701e-03,  9.9546e-03, -8.4740e-03,\n",
       "                      -8.3792e-03, -4.3764e-03,  5.3537e-03,  7.6069e-03,  5.7598e-03,\n",
       "                       1.1940e-02, -6.1740e-03, -3.3346e-03, -7.0810e-03, -9.5892e-03,\n",
       "                      -2.7603e-03, -7.6300e-03,  2.5554e-03,  1.5245e-03, -7.6806e-03,\n",
       "                       8.0293e-03, -4.2113e-03,  8.5804e-03, -4.3597e-03,  2.8030e-03,\n",
       "                       1.1594e-02, -8.5345e-04,  9.5708e-03, -4.7652e-03,  2.5521e-03,\n",
       "                      -8.4596e-03,  1.4573e-03, -2.0375e-03, -2.3617e-03, -6.0857e-03,\n",
       "                      -5.1655e-03,  3.9838e-03,  6.0158e-03, -5.5576e-03, -7.2624e-04,\n",
       "                       7.9467e-03, -4.2800e-03, -7.0445e-03,  8.7132e-03,  1.7497e-03,\n",
       "                      -1.8321e-03,  1.1218e-02, -6.4445e-03, -7.6990e-03, -1.4759e-03,\n",
       "                       1.2077e-02,  1.6725e-03, -5.2103e-03, -4.5157e-03,  8.3019e-03,\n",
       "                       1.0823e-02,  9.4981e-03,  9.8368e-03, -1.1034e-02,  1.1844e-02,\n",
       "                      -7.1172e-03,  9.4720e-03, -5.2730e-03, -8.7909e-03,  6.5535e-03,\n",
       "                       7.2437e-03, -1.2453e-02, -7.2451e-03,  5.6314e-04, -1.2469e-03,\n",
       "                      -1.5939e-03, -7.8795e-03, -5.8418e-03,  2.2917e-03,  1.0830e-02,\n",
       "                       9.3799e-03, -2.6539e-03, -5.7341e-03, -1.2668e-02,  5.2282e-03,\n",
       "                       2.8556e-03,  7.4684e-03, -5.0670e-03,  9.8506e-03, -8.7664e-03,\n",
       "                       1.1356e-02, -1.2097e-02, -1.8511e-03, -9.8733e-03,  7.5208e-03,\n",
       "                       1.0358e-02,  1.0908e-02,  5.3537e-03,  7.6893e-03, -8.6978e-03,\n",
       "                      -9.4921e-03,  4.7044e-03,  5.8353e-03, -1.9087e-03,  3.2925e-03,\n",
       "                      -1.2348e-02,  5.8083e-03, -7.6269e-03,  1.0611e-02,  1.1690e-02,\n",
       "                      -3.1087e-03,  1.2230e-02], device='cuda:0')),\n",
       "             ('fc_layers.2.weight',\n",
       "              tensor([[ 0.0319, -0.0419,  0.0304,  ...,  0.0193, -0.0053,  0.0383],\n",
       "                      [ 0.0330,  0.0388,  0.0101,  ..., -0.0223, -0.0126,  0.0166],\n",
       "                      [-0.0110, -0.0422, -0.0411,  ...,  0.0301, -0.0340,  0.0248],\n",
       "                      ...,\n",
       "                      [-0.0026,  0.0363, -0.0235,  ...,  0.0012,  0.0106,  0.0251],\n",
       "                      [-0.0210, -0.0351,  0.0149,  ..., -0.0180,  0.0229,  0.0222],\n",
       "                      [-0.0059,  0.0352,  0.0107,  ...,  0.0138, -0.0046,  0.0270]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_layers.2.bias',\n",
       "              tensor([ 0.0054, -0.0404, -0.0098,  0.0201, -0.0322, -0.0187,  0.0122],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder='../Video',    # Folder to save videos\n",
    "    name_prefix=f'eval-V{version}-E{episode}-S{max_steps_per_episode}',               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: True    # Record every episode\n",
    ")\n",
    "dim = env.action_space.n\n",
    "print(action_dim)\n",
    "agent = ConvD3QNAgent(input_shape, action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "model_load_path = get_last_modified_file(target_directory)\n",
    "agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ae8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 207.73608881235123\n",
      "Episode: 1 Reward: 836.6357502937317\n",
      "Episode: 2 Reward: 1641.0202419161797\n",
      "Episode: 3 Reward: 8.537099540233612\n",
      "Episode: 4 Reward: 8.537099540233612\n",
      "Episode: 5 Reward: 0.0\n",
      "Episode: 6 Reward: 91.0623950958252\n",
      "Episode: 7 Reward: 88.21669524908066\n",
      "Episode: 8 Reward: 389.86087876558304\n",
      "Episode: 9 Reward: 56.91399693489075\n"
     ]
    }
   ],
   "source": [
    "episode = 10\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state = obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
