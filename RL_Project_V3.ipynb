{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc92308",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = True\n",
    "LOAD_MODEL = False\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "batch_size = 32\n",
    "num_episodes = 200\n",
    "max_steps_per_episode = 5400\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 2\n",
    "version = 3\n",
    "Model = \"DQN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c752692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_last_modified_file(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
    "        return None\n",
    "    files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=os.path.getmtime, reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "model_load_path = get_last_modified_file(target_directory)\n",
    "\n",
    "if model_load_path:\n",
    "    print(f\"The last modified file is: {model_load_path}\")\n",
    "else:\n",
    "    print(\"No files found in the directory or directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, buttons):\n",
    "        super().__init__(env)\n",
    "        self.buttons = buttons\n",
    "        self._actions = np.identity(len(buttons), dtype=np.int8)\n",
    "        self.action_space = gym.spaces.Discrete(len(buttons))\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(self._actions[action])\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a2d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, mov_rew=0.01, score_rew=0.05, hp_rew=4, ring_rew=1, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= (self.mov_rew/2)\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, buttons):\n",
    "        super().__init__(env)\n",
    "        self.buttons = buttons\n",
    "        # Create a mapping from a single action index to the full button array.\n",
    "        self._actions = np.identity(len(buttons), dtype=np.int8)\n",
    "        self.action_space = gym.spaces.Discrete(len(buttons))\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(self._actions[action])\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticFrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, n, stickprob):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n = n\n",
    "        self.stickprob = stickprob\n",
    "        self.curac = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.supports_want_render = hasattr(env, \"supports_want_render\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.curac = None\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, ac):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        totrew = 0\n",
    "        for i in range(self.n):\n",
    "            # First step after reset, use action\n",
    "            if self.curac is None:\n",
    "                self.curac = ac\n",
    "            # First substep, delay with probability=stickprob\n",
    "            elif i == 0:\n",
    "                if self.rng.rand() > self.stickprob:\n",
    "                    self.curac = ac\n",
    "            # Second substep, new action definitely kicks in\n",
    "            elif i == 1:\n",
    "                self.curac = ac\n",
    "            if self.supports_want_render and i < self.n - 1:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(\n",
    "                    self.curac,\n",
    "                    want_render=False,\n",
    "                )\n",
    "            else:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(self.curac)\n",
    "            totrew += rew\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        return ob, totrew, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2246aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, input_shape, num_actions, lr, gamma, epsilon, epsilon_decay, buffer_size):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model = ConvDQN(input_shape, num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        transform = T.Compose([T.ToPILImage(), T.Grayscale(), T.Resize(self.input_shape[1:]), T.ToTensor()])\n",
    "        return transform(state)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess(state)\n",
    "        q_values = self.model(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = self.preprocess(next_state)\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state.unsqueeze(0))).item()\n",
    "            state = self.preprocess(state)\n",
    "            target_f = self.model(state.unsqueeze(0)).detach().numpy()\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(torch.tensor(target_f), self.model(state.unsqueeze(0)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a63e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/DQN/DQN-Sonic-V{version}-E{episode}-S{max_episode_steps}.pth' #ppt para jit, pth para statedict\n",
    "    try:\n",
    "        torch.save(agent.model.state_dict(), model_save_path)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('No Enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e187fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(*, game, state=None, max_episode_steps=4500, **kwargs):\n",
    "    if state is None:\n",
    "        state = retro.State.DEFAULT\n",
    "    env = retro.make(game, state, **kwargs)\n",
    "    env = ButtonActionWrapper(env, buttons=['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z']) #['LEFT', 'RIGHT', 'A']\n",
    "    env = CustomRewardWrapper(env)\n",
    "    env = MaxAndSkipObservation(env, skip=num_frame_skip)\n",
    "    if RESIZE_ENV:\n",
    "        input_shape = (num_stacked_frames*3, *new_size)\n",
    "        env = ResizeObservation(env, new_size)\n",
    "    else:\n",
    "        input_shape = (num_stacked_frames*3, 224, 320)\n",
    "    env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = StochasticFrameSkip(env, n=4, stickprob=0.25)\n",
    "    if max_episode_steps is not None:\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    return env, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37280243",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "#venv = VecTransposeImage(VecFrameStack(SubprocVecEnv([make_env] * 8), n_stack=4))\n",
    "agent = ConvDQNAgent(input_shape, action_dim, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.99, buffer_size=10000)\n",
    "if LOAD_MODEL:\n",
    "  agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "\n",
    "temp_reward = 0\n",
    "frame_count_prev = 0\n",
    "frame_count = 0\n",
    "for episode in range(num_episodes):\n",
    "  state, info = env.reset()\n",
    "  total_reward = 0\n",
    "  done = False\n",
    "  while not done:\n",
    "    frame_count += 1\n",
    "    action = agent.act(state = state)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    agent.remember(state, action, reward, observation, done)\n",
    "    state = observation\n",
    "    total_reward += reward\n",
    "    temp_reward += reward\n",
    "    agent.replay(batch_size)\n",
    "\n",
    "  if (episode+1) % 2 == 0:\n",
    "    print(f'Episode {episode+1} \\nstep n={(frame_count-frame_count_prev)/4}\\nreward {temp_reward/4}\\n')\n",
    "    temp_reward = 0\n",
    "    frame_count_prev=frame_count\n",
    "  if (episode+1) % 12 == 0:\n",
    "    save_model(agent, episode)\n",
    "env.close()\n",
    "print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array')\n",
    "env = ButtonActionWrapper(env, buttons=['LEFT', 'RIGHT', 'A'])\n",
    "env = CustomRewardWrapper(env)\n",
    "env = MaxAndSkipObservation(env, skip=num_frame_skip)\n",
    "if RESIZE_ENV:\n",
    "  input_shape = (num_stacked_frames, *new_size)\n",
    "  env = ResizeObservation(env, new_size)\n",
    "else:\n",
    "  input_shape = (num_stacked_frames, 320, 224)\n",
    "env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder='../Video',    # Folder to save videos\n",
    "    name_prefix=f'eval-V{version}-E{episode}-S{max_episode_steps}',               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: True    # Record every episode\n",
    ")\n",
    "dim = env.action_space.n\n",
    "print(action_dim)\n",
    "agent = ConvDQNAgent(input_shape, action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "model_load_path = get_last_modified_file(target_directory)\n",
    "agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ae8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 3\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state = obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
