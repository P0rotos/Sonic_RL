{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce02c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.distributions import MultivariateNormal, Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "import io\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc92308",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = True\n",
    "LOAD_MODEL = False\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 1800\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 2\n",
    "Model = \"PPO\"\n",
    "save_interval = 100\n",
    "episode_p_interval = 4\n",
    "rew_p_interval = 5\n",
    "\n",
    "#Version\n",
    "version = 1\n",
    "\n",
    "#Hiperparametros\n",
    "LR = 1e-4\n",
    "GAMMA = 0.99\n",
    "\n",
    "#DQN y D3QN Params\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.99\n",
    "BUFFER_SIZE = 10000\n",
    "batch_size = 64\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "#D3QN\n",
    "UPDATE_TARGET_FREQ = 10000\n",
    "\n",
    "#PPO Params\n",
    "TIMESTEPS_PER_BATCH = 1 # 1  Por ahora   mientras    implemeto   mini-batch\n",
    "MAX_TIMESTEPS_PER_EPISODE = 1800\n",
    "N_UPDATES_PER_ITERATION = 5\n",
    "CLIP = 0.2\n",
    "ENTROPY_COEF = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c752692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found in the directory or directory does not exist.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_last_modified_file(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
    "        return None\n",
    "    files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=os.path.getmtime, reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "model_load_path = get_last_modified_file(target_directory)\n",
    "\n",
    "if model_load_path:\n",
    "    print(f\"The last modified file is: {model_load_path}\")\n",
    "else:\n",
    "    print(\"No files found in the directory or directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985a3502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 17] File exists: '../Saved_Models'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO/Actor'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO/Critic'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/DQN'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/D3QN'\n",
      "Error: [Errno 17] File exists: '../Video'\n",
      "Error: [Errno 17] File exists: '../Video/PPO'\n",
      "Error: [Errno 17] File exists: '../Video/DQN'\n",
      "Error: [Errno 17] File exists: '../Video/D3QN'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"../Saved_Models\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Actor\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Critic\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "try:\n",
    "    os.mkdir(\"../Video/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937d962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/{Model}' #ppt para jit, pth para statedict\n",
    "    model_file_name = f'/{Model}-Sonic-V{version}-E{episode}-S{max_steps_per_episode}.pth'\n",
    "    try:\n",
    "        if Model == \"DQN\":\n",
    "            torch.save(agent.model.state_dict(), model_save_path+model_file_name)\n",
    "        if Model == \"D3QN\":\n",
    "            torch.save(agent.model_online.state_dict(), model_save_path+model_file_name)\n",
    "        if Model == \"PPO\":\n",
    "            torch.save(agent.model_actor.state_dict(), model_save_path+\"/Actor\"+model_file_name)\n",
    "            torch.save(agent.model_critic.state_dict(), model_save_path+\"/Critic\"+model_file_name)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6229fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym-retro environment and make it use discrete\n",
    "    actions for the Sonic game.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ButtonActionWrapper, self).__init__(env)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],\n",
    "                   ['DOWN', 'B'], ['B']]\n",
    "        self._actions = []\n",
    "        for action in actions:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in action:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._actions.append(arr)\n",
    "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
    "\n",
    "    def action(self, a): # pylint: disable=W0221\n",
    "        return self._actions[a].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a2d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, mov_rew=0.01, score_rew=0.05, hp_rew=1, ring_rew=0.1, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= self.mov_rew\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4e5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticFrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, n, stickprob):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n = n\n",
    "        self.stickprob = stickprob\n",
    "        self.curac = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.supports_want_render = hasattr(env, \"supports_want_render\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.curac = None\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, ac):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        totrew = 0\n",
    "        for i in range(self.n):\n",
    "            # First step after reset, use action\n",
    "            if self.curac is None:\n",
    "                self.curac = ac\n",
    "            # First substep, delay with probability=stickprob\n",
    "            elif i == 0:\n",
    "                if self.rng.rand() > self.stickprob:\n",
    "                    self.curac = ac\n",
    "            # Second substep, new action definitely kicks in\n",
    "            elif i == 1:\n",
    "                self.curac = ac\n",
    "            if self.supports_want_render and i < self.n - 1:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(\n",
    "                    self.curac,\n",
    "                    want_render=False,\n",
    "                )\n",
    "            else:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(self.curac)\n",
    "            totrew += rew\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        return ob, totrew, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2246aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000, epsilon_end=0.01):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model = ConvDQN(input_shape, num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.epsilon_end = epsilon_end\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        transform = T.Lambda(lambda x: x.permute(0,3,1,2).reshape(-1, self.input_shape[1], self.input_shape[2]))\n",
    "        return transform(state)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = self.preprocess_wv(next_state)\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state.unsqueeze(0))).item()\n",
    "            state = self.preprocess_wv(state)\n",
    "            target_f = self.model(state.unsqueeze(0)).to(\"cpu\").detach().numpy()\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(torch.tensor(target_f).to(self.device), self.model(state.unsqueeze(0)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states_tensor)\n",
    "            max_next_q = torch.max(next_q_values, dim=1)[0]\n",
    "        target_q_values = rewards_tensor + self.gamma * max_next_q * (~dones_tensor)\n",
    "        current_q_values = self.model(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.epsilon > 0.05:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d750b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvD3QN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.advance_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        advantages = self.advance_stream(conv_out)\n",
    "        value = self.value_stream(conv_out)\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6de7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000, update_target_freq=10000, epsilon_end = 0.01):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model_online = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model_online.parameters(), lr=lr)\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.step_counter = 0\n",
    "        self.epsilon_end = epsilon_end\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        transform = T.Lambda(lambda x: x.permute(0,3,1,2).reshape(-1, self.input_shape[1], self.input_shape[2]))\n",
    "        return transform(state)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model_online(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values_online = self.model_online(next_states_tensor)\n",
    "            best_action_online_indices = torch.argmax(next_q_values_online, dim=1).unsqueeze(1)\n",
    "            max_Q_next = self.model_target(next_states_tensor).gather(1, best_action_online_indices).squeeze()\n",
    "        target_q_values = rewards_tensor + self.gamma * max_Q_next * (~dones_tensor)\n",
    "        current_q_values = self.model_online(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.step_counter += 1\n",
    "        self.update_target_network()\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        if self.step_counter % self.update_target_freq == 0:\n",
    "            self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPPOAgent:\n",
    "    def __init__(self, env, input_shape, clip=0.2, learning_rate=1e-4, gamma=0.99, timestep_per_batch=4800, max_timesteps_per_episode=1600, n_updates_per_iteration=5, entropy_coef=0.01, minibatch_size = 64, max_grad_norm=0.5, lam = 0.95):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.ts_batch = timestep_per_batch\n",
    "        self.max_ts_ep = max_timesteps_per_episode\n",
    "        self.device = 'cuda'\n",
    "        self.model_actor = ConvDQN(input_shape, self.num_actions).to(self.device)\n",
    "        self.model_critic = ConvDQN(input_shape, 1).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.model_actor.parameters(), lr=self.lr)\n",
    "        self.critic_optimizer = optim.Adam(self.model_critic.parameters(), lr=self.lr)\n",
    "        self.n_updates = n_updates_per_iteration\n",
    "        self.clip = clip\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.mb_size = minibatch_size\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.lam = lam\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        obs = self.preprocess_wv(obs)\n",
    "        mean = self.model_actor(obs.unsqueeze(0))\n",
    "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        dist = Categorical(logits=mean)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.detach().squeeze()\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def rollout(self):\n",
    "        batch_obs = []      \n",
    "        batch_acts = []            \n",
    "        batch_log_probs = []       \n",
    "        batch_rews = []          \n",
    "        batch_lens = []     \n",
    "        batch_vals = []\n",
    "        batch_dones = []\n",
    "        ep_rews = []\n",
    "        ep_vals = []\n",
    "        ep_dones = []\n",
    "        t = 0 \n",
    "        while t < self.ts_batch:\n",
    "            ep_rews = []\n",
    "            ep_vals = []\n",
    "            ep_dones = []\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "            for ep_t in range(self.max_ts_ep):\n",
    "                t += 1\n",
    "                batch_obs.append(obs)\n",
    "                action, log_prob = self.get_action(obs)\n",
    "                obs_tensor = self.preprocess_wv(obs)\n",
    "                val = self.model_critic(obs_tensor)\n",
    "                obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                ep_dones.append(done)\n",
    "                ep_rews.append(reward)\n",
    "                ep_vals.append(val.flatten())\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                if done:\n",
    "                    break\n",
    "            batch_lens.append(ep_t + 1)\n",
    "            batch_rews.append(ep_rews)\n",
    "            batch_vals.append(ep_vals)\n",
    "            batch_dones.append(ep_dones)\n",
    "        #batch_obs = torch.stack([self.preprocess_wv(s) for s in batch_obs])\n",
    "        #batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens, batch_vals, batch_dones\n",
    "    \n",
    "    # def compute_rtgs(self, batch_rews):\n",
    "    #     batch_rtgs = []\n",
    "    #     for ep_rews in reversed(batch_rews):\n",
    "    #         discounted_reward = 0 \n",
    "    #         for rew in reversed(ep_rews):\n",
    "    #             discounted_reward = rew + discounted_reward * self.gamma\n",
    "    #             batch_rtgs.insert(0, discounted_reward)\n",
    "    #     batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float, device=self.device)\n",
    "    #     return batch_rtgs\n",
    "\n",
    "    def calculate_gae(self, rewards, values, dones):\n",
    "        batch_advantages = []\n",
    "        for ep_rews, ep_vals, ep_dones in zip(rewards, values, dones):\n",
    "            advantages = []\n",
    "            last_advantage = 0\n",
    "            for t in reversed(range(len(ep_rews))):\n",
    "                if t + 1 < len(ep_rews):\n",
    "                    delta = ep_rews[t] + self.gamma * ep_vals[t+1] * (1 - ep_dones[t+1]) - ep_vals[t]\n",
    "                else:\n",
    "                    delta = ep_rews[t] - ep_vals[t]\n",
    "                advantage = delta + self.gamma * self.lam * (1 - ep_dones[t]) * last_advantage\n",
    "                last_advantage = advantage\n",
    "                advantages.insert(0, advantage)\n",
    "            batch_advantages.extend(advantages)\n",
    "        return torch.tensor(batch_advantages, dtype=torch.float)\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        V = self.model_critic(batch_obs).squeeze()\n",
    "        mean = self.model_actor(batch_obs)\n",
    "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        dist = Categorical(logits=mean)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        entropy_loss = dist.entropy().mean()\n",
    "        return V, log_probs, entropy_loss\n",
    "    \n",
    "    def learn(self, total_timesteps):\n",
    "        act_t = 0\n",
    "        while act_t < total_timesteps:\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens, batch_vals, batch_dones = self.rollout()\n",
    "            act_t += np.sum(batch_lens)\n",
    "            batch_obs_temp = torch.stack([self.preprocess_wv(s) for s in batch_obs])\n",
    "            A_k = self.calculate_gae(batch_rews, batch_vals, batch_dones)\n",
    "            V = self.model_critic(batch_obs_temp).squeeze()\n",
    "            batch_obs_temp = []\n",
    "            batch_acts = torch.tensor(batch_acts, dtype=torch.float, device=self.device)\n",
    "            batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float, device=self.device)\n",
    "            batch_rtgs = A_k + V.detach()\n",
    "            step = batch_obs.size(0)\n",
    "            inds = np.arange(step)\n",
    "            for _ in range(self.n_updates):\n",
    "                np.random.shuffle(inds)\n",
    "                for start in range(0, step, self.mb_size):\n",
    "                    end = start + self.mb_size\n",
    "                    idx = inds[start:end]\n",
    "                    mini_obs = batch_obs[idx]\n",
    "                    mini_obs = torch.stack([self.preprocess_wv(s) for s in mini_obs])\n",
    "                    mini_acts = batch_acts[idx]\n",
    "                    mini_log_probs = batch_log_probs[idx]\n",
    "                    mini_advantage = A_k[idx]\n",
    "                    mini_rtgs = batch_rtgs[idx]\n",
    "                    V, curr_log_probs, curr_entropy_loss = self.evaluate(mini_obs, mini_acts)\n",
    "                    ratios = torch.exp(curr_log_probs - mini_log_probs)\n",
    "                    surr1 = ratios * mini_advantage\n",
    "                    surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * mini_advantage\n",
    "                    actor_loss = (-torch.min(surr1, surr2)).mean() - self.entropy_coef*curr_entropy_loss\n",
    "                    critic_loss = nn.MSELoss()(V, mini_rtgs)\n",
    "                    self.actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward(retain_graph=True)\n",
    "                    nn.utils.clip_grad_norm_(self.model_actor.parameters(), self.max_grad_norm)\n",
    "                    self.actor_optimizer.step()\n",
    "                    self.critic_optimizer.zero_grad()    \n",
    "                    critic_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.model_critic.parameters(), self.max_grad_norm)\n",
    "                    self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "348c5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Enviroment to close\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('No Enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e187fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(*, game, state=None, max_episode_steps=4500, **kwargs):\n",
    "    if state is None:\n",
    "        state = retro.State.DEFAULT\n",
    "    env = retro.make(game, state, **kwargs)\n",
    "    env = ButtonActionWrapper(env)\n",
    "    #env = CustomRewardWrapper(env)\n",
    "    env = StochasticFrameSkip(env, n=num_frame_skip, stickprob=0.25)\n",
    "    if RESIZE_ENV:\n",
    "        input_shape = (num_stacked_frames*3, *new_size)\n",
    "        env = ResizeObservation(env, new_size)\n",
    "    else:\n",
    "        input_shape = (num_stacked_frames*3, 224, 320)\n",
    "    if max_episode_steps is not None:\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "    return env, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37280243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1800\n",
      "2890\n",
      "4690\n",
      "6490\n",
      "8290\n",
      "8703\n",
      "8989\n",
      "10789\n",
      "12589\n",
      "14389\n",
      "15011\n",
      "16811\n",
      "18611\n",
      "19062\n",
      "20862\n",
      "22662\n",
      "24462\n",
      "26262\n",
      "28062\n",
      "28252\n",
      "30052\n",
      "31852\n",
      "32488\n",
      "32707\n",
      "34507\n",
      "36307\n",
      "36477\n",
      "38277\n",
      "40077\n",
      "40405\n",
      "40664\n",
      "40809\n",
      "40968\n",
      "42768\n",
      "44568\n",
      "44764\n",
      "46564\n",
      "48364\n",
      "50164\n",
      "50316\n",
      "52116\n",
      "52338\n",
      "53278\n",
      "55078\n",
      "56878\n",
      "58678\n",
      "60478\n",
      "62278\n",
      "64078\n",
      "65878\n",
      "67678\n",
      "69478\n",
      "71278\n",
      "73078\n",
      "74663\n",
      "75536\n",
      "77336\n",
      "78540\n",
      "80340\n",
      "81214\n",
      "83014\n",
      "83987\n",
      "85363\n",
      "87163\n",
      "88223\n",
      "90023\n",
      "91823\n",
      "93623\n",
      "95423\n",
      "96707\n",
      "98507\n",
      "99589\n",
      "101389\n",
      "103189\n",
      "104989\n",
      "106789\n",
      "108589\n",
      "110389\n",
      "111292\n",
      "113092\n",
      "114892\n",
      "115844\n",
      "117644\n",
      "118720\n",
      "120041\n",
      "120951\n",
      "122751\n",
      "124551\n",
      "125906\n",
      "127049\n",
      "128849\n",
      "129704\n",
      "130893\n",
      "132693\n",
      "134314\n",
      "134518\n",
      "136318\n",
      "137493\n",
      "139293\n",
      "141093\n",
      "142893\n",
      "144004\n",
      "145804\n",
      "145994\n",
      "147794\n",
      "149594\n",
      "151394\n",
      "151873\n",
      "153673\n",
      "153877\n",
      "155677\n",
      "156518\n",
      "157339\n",
      "159139\n",
      "160939\n",
      "162739\n",
      "164539\n",
      "166339\n",
      "168139\n",
      "169620\n",
      "169769\n",
      "169920\n",
      "171720\n",
      "172515\n",
      "172658\n",
      "172798\n",
      "174598\n",
      "176398\n",
      "178198\n",
      "179238\n",
      "180124\n",
      "181924\n",
      "183527\n",
      "185327\n",
      "187127\n",
      "188927\n",
      "190727\n",
      "192527\n",
      "193877\n",
      "195525\n",
      "197325\n",
      "199038\n",
      "200838\n",
      "201030\n",
      "202830\n",
      "204630\n",
      "206430\n",
      "208230\n",
      "209140\n",
      "210940\n",
      "212740\n",
      "214540\n",
      "216340\n",
      "218140\n",
      "219940\n",
      "221511\n",
      "223311\n",
      "225111\n",
      "225257\n",
      "226114\n",
      "227223\n",
      "228363\n",
      "229567\n",
      "230525\n",
      "232325\n",
      "234125\n",
      "235925\n",
      "237725\n",
      "239525\n",
      "241151\n",
      "242951\n",
      "244751\n",
      "245717\n",
      "246686\n",
      "247552\n",
      "249352\n",
      "251152\n",
      "252927\n",
      "254368\n",
      "255296\n",
      "257096\n",
      "258896\n",
      "259975\n",
      "261775\n",
      "262682\n",
      "264482\n",
      "266282\n",
      "268082\n",
      "269273\n",
      "270174\n",
      "271974\n",
      "272426\n",
      "273291\n",
      "275091\n",
      "276891\n",
      "278691\n",
      "280491\n",
      "282291\n",
      "284091\n",
      "285891\n",
      "287691\n",
      "288689\n",
      "290489\n",
      "292289\n",
      "294089\n",
      "295388\n",
      "297188\n",
      "297967\n",
      "299767\n",
      "301567\n",
      "303367\n",
      "305167\n",
      "306967\n",
      "307764\n",
      "308937\n",
      "309653\n",
      "311453\n",
      "312324\n",
      "313981\n",
      "315781\n",
      "317119\n",
      "318919\n",
      "319718\n",
      "320822\n",
      "322622\n",
      "324422\n",
      "326222\n",
      "328022\n",
      "329822\n",
      "330593\n",
      "331288\n",
      "333088\n",
      "334888\n",
      "336688\n",
      "337669\n",
      "339469\n",
      "341269\n",
      "343069\n",
      "344869\n",
      "346669\n",
      "348469\n",
      "349283\n",
      "351083\n",
      "352883\n",
      "354683\n",
      "356483\n",
      "358283\n",
      "360083\n",
      "361883\n",
      "363683\n",
      "365053\n",
      "366853\n",
      "367724\n",
      "369524\n",
      "370360\n",
      "371961\n",
      "373761\n",
      "374692\n",
      "376492\n",
      "377362\n",
      "379162\n",
      "380962\n",
      "382762\n",
      "382908\n",
      "383793\n",
      "384918\n",
      "385813\n",
      "386933\n",
      "388150\n",
      "388982\n",
      "389825\n",
      "390613\n",
      "391430\n",
      "393230\n",
      "395030\n",
      "396484\n",
      "397736\n",
      "399536\n",
      "401336\n",
      "403136\n",
      "403795\n",
      "405140\n",
      "406155\n",
      "407955\n",
      "408934\n",
      "410062\n",
      "411313\n",
      "412180\n",
      "412973\n",
      "413974\n",
      "415191\n",
      "416991\n",
      "417500\n",
      "419300\n",
      "421100\n",
      "422137\n",
      "422283\n",
      "424083\n",
      "425883\n",
      "427683\n",
      "428876\n",
      "430676\n",
      "430880\n",
      "432417\n",
      "434217\n",
      "435396\n",
      "436255\n",
      "437071\n",
      "437923\n",
      "439723\n",
      "441523\n",
      "442142\n",
      "443942\n",
      "445476\n",
      "447276\n",
      "449076\n",
      "450876\n",
      "452676\n",
      "454476\n",
      "456276\n",
      "458076\n",
      "459621\n",
      "461421\n",
      "463221\n",
      "464369\n",
      "465130\n",
      "466930\n",
      "468730\n",
      "470530\n",
      "471242\n",
      "473042\n",
      "474842\n",
      "476475\n",
      "477233\n",
      "479033\n",
      "479861\n",
      "480564\n",
      "482364\n",
      "484164\n",
      "485964\n",
      "486961\n",
      "488761\n",
      "490561\n",
      "492361\n",
      "494161\n",
      "495961\n",
      "497470\n",
      "499270\n",
      "501070\n",
      "502870\n",
      "503699\n",
      "503845\n",
      "505645\n",
      "507445\n",
      "509245\n",
      "511045\n",
      "512845\n",
      "514250\n",
      "516049\n",
      "517849\n",
      "519649\n",
      "521449\n",
      "522816\n",
      "524616\n",
      "525463\n",
      "527263\n",
      "529063\n",
      "530863\n",
      "531572\n",
      "533372\n",
      "535172\n",
      "536972\n",
      "538772\n",
      "540572\n",
      "542372\n",
      "544172\n",
      "545972\n",
      "547772\n",
      "549572\n",
      "551372\n",
      "553172\n",
      "554972\n",
      "556772\n",
      "558572\n",
      "560372\n",
      "561441\n",
      "562337\n",
      "564137\n",
      "565937\n",
      "567737\n",
      "569537\n",
      "571337\n",
      "572097\n",
      "573897\n",
      "574107\n",
      "575907\n",
      "577707\n",
      "579507\n",
      "581307\n",
      "583107\n",
      "584006\n",
      "584747\n",
      "586547\n",
      "588347\n",
      "590147\n",
      "591947\n",
      "593747\n",
      "595547\n",
      "597347\n",
      "599147\n",
      "600947\n",
      "602747\n",
      "604547\n",
      "606347\n",
      "608147\n",
      "609947\n",
      "611747\n",
      "613547\n",
      "615347\n",
      "617147\n",
      "618947\n",
      "620747\n",
      "622254\n",
      "623577\n",
      "625377\n",
      "626474\n",
      "626915\n",
      "627905\n",
      "629705\n",
      "631505\n",
      "633305\n",
      "634555\n",
      "635622\n",
      "637422\n",
      "638049\n",
      "638909\n",
      "640709\n",
      "641701\n",
      "642469\n",
      "644269\n",
      "645255\n",
      "647055\n",
      "647733\n",
      "649533\n",
      "650793\n",
      "651581\n",
      "652485\n",
      "653556\n",
      "655356\n",
      "656181\n",
      "657150\n",
      "657804\n",
      "659604\n",
      "660420\n",
      "661589\n",
      "663389\n",
      "665189\n",
      "666142\n",
      "667942\n",
      "669742\n",
      "670405\n",
      "671089\n",
      "671276\n",
      "672110\n",
      "673910\n",
      "675710\n",
      "677510\n",
      "679310\n",
      "680909\n",
      "681771\n",
      "681917\n",
      "682655\n",
      "683571\n",
      "684369\n",
      "686169\n",
      "687969\n",
      "688607\n",
      "690407\n",
      "692207\n",
      "694007\n",
      "694987\n",
      "696787\n",
      "698587\n",
      "699166\n",
      "700966\n",
      "702171\n",
      "703388\n",
      "705188\n",
      "706572\n",
      "708372\n",
      "710172\n",
      "711972\n",
      "713772\n",
      "715572\n",
      "717372\n",
      "718634\n",
      "720434\n",
      "722234\n",
      "723043\n",
      "724104\n",
      "725904\n",
      "727609\n",
      "728907\n",
      "730707\n",
      "732507\n",
      "734307\n",
      "736107\n",
      "737105\n",
      "738905\n",
      "740705\n",
      "742505\n",
      "743538\n",
      "745338\n",
      "747138\n",
      "748056\n",
      "749856\n",
      "750889\n",
      "752486\n",
      "753895\n",
      "755243\n",
      "757043\n",
      "758796\n",
      "760596\n",
      "762396\n",
      "764196\n",
      "764756\n",
      "766556\n",
      "767765\n",
      "769565\n",
      "771365\n",
      "773165\n",
      "774820\n",
      "775012\n",
      "776812\n",
      "778612\n",
      "779565\n",
      "780591\n",
      "782391\n",
      "783297\n",
      "784507\n",
      "786307\n",
      "788107\n",
      "788705\n",
      "788843\n",
      "789476\n",
      "791034\n",
      "792834\n",
      "793704\n",
      "795504\n",
      "796359\n",
      "798159\n",
      "799959\n",
      "800712\n",
      "802512\n",
      "802945\n",
      "803645\n",
      "804450\n",
      "806250\n",
      "807008\n",
      "807777\n",
      "809577\n",
      "810501\n",
      "811410\n",
      "813210\n",
      "815010\n",
      "816190\n",
      "817990\n",
      "819790\n",
      "820485\n",
      "821342\n",
      "823142\n",
      "824942\n",
      "826742\n",
      "828542\n",
      "829513\n",
      "830233\n",
      "832033\n",
      "833833\n",
      "833981\n",
      "834741\n",
      "835455\n",
      "837255\n",
      "839055\n",
      "839944\n",
      "840807\n",
      "841657\n",
      "842323\n",
      "844123\n",
      "845923\n",
      "846500\n",
      "847304\n",
      "849104\n",
      "849765\n",
      "850661\n",
      "851387\n",
      "853187\n",
      "854987\n",
      "856787\n",
      "858587\n",
      "860387\n",
      "862187\n",
      "863134\n",
      "864934\n",
      "865678\n",
      "866454\n",
      "867399\n",
      "868356\n",
      "870156\n",
      "871956\n",
      "872832\n",
      "874632\n",
      "876432\n",
      "877265\n",
      "877924\n",
      "878642\n",
      "880442\n",
      "882242\n",
      "884042\n",
      "884958\n",
      "886758\n",
      "888558\n",
      "889122\n",
      "890001\n",
      "890955\n",
      "891976\n",
      "892570\n",
      "893312\n",
      "895112\n",
      "895947\n",
      "896644\n",
      "897426\n",
      "899226\n",
      "899658\n",
      "901458\n",
      "902248\n",
      "904048\n",
      "904711\n",
      "906511\n",
      "907144\n",
      "907966\n",
      "908994\n",
      "909760\n",
      "911560\n",
      "912442\n",
      "913267\n",
      "913415\n",
      "914304\n",
      "915036\n",
      "915648\n",
      "916837\n",
      "917331\n",
      "919131\n",
      "919720\n",
      "920473\n",
      "921237\n",
      "923037\n",
      "924837\n",
      "925568\n",
      "926289\n",
      "927115\n",
      "928915\n",
      "930620\n",
      "931045\n",
      "931592\n",
      "932451\n",
      "933467\n",
      "935267\n",
      "936135\n",
      "937935\n",
      "938873\n",
      "939517\n",
      "941317\n",
      "941816\n",
      "943616\n",
      "945416\n",
      "946244\n",
      "948026\n",
      "949826\n",
      "950857\n",
      "952657\n",
      "954018\n",
      "954793\n",
      "955465\n",
      "957265\n",
      "959065\n",
      "960109\n",
      "961000\n",
      "962022\n",
      "963822\n",
      "965622\n",
      "966303\n",
      "968103\n",
      "968645\n",
      "969417\n",
      "970012\n",
      "971057\n",
      "972857\n",
      "973973\n",
      "975357\n",
      "976056\n",
      "976678\n",
      "977385\n",
      "978044\n",
      "979844\n",
      "981644\n",
      "982897\n",
      "984697\n",
      "985313\n",
      "987113\n",
      "987930\n",
      "988742\n",
      "989566\n",
      "991366\n",
      "992312\n",
      "994112\n",
      "994955\n",
      "996529\n",
      "997328\n",
      "997986\n",
      "999786\n",
      "1000893\n",
      "1001680\n",
      "1002412\n",
      "1003080\n",
      "1003768\n",
      "1004971\n",
      "1005642\n",
      "1006390\n",
      "1008190\n",
      "1009990\n",
      "1011021\n",
      "1011766\n",
      "1012464\n",
      "1014264\n",
      "1015416\n",
      "1017216\n",
      "1019016\n",
      "1019871\n",
      "1021671\n",
      "1022371\n",
      "1023563\n",
      "1024237\n",
      "1026037\n",
      "1026774\n",
      "1027534\n",
      "1028392\n",
      "1029870\n",
      "1030559\n",
      "1031285\n",
      "1031906\n",
      "1032603\n",
      "1034403\n",
      "1035009\n",
      "1035835\n",
      "1036256\n",
      "1037299\n",
      "1039099\n",
      "1040038\n",
      "1041016\n",
      "1042253\n",
      "1042993\n",
      "1043828\n",
      "1044513\n",
      "1046313\n",
      "1048113\n",
      "1049107\n",
      "1050007\n",
      "1051807\n",
      "1052559\n",
      "1053532\n",
      "1054798\n",
      "1055422\n",
      "1056331\n",
      "1056718\n",
      "1057425\n",
      "1058245\n",
      "1059184\n",
      "1060984\n",
      "1062142\n",
      "1062902\n",
      "1063642\n",
      "1064068\n",
      "1065868\n",
      "1067668\n",
      "1069468\n",
      "1071268\n",
      "1073068\n",
      "1074868\n",
      "1076668\n",
      "1078468\n",
      "1079297\n",
      "1081097\n",
      "1082897\n",
      "1084697\n",
      "1085398\n",
      "1087198\n",
      "1088998\n",
      "1089687\n",
      "1090746\n",
      "1092172\n",
      "1093067\n",
      "1093654\n",
      "1095454\n",
      "1097254\n",
      "1099054\n",
      "1100854\n",
      "1101560\n",
      "1103360\n",
      "1105160\n",
      "1105925\n",
      "1106643\n",
      "1108443\n",
      "1110243\n",
      "1110829\n",
      "1111756\n",
      "1113010\n",
      "1114810\n",
      "1116610\n",
      "1118410\n",
      "1120210\n",
      "1121395\n",
      "1123195\n",
      "1124995\n",
      "1126795\n",
      "1127627\n",
      "1129427\n",
      "1131227\n",
      "1132068\n",
      "1133868\n",
      "1135668\n",
      "1136386\n",
      "1137870\n",
      "1138770\n",
      "1140570\n",
      "1142370\n",
      "1144170\n",
      "1145970\n",
      "1147770\n",
      "1149570\n",
      "1151370\n",
      "1153170\n",
      "1154970\n",
      "1156770\n",
      "1158570\n",
      "1160370\n",
      "1162170\n",
      "1163970\n",
      "1165770\n",
      "1166743\n",
      "1167570\n",
      "1169370\n",
      "1171170\n",
      "1171997\n",
      "1173797\n",
      "1175597\n",
      "1177397\n",
      "1179197\n",
      "1180997\n",
      "1182797\n",
      "1183556\n",
      "1184158\n",
      "1185958\n",
      "1187758\n",
      "1188490\n",
      "1189262\n",
      "1189885\n",
      "1190455\n",
      "1192255\n",
      "1192727\n",
      "1193434\n",
      "1194082\n",
      "1194813\n",
      "1196613\n",
      "1198413\n",
      "1199194\n",
      "1200994\n",
      "1201913\n",
      "1203713\n",
      "1204704\n",
      "1205549\n",
      "1207349\n",
      "1209149\n",
      "1210038\n",
      "1211838\n",
      "1212722\n",
      "1214522\n",
      "1216322\n",
      "1217097\n",
      "1218897\n",
      "1220697\n",
      "1221463\n",
      "1222215\n",
      "1224015\n",
      "1225815\n",
      "1226375\n",
      "1228175\n",
      "1228978\n",
      "1229645\n",
      "1230324\n",
      "1232124\n",
      "1232741\n",
      "1233327\n",
      "1234081\n",
      "1234724\n",
      "1236524\n",
      "1238324\n",
      "1238909\n",
      "1240709\n",
      "1242509\n",
      "1244309\n",
      "1245184\n",
      "1246984\n",
      "1248784\n",
      "1250584\n",
      "1252384\n",
      "1253050\n",
      "1254166\n",
      "1255966\n",
      "1257766\n",
      "1258461\n",
      "1259143\n",
      "1260335\n",
      "1262135\n",
      "1263935\n",
      "1265735\n",
      "1267535\n",
      "1268222\n",
      "1268938\n",
      "1269669\n",
      "1270306\n",
      "1271008\n",
      "1272808\n",
      "1273704\n",
      "1274392\n",
      "1276192\n",
      "1276948\n",
      "1277580\n",
      "1279380\n",
      "1281180\n",
      "1281881\n",
      "1283681\n",
      "1284328\n",
      "1286128\n",
      "1287269\n",
      "1289069\n",
      "1290869\n",
      "1292669\n",
      "1294469\n",
      "1295136\n",
      "1295284\n",
      "1295940\n",
      "1296901\n",
      "1298701\n",
      "1299328\n",
      "1300027\n",
      "1301178\n",
      "1302978\n",
      "1304778\n",
      "1306374\n",
      "1307091\n",
      "1307634\n",
      "1309434\n",
      "1311234\n",
      "1313034\n",
      "1313700\n",
      "1315500\n",
      "1316323\n",
      "1316969\n",
      "1318769\n",
      "1320569\n",
      "1322369\n",
      "1324169\n",
      "1324604\n",
      "1326404\n",
      "1326835\n",
      "1327439\n",
      "1329239\n",
      "1329946\n",
      "1330876\n",
      "1332676\n",
      "1333279\n",
      "1335079\n",
      "1336519\n",
      "1337593\n",
      "1339036\n",
      "1339782\n",
      "1341196\n",
      "1341875\n",
      "1343675\n",
      "1344327\n",
      "1345503\n",
      "1347303\n",
      "1348014\n",
      "1348780\n",
      "1350580\n",
      "1352380\n",
      "1353117\n",
      "1353835\n",
      "1354969\n",
      "1356013\n",
      "1357813\n",
      "1358412\n",
      "1359478\n",
      "1360810\n",
      "1361880\n",
      "1362943\n",
      "1363934\n",
      "1364965\n",
      "1366765\n",
      "1367836\n",
      "1369636\n",
      "1371436\n",
      "1373236\n",
      "1374480\n",
      "1375547\n",
      "1377347\n",
      "1378668\n",
      "1380468\n",
      "1381694\n",
      "1382867\n",
      "1384667\n",
      "1385167\n",
      "1386645\n",
      "1387972\n",
      "1389227\n",
      "1391027\n",
      "1392827\n",
      "1393846\n",
      "1395646\n",
      "1396802\n",
      "1397917\n",
      "1398066\n",
      "1398214\n",
      "1399373\n",
      "1400356\n",
      "1400833\n",
      "1400982\n",
      "1402782\n",
      "1404582\n",
      "1406382\n",
      "1407454\n",
      "1409254\n",
      "1411054\n",
      "1412854\n",
      "1414654\n",
      "1416317\n",
      "1417495\n",
      "1419295\n",
      "1420365\n",
      "1421747\n",
      "1423131\n",
      "1424515\n",
      "1426315\n",
      "1427462\n",
      "1429262\n",
      "1429410\n",
      "1430632\n",
      "1432432\n",
      "1433589\n",
      "1435177\n",
      "1435825\n",
      "1436468\n",
      "1437551\n",
      "1439179\n",
      "1440713\n",
      "1442513\n",
      "1443636\n",
      "1444692\n",
      "1445719\n",
      "1447179\n",
      "1448163\n",
      "1449300\n",
      "1451100\n",
      "1452265\n",
      "1454065\n",
      "1455083\n",
      "1456129\n",
      "1457173\n",
      "1457769\n",
      "1459569\n",
      "1461369\n",
      "1463169\n",
      "1464249\n",
      "1465094\n",
      "1466894\n",
      "1467043\n",
      "1467192\n",
      "1468305\n",
      "1469569\n",
      "1470542\n",
      "1471235\n",
      "1473035\n",
      "1474114\n",
      "1475185\n",
      "1476173\n",
      "1476978\n",
      "1478316\n",
      "1479420\n",
      "1480821\n",
      "1482621\n",
      "1483614\n",
      "1484618\n",
      "1485700\n",
      "1486234\n",
      "1486712\n",
      "1487830\n",
      "1488943\n",
      "1490007\n",
      "1491178\n",
      "1492194\n",
      "1493994\n",
      "1495051\n",
      "1496851\n",
      "1497957\n",
      "1499757\n",
      "1501557\n",
      "1503099\n",
      "1504070\n",
      "1505161\n",
      "1506317\n",
      "1507498\n",
      "1508539\n",
      "1510339\n",
      "1511381\n",
      "1513181\n",
      "1514358\n",
      "1516158\n",
      "1517433\n",
      "1518528\n",
      "1519496\n",
      "1521296\n",
      "1522282\n",
      "1524082\n",
      "1525064\n",
      "1526186\n",
      "1527258\n",
      "1528494\n",
      "1529690\n",
      "1530843\n",
      "1531822\n",
      "1532842\n",
      "1533811\n",
      "1534971\n",
      "1535990\n",
      "1537050\n",
      "1538850\n",
      "1540041\n",
      "1541021\n",
      "1542055\n",
      "1543091\n",
      "1544891\n",
      "1545863\n",
      "1546969\n",
      "1547951\n",
      "1549042\n",
      "1550842\n",
      "1551878\n",
      "1552935\n",
      "1554735\n",
      "1556535\n",
      "1557498\n",
      "1558754\n",
      "1559541\n",
      "1560625\n",
      "1561655\n",
      "1563455\n",
      "1565255\n",
      "1566350\n",
      "1567385\n",
      "1568467\n",
      "1569506\n",
      "1570548\n",
      "1571134\n",
      "1572122\n",
      "1572605\n",
      "1574320\n",
      "1575402\n",
      "1575874\n",
      "1577674\n",
      "1578675\n",
      "1579316\n",
      "1580412\n",
      "1582212\n",
      "1583387\n",
      "1583962\n",
      "1585044\n",
      "1586844\n",
      "1587810\n",
      "1588487\n",
      "1590287\n",
      "1591388\n",
      "1593188\n",
      "1594988\n",
      "1596788\n",
      "1598588\n",
      "1600388\n",
      "1601019\n",
      "1602819\n",
      "1603300\n",
      "1604276\n",
      "1604951\n",
      "1606751\n",
      "1607837\n",
      "1608434\n",
      "1610234\n",
      "1611226\n",
      "1613026\n",
      "1614826\n",
      "1616626\n",
      "1617790\n",
      "1618744\n",
      "1620544\n",
      "1621489\n",
      "1622600\n",
      "1624400\n",
      "1625480\n",
      "1626729\n",
      "1627714\n",
      "1628710\n",
      "1629720\n",
      "1631520\n",
      "1632568\n",
      "1633603\n",
      "1634647\n",
      "1635685\n",
      "1636647\n",
      "1637689\n",
      "1639489\n",
      "1641289\n",
      "1642262\n",
      "1643284\n",
      "1643936\n",
      "1644931\n",
      "1646066\n",
      "1647866\n",
      "1649057\n",
      "1650857\n",
      "1652657\n",
      "1653765\n",
      "1655565\n",
      "1656690\n",
      "1657824\n",
      "1658896\n",
      "1659783\n",
      "1660745\n",
      "1661884\n",
      "1662895\n",
      "1663888\n",
      "1664954\n",
      "1665926\n",
      "1666966\n",
      "1667568\n",
      "1668795\n",
      "1670074\n",
      "1671038\n",
      "1672838\n",
      "1674638\n",
      "1675693\n",
      "1676188\n",
      "1677988\n",
      "1678957\n",
      "1680055\n",
      "1680690\n",
      "1681847\n",
      "1682816\n",
      "1683913\n",
      "1684962\n",
      "1685928\n",
      "1686972\n",
      "1687930\n",
      "1688983\n",
      "1690017\n",
      "1690533\n",
      "1691515\n",
      "1692492\n",
      "1694292\n",
      "1696092\n",
      "1697128\n",
      "1698094\n",
      "1699069\n",
      "1700108\n",
      "1701908\n",
      "1702962\n",
      "1703936\n",
      "1704931\n",
      "1705984\n",
      "1706978\n",
      "1708778\n",
      "1710578\n",
      "1711652\n",
      "1712700\n",
      "1714500\n",
      "1715544\n",
      "1717344\n",
      "1718402\n",
      "1719444\n",
      "1719862\n",
      "1720925\n",
      "1721924\n",
      "1722973\n",
      "1723934\n",
      "1725734\n",
      "1727534\n",
      "1729334\n",
      "1730355\n",
      "1730781\n",
      "1731813\n",
      "1733613\n",
      "1735413\n",
      "1736481\n",
      "1737501\n",
      "1738198\n",
      "1739998\n",
      "1741798\n",
      "1743049\n",
      "1744849\n",
      "1746649\n",
      "1748449\n",
      "1749119\n",
      "1750177\n",
      "1751977\n",
      "1752784\n",
      "1754332\n",
      "1756132\n",
      "1756281\n",
      "1757649\n",
      "1758257\n",
      "1760057\n",
      "1761857\n",
      "1763657\n",
      "1765457\n",
      "1767257\n",
      "1767970\n",
      "1769770\n",
      "1771570\n",
      "1773370\n",
      "1775170\n",
      "1775764\n",
      "1777564\n",
      "1779364\n",
      "1780087\n",
      "1781887\n",
      "1783687\n",
      "1785487\n",
      "1787287\n",
      "1789087\n",
      "1790887\n",
      "1792687\n",
      "1793314\n",
      "1795114\n",
      "1796914\n",
      "1798714\n",
      "1800095\n",
      "Modelo exitosamente guardado en ../Saved_Models/PPO\n"
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "#venv = VecTransposeImage(VecFrameStack(SubprocVecEnv([make_env] * 8), n_stack=4))\n",
    "if Model == \"DQN\":\n",
    "  agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"D3QN\":\n",
    "  agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE, update_target_freq=UPDATE_TARGET_FREQ)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model_online.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"PPO\":\n",
    "  agent = ConvPPOAgent(env=env, input_shape=input_shape, learning_rate=LR, gamma=GAMMA, timestep_per_batch=TIMESTEPS_PER_BATCH, max_timesteps_per_episode=MAX_TIMESTEPS_PER_EPISODE, clip=CLIP, n_updates_per_iteration=N_UPDATES_PER_ITERATION, entropy_coef=ENTROPY_COEF)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "\n",
    "agent.learn(num_episodes*max_steps_per_episode)\n",
    "save_model(agent, num_episodes)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d648480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/seba/Documentos/AI/RL/Video/PPO folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10741/3507460547.py:23: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  agent.model_actor.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
      "/tmp/ipykernel_10741/3507460547.py:25: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  agent.model_critic.state_dict(torch.load(model_load_path, map_location=agent.device))\n"
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=f'../Video/{Model}',    # Folder to save videos\n",
    "    name_prefix=f'eval-V{version}-S{max_steps_per_episode*num_episodes}',               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: True    # Record every episode\n",
    ")\n",
    "dim = env.action_space.n\n",
    "print(action_dim)\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "if Model == \"DQN\":\n",
    "    model_load_path = get_last_modified_file(target_directory)\n",
    "    agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"D3QN\":  \n",
    "    model_load_path = get_last_modified_file(target_directory)\n",
    "    agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "    agent.model_online.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"PPO\":\n",
    "    agent = ConvPPOAgent(env=env, input_shape=input_shape)\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Actor\")\n",
    "    agent.model_actor.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Critic\")\n",
    "    agent.model_critic.state_dict(torch.load(model_load_path, map_location=agent.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e22ae8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 222.91315460205078\n",
      "Episode: 1 Reward: 527.4030382633209\n",
      "Episode: 2 Reward: 446.7748763561249\n",
      "Episode: 3 Reward: 214.3760548233986\n",
      "Episode: 4 Reward: 110.98229402303696\n",
      "Episode: 5 Reward: 0.0\n",
      "Episode: 6 Reward: 335.7925816178322\n",
      "Episode: 7 Reward: 175.4848240017891\n",
      "Episode: 8 Reward: 289.3128177523613\n",
      "Episode: 9 Reward: 239.03878712654114\n"
     ]
    }
   ],
   "source": [
    "episode = 10\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if Model == \"PPO\":\n",
    "            action = agent.get_action(obs = obs)[0]\n",
    "        else:\n",
    "            action = agent.act(state = obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
