{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce02c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.distributions import MultivariateNormal, Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "import io\n",
    "import time\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc92308",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = True\n",
    "LOAD_MODEL = False\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "num_episodes = 100\n",
    "max_steps_per_episode = 1800\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 2\n",
    "Model = \"PPO\"\n",
    "save_interval = 100\n",
    "episode_p_interval = 4\n",
    "rew_p_interval = 5\n",
    "\n",
    "#Version\n",
    "version = 1\n",
    "\n",
    "#Hiperparametros\n",
    "LR = 5e-5\n",
    "GAMMA = 0.99\n",
    "\n",
    "#DQN y D3QN Params\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.99\n",
    "BUFFER_SIZE = 10000\n",
    "batch_size = 64\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "#D3QN\n",
    "UPDATE_TARGET_FREQ = 10000\n",
    "\n",
    "#PPO Params\n",
    "N_STEPS = 2048\n",
    "N_UPDATES_PER_ITERATION = 5\n",
    "CLIP = 0.2\n",
    "ENTROPY_COEF = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c752692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found in the directory or directory does not exist.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_last_modified_file(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
    "        return None\n",
    "    files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=os.path.getmtime, reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "model_load_path = get_last_modified_file(target_directory)\n",
    "\n",
    "if model_load_path:\n",
    "    print(f\"The last modified file is: {model_load_path}\")\n",
    "else:\n",
    "    print(\"No files found in the directory or directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "985a3502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 17] File exists: '../Saved_Models'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO/Actor'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO/Critic'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/DQN'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/D3QN'\n",
      "Error: [Errno 17] File exists: '../Video'\n",
      "Error: [Errno 17] File exists: '../Video/PPO'\n",
      "Error: [Errno 17] File exists: '../Video/DQN'\n",
      "Error: [Errno 17] File exists: '../Video/D3QN'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"../Saved_Models\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Actor\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Critic\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "try:\n",
    "    os.mkdir(\"../Video/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "937d962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/{Model}' #ppt para jit, pth para statedict\n",
    "    model_file_name = f'/{Model}-Sonic-V{version}-E{episode}-S{max_steps_per_episode}.pth'\n",
    "    try:\n",
    "        if Model == \"DQN\":\n",
    "            torch.save(agent.model.state_dict(), model_save_path+model_file_name)\n",
    "        if Model == \"D3QN\":\n",
    "            torch.save(agent.model_online.state_dict(), model_save_path+model_file_name)\n",
    "        if Model == \"PPO\":\n",
    "            torch.save(agent.model_actor.state_dict(), model_save_path+\"/Actor\"+model_file_name)\n",
    "            torch.save(agent.model_critic.state_dict(), model_save_path+\"/Critic\"+model_file_name)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6229fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym-retro environment and make it use discrete\n",
    "    actions for the Sonic game.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ButtonActionWrapper, self).__init__(env)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],\n",
    "                   ['DOWN', 'B'], ['B']]\n",
    "        self._actions = []\n",
    "        for action in actions:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in action:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._actions.append(arr)\n",
    "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
    "\n",
    "    def action(self, a): # pylint: disable=W0221\n",
    "        return self._actions[a].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91a2d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, mov_rew=0.01, score_rew=0.05, hp_rew=1, ring_rew=0.1, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= self.mov_rew\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f4e5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticFrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, n, stickprob):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n = n\n",
    "        self.stickprob = stickprob\n",
    "        self.curac = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.supports_want_render = hasattr(env, \"supports_want_render\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.curac = None\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, ac):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        totrew = 0\n",
    "        for i in range(self.n):\n",
    "            # First step after reset, use action\n",
    "            if self.curac is None:\n",
    "                self.curac = ac\n",
    "            # First substep, delay with probability=stickprob\n",
    "            elif i == 0:\n",
    "                if self.rng.rand() > self.stickprob:\n",
    "                    self.curac = ac\n",
    "            # Second substep, new action definitely kicks in\n",
    "            elif i == 1:\n",
    "                self.curac = ac\n",
    "            if self.supports_want_render and i < self.n - 1:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(\n",
    "                    self.curac,\n",
    "                    want_render=False,\n",
    "                )\n",
    "            else:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(self.curac)\n",
    "            totrew += rew\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        return ob, totrew, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d2246aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f3cce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000, epsilon_end=0.01):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model = ConvDQN(input_shape, num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.epsilon_end = epsilon_end\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        transform = T.Lambda(lambda x: x.permute(0,3,1,2).reshape(-1, self.input_shape[1], self.input_shape[2]))\n",
    "        return transform(state)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = self.preprocess_wv(next_state)\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state.unsqueeze(0))).item()\n",
    "            state = self.preprocess_wv(state)\n",
    "            target_f = self.model(state.unsqueeze(0)).to(\"cpu\").detach().numpy()\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(torch.tensor(target_f).to(self.device), self.model(state.unsqueeze(0)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states_tensor)\n",
    "            max_next_q = torch.max(next_q_values, dim=1)[0]\n",
    "        target_q_values = rewards_tensor + self.gamma * max_next_q * (~dones_tensor)\n",
    "        current_q_values = self.model(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.epsilon > 0.05:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23d750b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvD3QN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.advance_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        advantages = self.advance_stream(conv_out)\n",
    "        value = self.value_stream(conv_out)\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b6de7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000, update_target_freq=10000, epsilon_end = 0.01):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model_online = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model_online.parameters(), lr=lr)\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.step_counter = 0\n",
    "        self.epsilon_end = epsilon_end\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        transform = T.Lambda(lambda x: x.permute(0,3,1,2).reshape(-1, self.input_shape[1], self.input_shape[2]))\n",
    "        return transform(state)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model_online(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values_online = self.model_online(next_states_tensor)\n",
    "            best_action_online_indices = torch.argmax(next_q_values_online, dim=1).unsqueeze(1)\n",
    "            max_Q_next = self.model_target(next_states_tensor).gather(1, best_action_online_indices).squeeze()\n",
    "        target_q_values = rewards_tensor + self.gamma * max_Q_next * (~dones_tensor)\n",
    "        current_q_values = self.model_online(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.step_counter += 1\n",
    "        self.update_target_network()\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        if self.step_counter % self.update_target_freq == 0:\n",
    "            self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPPOAgent:\n",
    "    def __init__(self, env, input_shape, clip=0.2, learning_rate=1e-4, gamma=0.99, n_steps=2048, n_updates_per_iteration=5, entropy_coef=0.01, minibatch_size = 64, max_grad_norm=0.5, lam = 0.95):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.n_steps = n_steps\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_actor = ConvDQN(input_shape, self.num_actions).to(self.device)\n",
    "        #self.model_actor.half()\n",
    "        self.model_critic = ConvDQN(input_shape, 1).to(self.device)\n",
    "        #self.model_critic.half()\n",
    "        self.actor_optimizer = optim.Adam(self.model_actor.parameters(), lr=self.lr)\n",
    "        self.critic_optimizer = optim.Adam(self.model_critic.parameters(), lr=self.lr)\n",
    "        self.n_updates = n_updates_per_iteration\n",
    "        self.clip = clip\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.mb_size = minibatch_size\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.lam = lam\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        mean = self.model_actor(obs.unsqueeze(0))\n",
    "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        dist = Categorical(logits=mean)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.detach().squeeze()\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def rollout(self):\n",
    "        batch_obs = []      \n",
    "        batch_acts = []            \n",
    "        batch_log_probs = []       \n",
    "        batch_rews = []          \n",
    "        batch_lens = []     \n",
    "        batch_vals = []\n",
    "        batch_dones = []\n",
    "        ep_rews = []\n",
    "        ep_vals = []\n",
    "        ep_dones = []\n",
    "        t = 0 \n",
    "        if not hasattr(self, 'current_obs'):\n",
    "            obs, _ =self.env.reset()\n",
    "            self.current_obs = obs\n",
    "        obs = self.current_obs\n",
    "        with torch.no_grad():\n",
    "            while t < self.n_steps:\n",
    "                t+=1\n",
    "                obs_tensor = self.preprocess_wv(obs)\n",
    "                batch_obs.append(obs_tensor)\n",
    "                action, log_prob = self.get_action(obs_tensor)\n",
    "                val = self.model_critic(obs_tensor.unsqueeze(0)).detach().cpu().item()#.numpy().flatten()[0]\n",
    "                obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                ep_dones.append(done)\n",
    "                ep_rews.append(reward)\n",
    "                ep_vals.append(val)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                if done:\n",
    "                    batch_lens.append(len(ep_rews))\n",
    "                    batch_rews.append(ep_rews)\n",
    "                    batch_vals.append(ep_vals)\n",
    "                    batch_dones.append(ep_dones)\n",
    "                    obs, _ = self.env.reset()\n",
    "                    ep_rews = []\n",
    "                    ep_vals = []\n",
    "                    ep_dones = []\n",
    "                    self.current_obs = obs\n",
    "        if len(ep_rews) > 0:\n",
    "            batch_lens.append(len(ep_rews))\n",
    "            batch_rews.append(ep_rews)\n",
    "            batch_vals.append(ep_vals)\n",
    "            batch_dones.append(ep_dones)\n",
    "        final_obs_tensor = self.preprocess_wv(obs)\n",
    "        with torch.no_grad():\n",
    "            self.final_val = self.model_critic(final_obs_tensor.unsqueeze(0)).squeeze().cpu().item()\n",
    "\n",
    "        self.current_obs = obs\n",
    "        batch_obs = torch.stack(batch_obs)\n",
    "        #batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.long, device=self.device)\n",
    "        batch_log_probs = torch.stack(batch_log_probs).float()\n",
    "        #batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float32, device=self.device)\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens, batch_vals, batch_dones\n",
    "    \n",
    "    # def compute_rtgs(self, batch_rews):\n",
    "    #     batch_rtgs = []\n",
    "    #     for ep_rews in reversed(batch_rews):\n",
    "    #         discounted_reward = 0 \n",
    "    #         for rew in reversed(ep_rews):\n",
    "    #             discounted_reward = rew + discounted_reward * self.gamma\n",
    "    #             batch_rtgs.insert(0, discounted_reward)\n",
    "    #     batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float, device=self.device)\n",
    "    #     return batch_rtgs\n",
    "\n",
    "    def calculate_gae(self, rewards, values, dones):\n",
    "        batch_advantages = []\n",
    "        for i, (ep_rews, ep_vals, ep_dones) in enumerate(zip(rewards, values, dones)):\n",
    "            is_last_segment = (i == len(rewards) - 1)\n",
    "            advantages = []\n",
    "            last_advantage = 0\n",
    "            for t in reversed(range(len(ep_rews))):\n",
    "                is_terminal = ep_dones[t]\n",
    "                if is_terminal:\n",
    "                    next_val = 0\n",
    "                elif t == len(ep_rews) - 1 and is_last_segment:\n",
    "                    next_val = self.final_val\n",
    "                else:\n",
    "                    next_val = ep_vals[t+1]\n",
    "                delta = ep_rews[t] + self.gamma * next_val - ep_vals[t]\n",
    "                advantage = delta + self.gamma * self.lam * (1 - is_terminal) * last_advantage\n",
    "                last_advantage = advantage\n",
    "                advantages.insert(0, advantage)\n",
    "            batch_advantages.extend(advantages)\n",
    "        return torch.tensor(batch_advantages, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        V = self.model_critic(batch_obs).view(-1) #.squeeze()\n",
    "        mean = self.model_actor(batch_obs)\n",
    "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        dist = Categorical(logits=mean)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        entropy_loss = dist.entropy().mean()\n",
    "        return V, log_probs, entropy_loss\n",
    "    \n",
    "    def learn(self, total_timesteps):\n",
    "        act_t = 0\n",
    "        while act_t < total_timesteps:\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens, batch_vals, batch_dones = self.rollout()\n",
    "            act_t += np.sum(batch_lens)\n",
    "            V_old_flat = [val for ep_vals in batch_vals for val in ep_vals]\n",
    "            V_old_flat = torch.tensor(V_old_flat, dtype=torch.float32, device=self.device).detach()\n",
    "            A_k = self.calculate_gae(batch_rews, batch_vals, batch_dones)\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "            # batch_acts = torch.tensor(batch_acts, dtype=torch.long, device=self.device)\n",
    "            # batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float, device=self.device)\n",
    "            batch_rtgs = A_k + V_old_flat\n",
    "            step = len(batch_obs)\n",
    "            inds = np.arange(step)\n",
    "            print(f\"Iteration {act_t}/{total_timesteps}, collected {np.sum(batch_lens)} steps\")\n",
    "            assert len(V_old_flat) == len(batch_obs), f\"Value mismatch: {len(V_old_flat)} vs {len(batch_obs)}\"\n",
    "            for _ in range(self.n_updates):\n",
    "                np.random.shuffle(inds)\n",
    "                for start in range(0, step, self.mb_size):\n",
    "                    end = start + self.mb_size\n",
    "                    idx = inds[start:end]\n",
    "                    mini_obs = batch_obs[idx]\n",
    "                    mini_acts = batch_acts[idx]\n",
    "                    mini_log_probs = batch_log_probs[idx]\n",
    "                    mini_advantage = A_k[idx]\n",
    "                    mini_rtgs = batch_rtgs[idx]\n",
    "                    V, curr_log_probs, curr_entropy_loss = self.evaluate(mini_obs, mini_acts)\n",
    "                    ratios = torch.exp(curr_log_probs - mini_log_probs)\n",
    "                    surr1 = ratios * mini_advantage\n",
    "                    surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * mini_advantage\n",
    "                    actor_loss = (-torch.min(surr1, surr2)).mean() - self.entropy_coef*curr_entropy_loss\n",
    "                    critic_loss = nn.MSELoss()(V, mini_rtgs)\n",
    "                    self.actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.model_actor.parameters(), self.max_grad_norm)\n",
    "                    self.actor_optimizer.step()\n",
    "                    self.critic_optimizer.zero_grad()    \n",
    "                    critic_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.model_critic.parameters(), self.max_grad_norm)\n",
    "                    self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "348c5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('No Enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8e187fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(*, game, state=None, max_episode_steps=4500, **kwargs):\n",
    "    if state is None:\n",
    "        state = retro.State.DEFAULT\n",
    "    env = retro.make(game, state, **kwargs)\n",
    "    env = ButtonActionWrapper(env)\n",
    "    #env = CustomRewardWrapper(env)\n",
    "    env = StochasticFrameSkip(env, n=num_frame_skip, stickprob=0.25)\n",
    "    if RESIZE_ENV:\n",
    "        input_shape = (num_stacked_frames*3, *new_size)\n",
    "        env = ResizeObservation(env, new_size)\n",
    "    else:\n",
    "        input_shape = (num_stacked_frames*3, 224, 320)\n",
    "    if max_episode_steps is not None:\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "    return env, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37280243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Modelo exitosamente guardado en ../Saved_Models/PPO\n"
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "#venv = VecTransposeImage(VecFrameStack(SubprocVecEnv([make_env] * 8), n_stack=4))\n",
    "if Model == \"DQN\":\n",
    "  agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"D3QN\":\n",
    "  agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE, update_target_freq=UPDATE_TARGET_FREQ)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model_online.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"PPO\":\n",
    "  agent = ConvPPOAgent(env=env, input_shape=input_shape, learning_rate=LR, gamma=GAMMA, n_steps=N_STEPS, clip=CLIP, n_updates_per_iteration=N_UPDATES_PER_ITERATION, entropy_coef=ENTROPY_COEF)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "\n",
    "agent.learn(num_episodes*max_steps_per_episode)\n",
    "save_model(agent, num_episodes)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d648480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/seba/Documentos/AI/RL/Video/PPO folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/tmp/ipykernel_8974/3507460547.py:23: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  agent.model_actor.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
      "/tmp/ipykernel_8974/3507460547.py:25: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  agent.model_critic.state_dict(torch.load(model_load_path, map_location=agent.device))\n"
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=f'../Video/{Model}',    # Folder to save videos\n",
    "    name_prefix=f'eval-V{version}-S{max_steps_per_episode*num_episodes}',               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: True    # Record every episode\n",
    ")\n",
    "dim = env.action_space.n\n",
    "print(action_dim)\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "if Model == \"DQN\":\n",
    "    model_load_path = get_last_modified_file(target_directory)\n",
    "    agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"D3QN\":  \n",
    "    model_load_path = get_last_modified_file(target_directory)\n",
    "    agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "    agent.model_online.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"PPO\":\n",
    "    agent = ConvPPOAgent(env=env, input_shape=input_shape)\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Actor\")\n",
    "    agent.model_actor.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Critic\")\n",
    "    agent.model_critic.state_dict(torch.load(model_load_path, map_location=agent.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ae8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 334.84401524066925\n",
      "Episode: 1 Reward: 541.6315374970436\n",
      "Episode: 2 Reward: 0.0\n",
      "Episode: 3 Reward: 0.0\n",
      "Episode: 4 Reward: 113.8279938697815\n",
      "Episode: 5 Reward: -1.7881393432617188e-07\n",
      "Episode: 6 Reward: -5.960464477539063e-08\n",
      "Episode: 7 Reward: 143.23355895280838\n",
      "Episode: 8 Reward: 264.6500856280327\n",
      "Episode: 9 Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "episode = 10\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if Model == \"PPO\":\n",
    "            obs = agent.preprocess_wv(state=obs)\n",
    "            action = agent.get_action(obs = obs)[0]\n",
    "        else:\n",
    "            action = agent.act(state = obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
