{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce02c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "import io\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbc92308",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = True\n",
    "LOAD_MODEL = False\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "batch_size = 64\n",
    "num_episodes = 10\n",
    "max_steps_per_episode = 1800\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 2\n",
    "version = 1\n",
    "Model = \"PPO\"\n",
    "save_interval = 100\n",
    "episode_p_interval = 4\n",
    "rew_p_interval = 5\n",
    "\n",
    "#Hiperparametros\n",
    "LR = 1e-4\n",
    "GAMMA = 0.99\n",
    "\n",
    "#DQN y D3QN Params\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.99\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "#PPO Params\n",
    "TIMESTEPS_PER_BATCH = 3600\n",
    "MAX_TIMESTEPS_PER_EPISODE = 1800\n",
    "N_UPDATES_PER_ITERATION = 5\n",
    "CLIP = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c752692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found in the directory or directory does not exist.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_last_modified_file(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
    "        return None\n",
    "    files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=os.path.getmtime, reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "model_load_path = get_last_modified_file(target_directory)\n",
    "\n",
    "if model_load_path:\n",
    "    print(f\"The last modified file is: {model_load_path}\")\n",
    "else:\n",
    "    print(\"No files found in the directory or directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "985a3502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 17] File exists: '../Saved_Models'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO/Actor'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO/Critic'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/DQN'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/D3QN'\n",
      "Error: [Errno 17] File exists: '../Video'\n",
      "Error: [Errno 17] File exists: '../Video/PPO'\n",
      "Error: [Errno 17] File exists: '../Video/DQN'\n",
      "Error: [Errno 17] File exists: '../Video/D3QN'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"../Saved_Models\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Actor\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Critic\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "try:\n",
    "    os.mkdir(\"../Video/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "937d962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/{Model}' #ppt para jit, pth para statedict\n",
    "    model_file_name = f'/{Model}-Sonic-V{version}-E{episode}-S{max_steps_per_episode}.pth'\n",
    "    try:\n",
    "        if Model == \"DQN\":\n",
    "            torch.save(agent.model.state_dict(), model_save_path+model_file_name)\n",
    "        if Model == \"D3QN\":\n",
    "            torch.save(agent.model_online.state_dict(), model_save_path+model_file_name)\n",
    "        if Model == \"PPO\":\n",
    "            torch.save(agent.model_actor.state_dict(), model_save_path+\"/Actor\"+model_file_name)\n",
    "            torch.save(agent.model_critic.state_dict(), model_save_path+\"/Critic\"+model_file_name)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6229fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym-retro environment and make it use discrete\n",
    "    actions for the Sonic game.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ButtonActionWrapper, self).__init__(env)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],\n",
    "                   ['DOWN', 'B'], ['B']]\n",
    "        self._actions = []\n",
    "        for action in actions:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in action:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._actions.append(arr)\n",
    "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
    "\n",
    "    def action(self, a): # pylint: disable=W0221\n",
    "        return self._actions[a].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91a2d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, mov_rew=0.01, score_rew=0.05, hp_rew=1, ring_rew=0.1, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= self.mov_rew\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f4e5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticFrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, n, stickprob):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n = n\n",
    "        self.stickprob = stickprob\n",
    "        self.curac = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.supports_want_render = hasattr(env, \"supports_want_render\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.curac = None\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, ac):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        totrew = 0\n",
    "        for i in range(self.n):\n",
    "            # First step after reset, use action\n",
    "            if self.curac is None:\n",
    "                self.curac = ac\n",
    "            # First substep, delay with probability=stickprob\n",
    "            elif i == 0:\n",
    "                if self.rng.rand() > self.stickprob:\n",
    "                    self.curac = ac\n",
    "            # Second substep, new action definitely kicks in\n",
    "            elif i == 1:\n",
    "                self.curac = ac\n",
    "            if self.supports_want_render and i < self.n - 1:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(\n",
    "                    self.curac,\n",
    "                    want_render=False,\n",
    "                )\n",
    "            else:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(self.curac)\n",
    "            totrew += rew\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        return ob, totrew, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d2246aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f3cce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model = ConvDQN(input_shape, num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        transform = T.Lambda(lambda x: x.permute(0,3,1,2).reshape(-1, self.input_shape[1], self.input_shape[2]))\n",
    "        return transform(state)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = self.preprocess_wv(next_state)\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state.unsqueeze(0))).item()\n",
    "            state = self.preprocess_wv(state)\n",
    "            target_f = self.model(state.unsqueeze(0)).to(\"cpu\").detach().numpy()\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(torch.tensor(target_f).to(self.device), self.model(state.unsqueeze(0)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states_tensor)\n",
    "            max_next_q = torch.max(next_q_values, dim=1)[0]\n",
    "        target_q_values = rewards_tensor + self.gamma * max_next_q * (~dones_tensor)\n",
    "        current_q_values = self.model(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.epsilon > 0.05:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23d750b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvD3QN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.advance_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        advantages = self.advance_stream(conv_out)\n",
    "        value = self.value_stream(conv_out)\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b6de7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model_online = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model_online.parameters(), lr=lr)\n",
    "        self.update_target_freq = 10000\n",
    "        self.step_counter = 0\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        transform = T.Lambda(lambda x: x.permute(0,3,1,2).reshape(-1, self.input_shape[1], self.input_shape[2]))\n",
    "        return transform(state)\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model_online(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values_online = self.model_online(next_states_tensor)\n",
    "            best_action_online_indices = torch.argmax(next_q_values_online, dim=1).unsqueeze(1)\n",
    "            max_Q_next = self.model_target(next_states_tensor).gather(1, best_action_online_indices).squeeze()\n",
    "        target_q_values = rewards_tensor + self.gamma * max_Q_next * (~dones_tensor)\n",
    "        current_q_values = self.model_online(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.step_counter += 1\n",
    "        self.update_target_network()\n",
    "        if self.epsilon > 0.05:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        if self.step_counter % self.update_target_freq == 0:\n",
    "            self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPPOAgent:\n",
    "    def __init__(self, env, input_shape, clip=0.2, learning_rate=1e-4, gamma=0.99, timestep_per_batch=4800, max_timesteps_per_episode=1600, n_updates_per_iteration=5):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.ts_batch = timestep_per_batch\n",
    "        self.max_ts_ep = max_timesteps_per_episode\n",
    "        self.device = 'cuda'\n",
    "        self.model_actor = ConvDQN(input_shape, self.num_actions).to(self.device)\n",
    "        self.model_critic = ConvDQN(input_shape, 1).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.model_actor.parameters(), lr=self.lr)\n",
    "        self.critic_optimizer = optim.Adam(self.model_critic.parameters(), lr=self.lr)\n",
    "        self.cov_var = torch.full(size=(self.num_actions,), fill_value=0.5)\n",
    "        self.cov_mat = (torch.diag(self.cov_var)).to(self.device)\n",
    "        self.n_updates = n_updates_per_iteration\n",
    "        self.clip = clip\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        obs = self.preprocess_wv(obs)\n",
    "        mean = self.model_actor(obs.unsqueeze(0))\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return torch.argmax(action).item(), log_prob.detach()\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "        C_out = self.input_shape[0]\n",
    "        H_out = self.input_shape[1] \n",
    "        W_out = self.input_shape[2] \n",
    "        state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def rollout(self):\n",
    "        batch_obs = []      \n",
    "        batch_acts = []            \n",
    "        batch_log_probs = []       \n",
    "        batch_rews = []        \n",
    "        batch_rtgs = []            \n",
    "        batch_lens = []\n",
    "        t = 0 \n",
    "        while t < self.ts_batch:\n",
    "            ep_rews = []\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "            for ep_t in range(self.max_ts_ep):\n",
    "                t += 1\n",
    "                batch_obs.append(obs)\n",
    "                action, log_prob = self.get_action(obs)\n",
    "                obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                ep_rews.append(reward)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                if done:\n",
    "                    break\n",
    "            batch_lens.append(ep_t + 1)\n",
    "            batch_rews.append(ep_rews)\n",
    "        batch_obs = torch.stack([self.preprocess_wv(s) for s in batch_obs])\n",
    "        #batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.float, device=self.device)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float, device=self.device)\n",
    "        batch_rtgs = self.compute_rtgs(batch_rews)\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens\n",
    "    \n",
    "    def compute_rtgs(self, batch_rews):\n",
    "        batch_rtgs = []\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "            discounted_reward = 0 \n",
    "            for rew in reversed(ep_rews):\n",
    "                discounted_reward = rew + discounted_reward * self.gamma\n",
    "                batch_rtgs.insert(0, discounted_reward)\n",
    "        batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float, device=self.device)\n",
    "        return batch_rtgs\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        V = self.model_critic(batch_obs).squeeze()\n",
    "        mean = self.model_actor(batch_obs)\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        return V, log_probs\n",
    "    \n",
    "    def learn(self, total_timesteps):\n",
    "        act_t = 0\n",
    "        while act_t < total_timesteps:\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout()\n",
    "            act_t += np.sum(batch_lens)\n",
    "            print(act_t)\n",
    "            V, _ = self.evaluate(batch_obs, batch_acts)\n",
    "            A_k = batch_rtgs - V.detach()\n",
    "            #A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10) #Ejemplo usado para este PPO recomienda esto para dar mas estabilidad\n",
    "            for _ in range(self.n_updates):\n",
    "                V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "                ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "                surr1 = ratios * A_k\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "                critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                self.critic_optimizer.zero_grad()    \n",
    "                critic_loss.backward()    \n",
    "                self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "348c5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('No Enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8e187fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(*, game, state=None, max_episode_steps=4500, **kwargs):\n",
    "    if state is None:\n",
    "        state = retro.State.DEFAULT\n",
    "    env = retro.make(game, state, **kwargs)\n",
    "    env = ButtonActionWrapper(env)\n",
    "    #env = CustomRewardWrapper(env)\n",
    "    env = StochasticFrameSkip(env, n=num_frame_skip, stickprob=0.25)\n",
    "    if RESIZE_ENV:\n",
    "        input_shape = (num_stacked_frames*3, *new_size)\n",
    "        env = ResizeObservation(env, new_size)\n",
    "    else:\n",
    "        input_shape = (num_stacked_frames*3, 224, 320)\n",
    "    if max_episode_steps is not None:\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "    return env, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37280243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "3600\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The right-most size of value must match event_shape: torch.Size([3600]) vs torch.Size([7]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m LOAD_MODEL:\n\u001b[1;32m     17\u001b[0m     agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(torch\u001b[38;5;241m.\u001b[39mload(model_load_path, map_location\u001b[38;5;241m=\u001b[39magent\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m---> 19\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmax_steps_per_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m save_model(agent, num_episodes)\n\u001b[1;32m     21\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[29], line 93\u001b[0m, in \u001b[0;36mConvPPOAgent.learn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m     91\u001b[0m act_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(batch_lens)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(act_t)\n\u001b[0;32m---> 93\u001b[0m V, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_acts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m A_k \u001b[38;5;241m=\u001b[39m batch_rtgs \u001b[38;5;241m-\u001b[39m V\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10) #Ejemplo usado para este PPO recomienda esto para dar mas estabilidad\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 84\u001b[0m, in \u001b[0;36mConvPPOAgent.evaluate\u001b[0;34m(self, batch_obs, batch_acts)\u001b[0m\n\u001b[1;32m     82\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_actor(batch_obs)\n\u001b[1;32m     83\u001b[0m dist \u001b[38;5;241m=\u001b[39m MultivariateNormal(mean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_mat)\n\u001b[0;32m---> 84\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_acts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m V, log_probs\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py:253\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[0;32m--> 253\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     diff \u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\n\u001b[1;32m    255\u001b[0m     M \u001b[38;5;241m=\u001b[39m _batch_mahalanobis(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril, diff)\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/distributions/distribution.py:299\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    297\u001b[0m event_dim_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_shape)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39msize()[event_dim_start:] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_shape:\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe right-most size of value must match event_shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m     )\n\u001b[1;32m    303\u001b[0m actual_shape \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    304\u001b[0m expected_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_shape \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_shape\n",
      "\u001b[0;31mValueError\u001b[0m: The right-most size of value must match event_shape: torch.Size([3600]) vs torch.Size([7])."
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "#venv = VecTransposeImage(VecFrameStack(SubprocVecEnv([make_env] * 8), n_stack=4))\n",
    "if Model == \"DQN\":\n",
    "  agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"D3QN\":\n",
    "  agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model_online.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"PPO\":\n",
    "  agent = ConvPPOAgent(env=env, input_shape=input_shape, learning_rate=LR, gamma=GAMMA, timestep_per_batch=TIMESTEPS_PER_BATCH, max_timesteps_per_episode=MAX_TIMESTEPS_PER_EPISODE, clip=CLIP, n_updates_per_iteration=N_UPDATES_PER_ITERATION)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "\n",
    "agent.learn(num_episodes*max_steps_per_episode)\n",
    "save_model(agent, num_episodes)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/seba/Documentos/AI/RL/Video/DQN folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/tmp/ipykernel_6257/2426561334.py:14: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n"
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=f'../Video/{Model}',    # Folder to save videos\n",
    "    name_prefix=f'eval-V{version}-S{max_steps_per_episode*num_episodes}',               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: True    # Record every episode\n",
    ")\n",
    "dim = env.action_space.n\n",
    "print(action_dim)\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "if Model == \"DQN\":\n",
    "    model_load_path = get_last_modified_file(target_directory)\n",
    "    agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"D3QN\":  \n",
    "    model_load_path = get_last_modified_file(target_directory)\n",
    "    agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "    agent.model_online.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"PPO\":\n",
    "    agent = ConvPPOAgent(env=env, input_shape=input_shape, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Actor\")\n",
    "    agent.model_actor.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Critic\")\n",
    "    agent.model_critic.state_dict(torch.load(model_load_path, map_location=agent.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ae8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 0.0\n",
      "Episode: 1 Reward: 0.0\n",
      "Episode: 2 Reward: 0.0\n",
      "Episode: 3 Reward: 0.0\n",
      "Episode: 4 Reward: 0.0\n",
      "Episode: 5 Reward: 0.0\n",
      "Episode: 6 Reward: 0.0\n",
      "Episode: 7 Reward: 0.0\n",
      "Episode: 8 Reward: 0.0\n",
      "Episode: 9 Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "episode = 10\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state = obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
