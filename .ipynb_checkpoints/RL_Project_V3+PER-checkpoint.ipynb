{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cbe9c13-8df3-4c42-b865-e731851135be",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce02c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.distributions import MultivariateNormal, Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation, GrayscaleObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "import io\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b7406-06cd-4f78-8b7d-e8b0222c38a5",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc92308",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = False\n",
    "LOAD_MODEL = False\n",
    "RGB = False\n",
    "FRAMESTACK = True\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "num_episodes = 200\n",
    "max_steps_per_episode = 1800\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 4\n",
    "Model = \"DQN\"\n",
    "save_interval = 100\n",
    "episode_p_interval = 4\n",
    "rew_p_interval = 900\n",
    "\n",
    "#Version\n",
    "version = 6\n",
    "\n",
    "#Hiperparametros\n",
    "LR = 2e-5\n",
    "GAMMA = 0.999\n",
    "\n",
    "#DQN y D3QN Params\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.01#0.99 Exponential / 0.01 Reciprocal\n",
    "BUFFER_SIZE = 20000\n",
    "batch_size = 256\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "#D3QN\n",
    "UPDATE_TARGET_FREQ = 10000\n",
    "\n",
    "#PPO Params\n",
    "N_STEPS = 1024\n",
    "N_UPDATES_PER_ITERATION = 5\n",
    "CLIP = 0.1\n",
    "ENTROPY_COEF = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a252dd-f567-4a6a-96cc-a015f3459327",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a9764-302a-4fc9-8a2c-dd62f8201411",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c752692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last modified file is: ../Saved_Models/DQN/DQN-Sonic-V5-Ehalted-S1800.pth\n"
     ]
    }
   ],
   "source": [
    "def get_last_modified_file(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
    "        return None\n",
    "    files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=os.path.getmtime, reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "model_load_path = get_last_modified_file(target_directory)\n",
    "\n",
    "if model_load_path:\n",
    "    print(f\"The last modified file is: {model_load_path}\")\n",
    "else:\n",
    "    print(\"No files found in the directory or directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985a3502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 17] File exists: '../Saved_Models'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO/Actor'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/PPO/Critic'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/DQN'\n",
      "Error: [Errno 17] File exists: '../Saved_Models/D3QN'\n",
      "Error: [Errno 17] File exists: '../Video'\n",
      "Error: [Errno 17] File exists: '../Video/PPO'\n",
      "Error: [Errno 17] File exists: '../Video/DQN'\n",
      "Error: [Errno 17] File exists: '../Video/D3QN'\n",
      "Error: [Errno 17] File exists: '../Logs/PPO'\n",
      "Error: [Errno 17] File exists: '../Logs/DQN'\n",
      "Error: [Errno 17] File exists: '../Logs/D3QN'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"../Saved_Models\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Actor\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/PPO/Critic\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Saved_Models/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "try:\n",
    "    os.mkdir(\"../Video/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Video/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Logs/PPO\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "try:\n",
    "    os.mkdir(\"../Logs/DQN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"../Logs/D3QN\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937d962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/{Model}' #ppt para jit, pth para statedict\n",
    "    model_file_name = f'/{Model}-Sonic-V{version}-E{episode}-S{max_steps_per_episode}.pth'\n",
    "    try:\n",
    "        if Model == \"PPO\":\n",
    "            torch.save(agent.model_actor.state_dict(), model_save_path+\"/Actor\"+model_file_name)\n",
    "            torch.save(agent.model_critic.state_dict(), model_save_path+\"/Critic\"+model_file_name)\n",
    "        else:\n",
    "            torch.save(agent.model_online.state_dict(), model_save_path+model_file_name)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6229fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym-retro environment and make it use discrete\n",
    "    actions for the Sonic game.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ButtonActionWrapper, self).__init__(env)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],\n",
    "                   ['DOWN', 'B'], ['B']]\n",
    "        self._actions = []\n",
    "        for action in actions:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in action:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._actions.append(arr)\n",
    "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
    "\n",
    "    def action(self, a): # pylint: disable=W0221\n",
    "        return self._actions[a].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a2d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, mov_rew=1, score_rew=1, hp_rew=1, ring_rew=0.5, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= self.mov_rew\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4e5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticFrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, n, stickprob):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n = n\n",
    "        self.stickprob = stickprob\n",
    "        self.curac = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.supports_want_render = hasattr(env, \"supports_want_render\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.curac = None\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, ac):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        totrew = 0\n",
    "        for i in range(self.n):\n",
    "            # First step after reset, use action\n",
    "            if self.curac is None:\n",
    "                self.curac = ac\n",
    "            # First substep, delay with probability=stickprob\n",
    "            elif i == 0:\n",
    "                if self.rng.rand() > self.stickprob:\n",
    "                    self.curac = ac\n",
    "            # Second substep, new action definitely kicks in\n",
    "            elif i == 1:\n",
    "                self.curac = ac\n",
    "            if self.supports_want_render and i < self.n - 1:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(\n",
    "                    self.curac,\n",
    "                    want_render=False,\n",
    "                )\n",
    "            else:\n",
    "                ob, rew, terminated, truncated, info = self.env.step(self.curac)\n",
    "            totrew += rew\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        return ob, totrew, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e187fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(*, game, state=None, max_episode_steps=4500,log_mon = False, **kwargs):\n",
    "    if state is None:\n",
    "        state = retro.State.DEFAULT\n",
    "    env = retro.make(game, state, **kwargs)\n",
    "    env = ButtonActionWrapper(env)\n",
    "    #env = CustomRewardWrapper(env)\n",
    "    env = StochasticFrameSkip(env, n=num_frame_skip, stickprob=0.25)\n",
    "    if RGB:\n",
    "        channels = 3\n",
    "    else:\n",
    "        env = GrayscaleObservation(env)\n",
    "        channels = 1\n",
    "    if RESIZE_ENV:\n",
    "        input_shape = (num_stacked_frames*channels, *new_size)\n",
    "        env = ResizeObservation(env, new_size)\n",
    "    else:\n",
    "        input_shape = (num_stacked_frames*channels, 224, 320)\n",
    "    if FRAMESTACK:\n",
    "        env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "    if max_episode_steps is not None:\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    if log_mon:\n",
    "        env = Monitor(env, f'../Logs/{Model}/{Model}_V{version}_S{num_episodes*max_steps_per_episode}.csv')\n",
    "    return env, input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b01301-3ab5-4626-a718-dfdb3b7c604b",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bff4f99-d49d-4875-ad76-a8885b6e961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    def add(self, experience):\n",
    "        e = self.experience(*experience)\n",
    "        self.memory.append(e)\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "        minibatch_compatible = [[e] for e in experiences]\n",
    "        tree_indices = None \n",
    "        is_weights = None\n",
    "        return tree_indices, minibatch_compatible, is_weights\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc36ece-aa1d-4039-a9e0-574535ccaa2c",
   "metadata": {},
   "source": [
    "## PrioritizedExperienceReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5419f093-e0a6-4b8c-bfa7-9142bf8e0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    data_pointer = 0\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity \n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.current_size = 0\n",
    "        \n",
    "    def add(self, priority, data):\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update (tree_index, priority)\n",
    "        if self.current_size < self.capacity:\n",
    "            self.current_size += 1\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  \n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    def update(self, tree_index, priority):\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        while tree_index != 0: \n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "            \n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "        while True: \n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else: \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "        \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674f1a8b-961d-4314-ae83-094c7d661a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedExperienceReplay:\n",
    "    PER_e = 0.01  \n",
    "    PER_a = 0.6  \n",
    "    PER_b = 0.4  \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    absolute_error_upper = 1.  \n",
    "        \n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        self.tree.add(max_priority, experience)   \n",
    "        \n",
    "    def sample(self, n):\n",
    "        memory_b = []\n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        priority_segment = self.tree.total_priority / n \n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling]) \n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        for i in range(n):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "            b_idx[i]= index\n",
    "            experience = [data]\n",
    "            memory_b.append(experience)\n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "        \n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)\n",
    "    def __len__(self):\n",
    "        return self.tree.current_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6eed1d-f3c2-4f7e-a6e9-226975ec164b",
   "metadata": {},
   "source": [
    "# Models & Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a22dd-7ff2-4a35-afa6-988f0474d218",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2246aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3cce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, buffer_class, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000, update_target_freq=10000, epsilon_end=0.01):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.st = epsilon_end-((epsilon-epsilon_end)*(100.0/(1.0+epsilon_decay*180000.0)))\n",
    "        self.memory = buffer_class(buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model_online = ConvDQN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target = ConvDQN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model_online.parameters(), lr=lr)\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.step_counter = 0\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_start = epsilon\n",
    "        self.decay_steps = 0\n",
    "        self.scaler = GradScaler()\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device) / 255.0\n",
    "        if RGB and FRAMESTACK:\n",
    "            state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "            C_out = self.input_shape[0]\n",
    "            H_out = self.input_shape[1] \n",
    "            W_out = self.input_shape[2] \n",
    "            state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model_online(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        if self.step_counter % self.update_target_freq == 0:\n",
    "            self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        tree_indices, minibatch, IS_weights = self.memory.sample(batch_size)\n",
    "        experiences = [item[0] for item in minibatch]\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        if IS_weights is not None:\n",
    "            IS_weights_tensor = torch.tensor(IS_weights, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values_target = self.model_target(next_states_tensor)\n",
    "            max_Q_next = torch.max(next_q_values_target, dim=1).values\n",
    "            target_q_values = rewards_tensor + self.gamma * max_Q_next * (~dones_tensor)\n",
    "        current_q_values = self.model_online(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "\n",
    "        td_error = torch.abs(target_q_values - current_q_for_actions).detach().cpu().numpy()\n",
    "        self.optimizer.zero_grad()\n",
    "        if IS_weights is not None:\n",
    "            loss = (IS_weights_tensor.squeeze() * nn.MSELoss(reduction='none')(current_q_for_actions, target_q_values)).mean()\n",
    "        else:\n",
    "            loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model_online.parameters(), max_norm=10.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.memory.batch_update(tree_indices, td_error)\n",
    "        \n",
    "        self.step_counter += 1\n",
    "        self.update_target_network()\n",
    "        \n",
    "        self.decay()\n",
    "        \n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                if frame_count % 4 == 0:\n",
    "                    #for _ in range(4):\n",
    "                    self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")\n",
    "    def decay(self):\n",
    "        self.decay_steps += 1\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            if self.epsilon_decay > 0.5:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            else:\n",
    "                self.epsilon = max(self.epsilon_end,self.st+((self.epsilon_start-self.epsilon_end)*(100.0/(1.0+self.epsilon_decay*self.decay_steps))))\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a27117-1886-4b15-8749-644f6ca4d73c",
   "metadata": {},
   "source": [
    "## D3QN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23d750b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvD3QN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.advance_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        advantages = self.advance_stream(conv_out)\n",
    "        value = self.value_stream(conv_out)\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b6de7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvD3QNAgent:\n",
    "    def __init__(self, env, input_shape, num_actions, buffer_class, lr = 1e-4, gamma = 0.99, epsilon = 1.0, epsilon_decay = 0.99, buffer_size = 10000, update_target_freq=10000, epsilon_end = 0.01):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.st = epsilon_end-((epsilon-epsilon_end)*(100.0/(1.0+epsilon_decay*180000.0)))\n",
    "        self.memory = buffer_class(buffer_size)\n",
    "        self.device = 'cuda'\n",
    "        self.model_online = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target = ConvD3QN(input_shape, num_actions).to(self.device)\n",
    "        self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model_online.parameters(), lr=lr)\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.step_counter = 0\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_start = epsilon\n",
    "        self.decay_steps = 0\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        if RGB and FRAMESTACK:\n",
    "            state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "            C_out = self.input_shape[0]\n",
    "            H_out = self.input_shape[1] \n",
    "            W_out = self.input_shape[2] \n",
    "            state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess_wv(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model_online(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_vect(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        tree_indices, minibatch, IS_weights = self.memory.sample(batch_size)\n",
    "        experiences = [item[0] for item in minibatch]\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states_tensor = torch.stack([self.preprocess_wv(s) for s in states])\n",
    "        next_states_tensor = torch.stack([self.preprocess_wv(ns) for ns in next_states])\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "        if IS_weights is not None:\n",
    "            IS_weights_tensor = torch.tensor(IS_weights, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_q_values_online = self.model_online(next_states_tensor)\n",
    "            best_action_online_indices = torch.argmax(next_q_values_online, dim=1).unsqueeze(1)\n",
    "            max_Q_next = self.model_target(next_states_tensor).gather(1, best_action_online_indices).squeeze()\n",
    "        target_q_values = rewards_tensor + self.gamma * max_Q_next * (~dones_tensor)\n",
    "        current_q_values = self.model_online(states_tensor)\n",
    "        current_q_for_actions = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "\n",
    "        td_error = torch.abs(target_q_values - current_q_for_actions).detach().cpu().numpy()\n",
    "        self.optimizer.zero_grad()\n",
    "        if IS_weights is not None:\n",
    "            loss = (IS_weights_tensor.squeeze() * nn.MSELoss(reduction='none')(current_q_for_actions, target_q_values)).mean()\n",
    "        else:\n",
    "            loss = nn.MSELoss()(current_q_for_actions, target_q_values)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model_online.parameters(), max_norm=10.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.memory.batch_update(tree_indices, td_error)\n",
    "        \n",
    "        self.step_counter += 1\n",
    "        self.update_target_network()\n",
    "        \n",
    "        self.decay()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        if self.step_counter % self.update_target_freq == 0:\n",
    "            self.model_target.load_state_dict(self.model_online.state_dict())\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        temp_reward = 0\n",
    "        frame_count_prev = 0\n",
    "        frame_count = 0\n",
    "        ep_count = 0\n",
    "        while frame_count < total_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            ep_count += 1\n",
    "            while not done:\n",
    "                frame_count += 1\n",
    "                action = self.act(state = state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.remember(state, action, reward, observation, done)\n",
    "                state = observation\n",
    "                total_reward += reward\n",
    "                temp_reward += reward\n",
    "                if frame_count % rew_p_interval == 0:\n",
    "                    print(f'step n={frame_count} with reward {temp_reward}')\n",
    "                    temp_reward = 0\n",
    "                self.replay_vect(batch_size)\n",
    "\n",
    "            if (ep_count+1) % episode_p_interval == 0:\n",
    "                print(f'Episode {ep_count+1} \\nstep n={(frame_count-frame_count_prev)/episode_p_interval}\\nreward {temp_reward/episode_p_interval}\\n')\n",
    "                temp_reward = 0\n",
    "                frame_count_prev=frame_count\n",
    "            if (ep_count+1) % save_interval == 0:\n",
    "                save_model(self, ep_count)\n",
    "            print(f\"Episode finished with total reward: {total_reward}\")\n",
    "            \n",
    "    def decay():\n",
    "        self.decay_steps += 1\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            if self.epsilon_decay > 0.5:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            else:\n",
    "                self.epsilon = max(self.epsilon_end,self.st+((self.epsilon_start-self.epsilon_end)*(100.0/(1.0+self.epsilon_decay*self.decay_steps))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45787692-a026-4b6d-8aa3-f365b6760c87",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ba8d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPPOActor(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvPPOActor, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "            #nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        conv_out = self.fc_layers(conv_out)\n",
    "        dist = Categorical(logits=conv_out)\n",
    "        return dist\n",
    "    \n",
    "class ConvPPOCritic(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(ConvPPOCritic, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e59d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPPOAgent:\n",
    "    def __init__(self, env, input_shape, clip=0.2, learning_rate=1e-4, gamma=0.99, n_steps=2048, n_updates_per_iteration=5, entropy_coef=0.01, minibatch_size = 64, max_grad_norm=0.5, lam = 0.95):\n",
    "        self.env = env\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.n_steps = n_steps\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_actor = ConvPPOActor(input_shape, self.num_actions).to(self.device)\n",
    "        #self.model_actor.half()\n",
    "        self.model_critic = ConvPPOCritic(input_shape).to(self.device)\n",
    "        #self.model_critic.half()\n",
    "        self.actor_optimizer = optim.Adam(self.model_actor.parameters(), lr=self.lr)\n",
    "        self.critic_optimizer = optim.Adam(self.model_critic.parameters(), lr=self.lr)\n",
    "        self.n_updates = n_updates_per_iteration\n",
    "        self.clip = clip\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.mb_size = minibatch_size\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.lam = lam\n",
    "        self.csv_file = f'../Logs/{Model}/PPO.csv'\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        dist = self.model_actor(obs.unsqueeze(0))\n",
    "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.detach().squeeze()\n",
    "    \n",
    "    def preprocess_wv(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state_tensor = state_tensor / 255.0\n",
    "        if RGB and FRAMESTACK:\n",
    "            state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "            C_out = self.input_shape[0]\n",
    "            H_out = self.input_shape[1] \n",
    "            W_out = self.input_shape[2] \n",
    "            state_tensor = state_tensor.contiguous().view(C_out, H_out, W_out)\n",
    "        return state_tensor\n",
    "    \n",
    "    def rollout(self):\n",
    "        batch_obs = []      \n",
    "        batch_acts = []            \n",
    "        batch_log_probs = []       \n",
    "        batch_rews = []          \n",
    "        batch_lens = []     \n",
    "        batch_vals = []\n",
    "        batch_dones = []\n",
    "        ep_rews = []\n",
    "        ep_vals = []\n",
    "        ep_dones = []\n",
    "        t = 0 \n",
    "        if not hasattr(self, 'current_obs'):\n",
    "            obs, _ =self.env.reset()\n",
    "            self.current_obs = obs\n",
    "        obs = self.current_obs\n",
    "        with torch.no_grad():\n",
    "            while t < self.n_steps:\n",
    "                t+=1\n",
    "                obs_tensor = self.preprocess_wv(obs)\n",
    "                batch_obs.append(obs_tensor)\n",
    "                action, log_prob = self.get_action(obs_tensor)\n",
    "                val = self.model_critic(obs_tensor.unsqueeze(0)).detach().cpu().item()#.numpy().flatten()[0]\n",
    "                obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                ep_dones.append(done)\n",
    "                ep_rews.append(reward)\n",
    "                ep_vals.append(val)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                if done:\n",
    "                    batch_lens.append(len(ep_rews))\n",
    "                    batch_rews.append(ep_rews)\n",
    "                    batch_vals.append(ep_vals)\n",
    "                    batch_dones.append(ep_dones)\n",
    "                    obs, _ = self.env.reset()\n",
    "                    ep_rews = []\n",
    "                    ep_vals = []\n",
    "                    ep_dones = []\n",
    "                    self.current_obs = obs\n",
    "        if len(ep_rews) > 0:\n",
    "            batch_lens.append(len(ep_rews))\n",
    "            batch_rews.append(ep_rews)\n",
    "            batch_vals.append(ep_vals)\n",
    "            batch_dones.append(ep_dones)\n",
    "        final_obs_tensor = self.preprocess_wv(obs)\n",
    "        with torch.no_grad():\n",
    "            self.final_val = self.model_critic(final_obs_tensor.unsqueeze(0)).squeeze().cpu().item()\n",
    "\n",
    "        self.current_obs = obs\n",
    "        batch_obs = torch.stack(batch_obs)\n",
    "        #batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.long, device=self.device)\n",
    "        batch_log_probs = torch.stack(batch_log_probs).float()\n",
    "        #batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float32, device=self.device)\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens, batch_vals, batch_dones\n",
    "    \n",
    "    # def compute_rtgs(self, batch_rews):\n",
    "    #     batch_rtgs = []\n",
    "    #     for ep_rews in reversed(batch_rews):\n",
    "    #         discounted_reward = 0 \n",
    "    #         for rew in reversed(ep_rews):\n",
    "    #             discounted_reward = rew + discounted_reward * self.gamma\n",
    "    #             batch_rtgs.insert(0, discounted_reward)\n",
    "    #     batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float, device=self.device)\n",
    "    #     return batch_rtgs\n",
    "\n",
    "    def calculate_gae(self, rewards, values, dones):\n",
    "        batch_advantages = []\n",
    "        for i, (ep_rews, ep_vals, ep_dones) in enumerate(zip(rewards, values, dones)):\n",
    "            is_last_segment = (i == len(rewards) - 1)\n",
    "            advantages = []\n",
    "            last_advantage = 0\n",
    "            for t in reversed(range(len(ep_rews))):\n",
    "                is_terminal = ep_dones[t]\n",
    "                if is_terminal:\n",
    "                    next_val = 0\n",
    "                elif t == len(ep_rews) - 1 and is_last_segment:\n",
    "                    next_val = self.final_val\n",
    "                else:\n",
    "                    next_val = ep_vals[t+1]\n",
    "                delta = ep_rews[t] + self.gamma * next_val - ep_vals[t]\n",
    "                advantage = delta + self.gamma * self.lam * (1 - is_terminal) * last_advantage\n",
    "                last_advantage = advantage\n",
    "                advantages.insert(0, advantage)\n",
    "            batch_advantages.extend(advantages)\n",
    "        return torch.tensor(batch_advantages, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        V = self.model_critic(batch_obs).view(-1) #.squeeze()\n",
    "        dist = self.model_actor(batch_obs)\n",
    "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        entropy_loss = dist.entropy().mean()\n",
    "        return V, log_probs, entropy_loss\n",
    "    \n",
    "    def learn(self, total_timesteps):\n",
    "        act_t = 0\n",
    "        while act_t < total_timesteps:\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens, batch_vals, batch_dones = self.rollout()\n",
    "            act_t += np.sum(batch_lens)\n",
    "            \n",
    "            batch_obs = batch_obs.cpu()\n",
    "            batch_acts = batch_acts.cpu()\n",
    "            batch_log_probs = batch_log_probs.cpu()\n",
    "            \n",
    "            V_old_flat = [val for ep_vals in batch_vals for val in ep_vals]\n",
    "            V_old_flat = torch.tensor(V_old_flat, dtype=torch.float32).detach() #, device=self.device\n",
    "            A_k = self.calculate_gae(batch_rews, batch_vals, batch_dones)\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "            A_k = A_k.cpu()\n",
    "            # batch_acts = torch.tensor(batch_acts, dtype=torch.long, device=self.device)\n",
    "            # batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float, device=self.device)\n",
    "            batch_rtgs = A_k + V_old_flat\n",
    "            step = len(batch_obs)\n",
    "            inds = np.arange(step)\n",
    "            print(f\"Iteration {act_t}/{total_timesteps}, collected {np.sum(batch_lens)} steps\")\n",
    "            assert len(V_old_flat) == len(batch_obs), f\"Value mismatch: {len(V_old_flat)} vs {len(batch_obs)}\"\n",
    "            for _ in range(self.n_updates):\n",
    "                np.random.shuffle(inds)\n",
    "                for start in range(0, step, self.mb_size):\n",
    "                    end = start + self.mb_size\n",
    "                    idx = inds[start:end]\n",
    "                    mini_obs = batch_obs[idx].to(self.device)\n",
    "                    mini_acts = batch_acts[idx].to(self.device)\n",
    "                    mini_log_probs = batch_log_probs[idx].to(self.device)\n",
    "                    mini_advantage = A_k[idx].to(self.device)\n",
    "                    mini_rtgs = batch_rtgs[idx].to(self.device)\n",
    "                    V, curr_log_probs, curr_entropy_loss = self.evaluate(mini_obs, mini_acts)\n",
    "                    ratios = torch.exp(curr_log_probs - mini_log_probs)\n",
    "                    surr1 = ratios * mini_advantage\n",
    "                    surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * mini_advantage\n",
    "                    actor_loss = (-torch.min(surr1, surr2)).mean() - self.entropy_coef*curr_entropy_loss\n",
    "                    critic_loss = nn.MSELoss()(V, mini_rtgs)\n",
    "                    self.actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.model_actor.parameters(), self.max_grad_norm)\n",
    "                    self.actor_optimizer.step()\n",
    "                    self.critic_optimizer.zero_grad()    \n",
    "                    critic_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.model_critic.parameters(), self.max_grad_norm)\n",
    "                    self.critic_optimizer.step()\n",
    "            del batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens\n",
    "            del batch_vals, batch_dones, V_old_flat, A_k, batch_rtgs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d2dce-2735-4094-b5c5-4726d4b5baa6",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "348c5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Enviroment to close\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('No Enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37280243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "step n=5 with reward 0.0\n",
      "step n=10 with reward 0.0\n",
      "step n=15 with reward 0.0\n",
      "step n=20 with reward 0.0\n",
      "step n=25 with reward 0.0\n",
      "step n=30 with reward 16.12563246488571\n",
      "step n=35 with reward 22.7655987739563\n",
      "step n=40 with reward 16.12563246488571\n",
      "step n=45 with reward 11.38279938697815\n",
      "step n=50 with reward 5.691399693489075\n",
      "step n=55 with reward -11.38279938697815\n",
      "step n=60 with reward -7.5885329246521\n",
      "step n=65 with reward 4.742833077907562\n",
      "step n=70 with reward -1.897133231163025\n",
      "step n=75 with reward 4.742833077907562\n",
      "step n=80 with reward 6.639966309070587\n",
      "step n=85 with reward -5.691399693489075\n",
      "step n=90 with reward -10.434232771396637\n",
      "step n=95 with reward -9.485666155815125\n",
      "step n=100 with reward -6.639966309070587\n",
      "step n=105 with reward -1.897133231163025\n",
      "step n=110 with reward -10.434232771396637\n",
      "step n=115 with reward -6.639966309070587\n",
      "step n=120 with reward 0.0\n",
      "step n=125 with reward -4.742833077907562\n",
      "step n=130 with reward -11.38279938697815\n",
      "step n=135 with reward 0.0\n",
      "step n=140 with reward 0.0\n",
      "step n=145 with reward 0.0\n",
      "step n=150 with reward 0.0\n",
      "step n=155 with reward 0.0\n",
      "step n=160 with reward 2.8456998467445374\n",
      "step n=165 with reward 10.434232771396637\n",
      "step n=170 with reward 6.639966309070587\n",
      "step n=175 with reward 0.9485666155815125\n",
      "step n=180 with reward 4.742833077907562\n",
      "step n=185 with reward 7.5885329246521\n",
      "step n=190 with reward 0.9485666155815125\n",
      "step n=195 with reward -16.12563246488571\n",
      "step n=200 with reward -11.38279938697815\n",
      "step n=205 with reward -6.639966309070587\n",
      "step n=210 with reward 0.0\n",
      "step n=215 with reward 0.0\n",
      "step n=220 with reward 0.0\n",
      "step n=225 with reward 0.0\n",
      "step n=230 with reward 0.0\n",
      "step n=235 with reward 0.0\n",
      "step n=240 with reward 9.485666155815125\n",
      "step n=245 with reward 23.71416538953781\n",
      "step n=250 with reward 14.228499233722687\n",
      "step n=255 with reward -4.742833077907562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8740/3008021144.py:24: RuntimeWarning: divide by zero encountered in scalar power\n",
      "  max_weight = (p_min * n) ** (-self.PER_b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step n=260 with reward -3.79426646232605\n",
      "step n=265 with reward -5.691399693489075\n",
      "step n=270 with reward -8.537099540233612\n",
      "step n=275 with reward -17.074199080467224\n",
      "step n=280 with reward -7.5885329246521\n",
      "step n=285 with reward 0.0\n",
      "step n=290 with reward 0.0\n",
      "step n=295 with reward 0.0\n",
      "step n=300 with reward 0.0\n",
      "step n=305 with reward 0.0\n",
      "step n=310 with reward 0.0\n",
      "step n=315 with reward 0.0\n",
      "step n=320 with reward 0.0\n",
      "step n=325 with reward 0.0\n",
      "step n=330 with reward 18.97133231163025\n",
      "step n=335 with reward 19.91989892721176\n",
      "step n=340 with reward 18.97133231163025\n",
      "step n=345 with reward 13.279932618141174\n",
      "step n=350 with reward 36.045531272888184\n",
      "step n=355 with reward 35.096964716911316\n",
      "step n=360 with reward -0.9485666155815125\n",
      "step n=365 with reward -5.691399693489075\n",
      "step n=370 with reward -1.897133231163025\n",
      "step n=375 with reward -13.279932618141174\n",
      "step n=380 with reward -20.868465542793274\n",
      "step n=385 with reward -36.045531272888184\n",
      "step n=390 with reward -29.405565083026886\n",
      "step n=395 with reward -21.817032158374786\n",
      "step n=400 with reward -12.331366002559662\n",
      "step n=405 with reward 0.0\n",
      "step n=410 with reward 0.0\n",
      "step n=415 with reward 0.0\n",
      "step n=420 with reward 0.0\n",
      "step n=425 with reward 0.0\n",
      "step n=430 with reward 0.0\n",
      "step n=435 with reward 0.0\n",
      "step n=440 with reward 0.0\n",
      "step n=445 with reward 0.0\n",
      "step n=450 with reward 0.0\n",
      "step n=455 with reward 0.0\n",
      "step n=460 with reward 0.0\n",
      "step n=465 with reward 0.9485666155815125\n",
      "step n=470 with reward 19.91989892721176\n",
      "step n=475 with reward 0.9485666155815125\n",
      "step n=480 with reward 10.434232771396637\n",
      "step n=485 with reward 31.30269831418991\n",
      "step n=490 with reward 28.456998467445374\n",
      "step n=495 with reward 7.5885329246521\n",
      "step n=500 with reward -19.91989892721176\n",
      "step n=505 with reward -19.91989892721176\n",
      "step n=510 with reward -18.022765696048737\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m LOAD_MODEL:\n\u001b[1;32m     17\u001b[0m     agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(torch\u001b[38;5;241m.\u001b[39mload(model_load_path, map_location\u001b[38;5;241m=\u001b[39magent\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m---> 19\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmax_steps_per_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m save_model(agent, num_episodes)\n\u001b[1;32m     21\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[14], line 110\u001b[0m, in \u001b[0;36mConvDQNAgent.learn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m--> 110\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_vect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (ep_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m episode_p_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mstep n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(frame_count\u001b[38;5;241m-\u001b[39mframe_count_prev)\u001b[38;5;241m/\u001b[39mepisode_p_interval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mreward \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_reward\u001b[38;5;241m/\u001b[39mepisode_p_interval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 55\u001b[0m, in \u001b[0;36mConvDQNAgent.replay_vect\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mexperiences)\n\u001b[1;32m     54\u001b[0m states_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_wv(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states])\n\u001b[0;32m---> 55\u001b[0m next_states_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_wv(ns) \u001b[38;5;28;01mfor\u001b[39;00m ns \u001b[38;5;129;01min\u001b[39;00m next_states])\n\u001b[1;32m     56\u001b[0m actions_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     57\u001b[0m rewards_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(rewards, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[14], line 55\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mexperiences)\n\u001b[1;32m     54\u001b[0m states_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_wv(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states])\n\u001b[0;32m---> 55\u001b[0m next_states_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_wv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ns \u001b[38;5;129;01min\u001b[39;00m next_states])\n\u001b[1;32m     56\u001b[0m actions_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     57\u001b[0m rewards_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(rewards, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36mConvDQNAgent.preprocess_wv\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_wv\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 24\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RGB \u001b[38;5;129;01mand\u001b[39;00m FRAMESTACK:\n\u001b[1;32m     26\u001b[0m         state_tensor \u001b[38;5;241m=\u001b[39m state_tensor\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', log_mon = True, max_episode_steps=max_steps_per_episode) #rgb_array, scenario = 'contest'\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "#venv = VecTransposeImage(VecFrameStack(SubprocVecEnv([make_env] * 8), n_stack=4))\n",
    "if Model == \"DQN\":\n",
    "  agent = ConvDQNAgent(env=env, input_shape=input_shape, num_actions=action_dim, buffer_class=PrioritizedExperienceReplay, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE, update_target_freq=UPDATE_TARGET_FREQ)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"D3QN\":\n",
    "  agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, buffer_class=PrioritizedExperienceReplay, lr=LR, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE, update_target_freq=UPDATE_TARGET_FREQ)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model_online.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "if Model == \"PPO\":\n",
    "  agent = ConvPPOAgent(env=env, input_shape=input_shape, learning_rate=LR, gamma=GAMMA, n_steps=N_STEPS, clip=CLIP, n_updates_per_iteration=N_UPDATES_PER_ITERATION, entropy_coef=ENTROPY_COEF)\n",
    "  if LOAD_MODEL:\n",
    "    agent.model.state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "\n",
    "agent.learn(num_episodes*max_steps_per_episode)\n",
    "save_model(agent, num_episodes)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf030d1-f77e-4cf8-aecb-cd92c64d7a02",
   "metadata": {},
   "source": [
    "# Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a8c0991-dab6-4d4d-b340-d3a541136e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Enviroment to close\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('No Enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d648480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/seba/Documentos/AI/RL/Video/DQN folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConvD3QN:\n\tMissing key(s) in state_dict: \"advance_stream.0.weight\", \"advance_stream.0.bias\", \"advance_stream.2.weight\", \"advance_stream.2.bias\", \"value_stream.0.weight\", \"value_stream.0.bias\", \"value_stream.2.weight\", \"value_stream.2.bias\". \n\tUnexpected key(s) in state_dict: \"fc_layers.0.weight\", \"fc_layers.0.bias\", \"fc_layers.2.weight\", \"fc_layers.2.bias\". \n\tsize mismatch for conv_layers.4.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 2, 2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m model_load_path \u001b[38;5;241m=\u001b[39m get_last_modified_file(target_directory)\n\u001b[1;32m     21\u001b[0m agent \u001b[38;5;241m=\u001b[39m ConvD3QNAgent(env\u001b[38;5;241m=\u001b[39menv, input_shape\u001b[38;5;241m=\u001b[39minput_shape, num_actions\u001b[38;5;241m=\u001b[39maction_dim, buffer_class\u001b[38;5;241m=\u001b[39mPrioritizedExperienceReplay, lr\u001b[38;5;241m=\u001b[39mLR, gamma\u001b[38;5;241m=\u001b[39mGAMMA, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, epsilon_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9955\u001b[39m, buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_online\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_load_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m agent\u001b[38;5;241m.\u001b[39mmodel_target\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_load_path, map_location\u001b[38;5;241m=\u001b[39magent\u001b[38;5;241m.\u001b[39mdevice))\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2624\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2619\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2620\u001b[0m             ),\n\u001b[1;32m   2621\u001b[0m         )\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2626\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2627\u001b[0m         )\n\u001b[1;32m   2628\u001b[0m     )\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConvD3QN:\n\tMissing key(s) in state_dict: \"advance_stream.0.weight\", \"advance_stream.0.bias\", \"advance_stream.2.weight\", \"advance_stream.2.bias\", \"value_stream.0.weight\", \"value_stream.0.bias\", \"value_stream.2.weight\", \"value_stream.2.bias\". \n\tUnexpected key(s) in state_dict: \"fc_layers.0.weight\", \"fc_layers.0.bias\", \"fc_layers.2.weight\", \"fc_layers.2.bias\". \n\tsize mismatch for conv_layers.4.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 2, 2])."
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', max_episode_steps=max_steps_per_episode) #rgb_array\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=f'../Video/{Model}',    # Folder to save videos\n",
    "    name_prefix=f'eval-V{version}-S{max_steps_per_episode*num_episodes}',               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: True    # Record every episode\n",
    ")\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "target_directory = f\"../Saved_Models/{Model}\"  # Replace with your directory path\n",
    "if Model == \"PPO\":\n",
    "    agent = ConvPPOAgent(env=env, input_shape=input_shape)\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Actor\")\n",
    "    agent.model_actor.load_state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_actor.eval()\n",
    "    model_load_path = get_last_modified_file(target_directory+\"/Critic\")\n",
    "    agent.model_critic.load_state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_critic.eval()\n",
    "else:  \n",
    "    model_load_path = get_last_modified_file(target_directory)\n",
    "    agent = ConvD3QNAgent(env=env, input_shape=input_shape, num_actions=action_dim, buffer_class=PrioritizedExperienceReplay, lr=LR, gamma=GAMMA, epsilon=0.05, epsilon_decay=0.9955, buffer_size=20000)\n",
    "    agent.model_online.load_state_dict(torch.load(model_load_path, map_location=agent.device))\n",
    "    agent.model_target.load_state_dict(torch.load(model_load_path, map_location=agent.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ae8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 10\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if Model == \"PPO\":\n",
    "            obs = agent.preprocess_wv(state=obs)\n",
    "            action = agent.get_action(obs = obs)[0]\n",
    "        else:\n",
    "            action = agent.act(state = obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf7e07-5e51-4ada-8dd7-cb360ec03e66",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3488e98-9a4d-401c-bbc4-1edb3d89b34a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FrameStackObservation' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env, input_shape \u001b[38;5;241m=\u001b[39m make_env(game\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSonicTheHedgehog-Genesis\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m, scenario \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontest\u001b[39m\u001b[38;5;124m'\u001b[39m, log_mon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, max_episode_steps\u001b[38;5;241m=\u001b[39mmax_steps_per_episode) \u001b[38;5;66;03m#rgb_array, scenario = 'contest'\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FrameStackObservation' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "env, input_shape = make_env(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array', scenario = 'contest', log_mon = False, max_episode_steps=max_steps_per_episode) #rgb_array, scenario = 'contest'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc0f1564-6593-4c04-a5e0-b4b42fc3fde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (4, 224, 320), uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b495fdb-945c-49ce-9399-4c1ca922ebbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18067\n",
      "68191\n",
      "180001\n",
      "-0.044969461410327594\n"
     ]
    }
   ],
   "source": [
    "i = 1.0\n",
    "steps = 0\n",
    "flag = True\n",
    "flag2 = True\n",
    "ep_end = 0.01\n",
    "ep_start = 1.0\n",
    "c = 0.01\n",
    "st = ep_end-((ep_start-ep_end)*(100.0/(1.0+c*180000.0)))\n",
    "while i > ep_end:\n",
    "    steps+=1\n",
    "    # et = max(end, end+(initial-end)*(1/(1+c*steps))) Reciprocal\n",
    "    i = max(ep_end,st+((ep_start-ep_end)*(100.0/(1.0+c*steps))))\n",
    "    # i = i*decay Exponential\n",
    "    # i = i*0.999\n",
    "    #i = (89.0/1800.0)+(100.0/steps)#Rational\n",
    "    if i<0.5 and flag:\n",
    "        flag = False\n",
    "        print(steps)\n",
    "    if i<0.1 and flag2:\n",
    "        flag2 = False\n",
    "        print(steps)\n",
    "print(steps)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "711ce5cf-00bf-4701-8fca-ed9ebf66a152",
   "metadata": {},
   "source": [
    "end, half, 1/10\n",
    "299, 69, 230 (Exponential, decay = 0.99)\n",
    "2995, 693, 2302 (Exponential, decay = 0.999)\n",
    "180000, 4106, 31357 (Reciprocal, c = 0.05)\n",
    "180000, 34008, 122035 (Reciprocal, c = 0.005)\n",
    "180000, 18797, 92359 (Reciprocal, c = 0.01) \n",
    "180000, 96450, 164258 (Reciprocal, c = 0.001) X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "968920ca-c8ff-41df-91fa-8359a2d2d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exitosamente guardado en ../Saved_Models/DQN\n"
     ]
    }
   ],
   "source": [
    "save_model(agent, 'halted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c1b67-97aa-47e0-8786-7f8620873053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
