{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 23421,
     "status": "ok",
     "timestamp": 1758605515480,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "uYoFOi1N7Kg8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758605515508,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "dIX408-VQbQi"
   },
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = True\n",
    "LOAD_MODEL = False\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "batch_size = 32\n",
    "num_episodes = 200\n",
    "max_episode_steps = 5400\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 2\n",
    "version = 2\n",
    "prev_model = 'DQN-Sonic-V1-E15-S5400.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758605515485,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "TKpUD3uBV687"
   },
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper that maps discrete actions to a set of button presses for the game.\n",
    "    This simplifies the action space for the agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, buttons):\n",
    "        super().__init__(env)\n",
    "        self.buttons = buttons\n",
    "        # Create a mapping from a single action index to the full button array.\n",
    "        self._actions = np.identity(len(buttons), dtype=np.int8)\n",
    "        self.action_space = gym.spaces.Discrete(len(buttons))\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(self._actions[action])\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758605515487,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "nJBsk4nqTB6B"
   },
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Custom reward shaping to encourage forward movement.\n",
    "    This wrapper modifies the reward based on the agent's horizontal position.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, mov_rew=0.01, score_rew=0.05, hp_rew=4, ring_rew=1, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= self.mov_rew\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758605515489,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "dWSxhNSlGiUa"
   },
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 16, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1758605515501,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "ceJ7e377Gizg"
   },
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, input_shape, num_actions, lr, gamma, epsilon, epsilon_decay, buffer_size):\n",
    "        self.input_shape = input_shape # input_shape is (Stack, Height, Width) or (Stack, Height, Width, Channels)\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        # Correct the input shape calculation for ConvDQN to be (Stack * Channels, Height, Width)\n",
    "        # Assuming input_shape is (Stack, Height, Width) and images are RGB (3 channels)\n",
    "        num_channels = 3 # Assuming RGB images\n",
    "        # The input shape from the environment after wrappers is (Stack, Height, Width, Channels)\n",
    "        # We need to transform it to (Stack * Channels, Height, Width) for the ConvDQN\n",
    "        conv_input_shape = (input_shape[0] * num_channels, *input_shape[1:])\n",
    "\n",
    "        # If the original input_shape included channels, we need to adjust\n",
    "        # Assuming input_shape is (Stack, Height, Width, Channels)\n",
    "        if len(input_shape) == 4:\n",
    "             # input_shape is (Stack, Height, Width, Channels)\n",
    "             # We want (Stack * Channels, Height, Width)\n",
    "             conv_input_shape = (input_shape[0] * input_shape[3], input_shape[1], input_shape[2])\n",
    "\n",
    "        self.device = 'cuda' #if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = ConvDQN(conv_input_shape, num_actions).to(self.device)\n",
    "        #dummy_input = torch.randn(1, conv_input_shape[0], conv_input_shape[1], conv_input_shape[2]).to(self.device)\n",
    "        #self.model = torch.jit.trace(ConvDQN(conv_input_shape, num_actions).to(self.device), dummy_input)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        # Convert the numpy array (Stack, Height, Width, Channels) to a PyTorch tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        # Permute dimensions from (Stack, Height, Width, Channels) to (Stack, Channels, Height, Width)\n",
    "        state = state.permute(0, 3, 1, 2)\n",
    "        # Reshape to (Stack * Channels, Height, Width)\n",
    "        state = state.reshape(-1, state.shape[2], state.shape[3])\n",
    "        return state\n",
    "    \n",
    "    def preprocess_vectorized(self, state):\n",
    "        is_batch = state.ndim == 5\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        if is_batch:\n",
    "            state = state.permute(0, 1, 4, 2, 3)\n",
    "            state = state.reshape(-1, state.shape[2], state.shape[3], state.shape[4])\n",
    "        else:\n",
    "            state = state.permute(0, 3, 1, 2)\n",
    "            state = state.reshape(-1, state.shape[2], state.shape[3])\n",
    "        return state\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess(state).unsqueeze(0).to(self.device)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = self.preprocess(next_state).unsqueeze(0).to(self.device) # Move to the correct device\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "            state = self.preprocess(state).unsqueeze(0).to(self.device)\n",
    "            current_q_values = self.model(state)\n",
    "            target_f = current_q_values.clone().detach()\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(target_f, current_q_values)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def replay_vectorized(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        # Unzip the minibatch into separate lists\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        # Convert lists to tensors and move them to the device\n",
    "        states_tensor = torch.tensor(states, device=self.device)\n",
    "        next_states_tensor = torch.tensor(next_states, device=self.device)\n",
    "        \n",
    "        # Preprocess all states and next states at once\n",
    "        states_tensor = self.preprocess_vectorized(states_tensor)\n",
    "        next_states_tensor = self.preprocess_vectorized(next_states_tensor)\n",
    "        \n",
    "        # Get current Q-values for all states in the batch\n",
    "        current_q_values = self.model(states_tensor)\n",
    "        \n",
    "        # Get max Q-values for all next states in the batch\n",
    "        next_q_values = self.model(next_states_tensor)\n",
    "        max_next_q = torch.max(next_q_values, 1)[0]\n",
    "        \n",
    "        # Calculate target Q-values using the Bellman equation\n",
    "        # Create a new tensor for the targets\n",
    "        target_q_values = rewards + self.gamma * max_next_q * (~dones)\n",
    "        \n",
    "        # Create the tensor for target_f\n",
    "        target_f = current_q_values.clone().detach()\n",
    "\n",
    "        # Update the Q-value for the action that was taken\n",
    "        target_f[range(batch_size), actions.long()] = target_q_values\n",
    "\n",
    "        # Perform a single optimization step for the entire batch\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(target_f, current_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('no enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26132141,
     "status": "ok",
     "timestamp": 1758631647667,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "NNa_hjqQhy2S",
    "outputId": "c1030ee9-1e71-465a-be7a-7e279d83b62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "step n=5 with reward -0.02\n",
      "step n=10 with reward -0.02\n",
      "step n=15 with reward -0.02\n",
      "step n=20 with reward -0.02\n",
      "step n=25 with reward -0.02\n",
      "step n=30 with reward -0.02\n",
      "step n=35 with reward -0.02\n",
      "step n=40 with reward -0.02\n",
      "step n=45 with reward -0.02\n",
      "step n=50 with reward 0.0\n",
      "step n=55 with reward 0.0\n",
      "step n=60 with reward 0.0\n",
      "step n=65 with reward 0.0\n",
      "step n=70 with reward 0.0\n",
      "step n=75 with reward -0.02\n",
      "step n=80 with reward -0.02\n",
      "step n=85 with reward -0.02\n",
      "step n=90 with reward -0.02\n",
      "step n=95 with reward -0.02\n",
      "step n=100 with reward -0.02\n",
      "step n=105 with reward -0.02\n",
      "step n=110 with reward -0.02\n",
      "step n=115 with reward -0.02\n",
      "step n=120 with reward -0.02\n",
      "step n=125 with reward -0.02\n",
      "step n=130 with reward -0.02\n",
      "step n=135 with reward -0.02\n",
      "step n=140 with reward 0.02\n",
      "step n=145 with reward 0.0\n",
      "step n=150 with reward 0.0\n",
      "step n=155 with reward -0.02\n",
      "step n=160 with reward 0.0\n",
      "step n=165 with reward 0.0\n",
      "step n=170 with reward -0.02\n",
      "step n=175 with reward -0.02\n",
      "step n=180 with reward -0.02\n",
      "step n=185 with reward -0.02\n",
      "step n=190 with reward -0.02\n",
      "step n=195 with reward -0.02\n",
      "step n=200 with reward -0.02\n",
      "step n=205 with reward 0.02\n",
      "step n=210 with reward 0.0\n",
      "step n=215 with reward 0.0\n",
      "step n=220 with reward 0.0\n",
      "step n=225 with reward 0.0\n",
      "step n=230 with reward 0.0\n",
      "step n=235 with reward -0.02\n",
      "step n=240 with reward -0.02\n",
      "step n=245 with reward -0.02\n",
      "step n=250 with reward -0.02\n",
      "step n=255 with reward -0.02\n",
      "step n=260 with reward -0.02\n",
      "step n=265 with reward -0.02\n",
      "step n=270 with reward -0.02\n",
      "step n=275 with reward -0.02\n",
      "step n=280 with reward -0.02\n",
      "step n=285 with reward -0.02\n",
      "step n=290 with reward -0.02\n",
      "step n=295 with reward -0.02\n",
      "step n=300 with reward -0.02\n",
      "step n=305 with reward -0.02\n",
      "step n=310 with reward -0.02\n",
      "step n=315 with reward -0.02\n",
      "step n=320 with reward -0.02\n",
      "step n=325 with reward 0.0\n",
      "step n=330 with reward 0.0\n",
      "step n=335 with reward 0.0\n",
      "step n=340 with reward 0.0\n",
      "step n=345 with reward -0.02\n",
      "step n=350 with reward -0.02\n",
      "step n=355 with reward 0.0\n",
      "step n=360 with reward -0.02\n",
      "step n=365 with reward -0.02\n",
      "step n=370 with reward -0.02\n",
      "step n=375 with reward -0.02\n",
      "step n=380 with reward -0.02\n",
      "step n=385 with reward -0.02\n",
      "step n=390 with reward -0.02\n",
      "step n=395 with reward -0.02\n",
      "step n=400 with reward -0.02\n",
      "step n=405 with reward -0.02\n",
      "step n=410 with reward -0.02\n",
      "step n=415 with reward -0.02\n",
      "step n=420 with reward -0.02\n",
      "step n=425 with reward -0.02\n",
      "step n=430 with reward -0.02\n",
      "step n=435 with reward -0.02\n",
      "step n=440 with reward -0.02\n",
      "step n=445 with reward -0.02\n",
      "step n=450 with reward -0.02\n",
      "step n=455 with reward -0.02\n",
      "step n=460 with reward -0.02\n",
      "step n=465 with reward -0.02\n",
      "step n=470 with reward -0.02\n",
      "step n=475 with reward -0.02\n",
      "step n=480 with reward -0.02\n",
      "step n=485 with reward -0.02\n",
      "step n=490 with reward -0.02\n",
      "step n=495 with reward -0.02\n",
      "step n=500 with reward 0.0\n",
      "step n=505 with reward 0.0\n",
      "step n=510 with reward -0.02\n",
      "step n=515 with reward 0.0\n",
      "step n=520 with reward 0.0\n",
      "step n=525 with reward 0.02\n",
      "step n=530 with reward 0.02\n",
      "step n=535 with reward 0.0\n",
      "step n=540 with reward 0.02\n",
      "step n=545 with reward 0.0\n",
      "step n=550 with reward -0.02\n",
      "step n=555 with reward -0.02\n",
      "step n=560 with reward -0.02\n",
      "step n=565 with reward -0.02\n",
      "step n=570 with reward -0.02\n",
      "step n=575 with reward -0.02\n",
      "step n=580 with reward -0.02\n",
      "step n=585 with reward -0.02\n",
      "step n=590 with reward -0.02\n",
      "step n=595 with reward -0.02\n",
      "step n=600 with reward -0.02\n",
      "step n=605 with reward -0.02\n",
      "step n=610 with reward -0.02\n",
      "step n=615 with reward -0.02\n",
      "step n=620 with reward -0.02\n",
      "step n=625 with reward -0.02\n",
      "step n=630 with reward -0.02\n",
      "step n=635 with reward -0.02\n",
      "step n=640 with reward -0.02\n",
      "step n=645 with reward -0.02\n",
      "step n=650 with reward -0.02\n",
      "step n=655 with reward -0.02\n",
      "step n=660 with reward -0.02\n",
      "step n=665 with reward -0.02\n",
      "step n=670 with reward -0.02\n",
      "step n=675 with reward -0.02\n",
      "step n=680 with reward -0.02\n",
      "step n=685 with reward -0.02\n",
      "step n=690 with reward -0.02\n",
      "step n=695 with reward -0.02\n",
      "step n=700 with reward -0.02\n",
      "step n=705 with reward -0.02\n",
      "step n=710 with reward -0.02\n",
      "step n=715 with reward -0.02\n",
      "step n=720 with reward -0.02\n",
      "step n=725 with reward -0.02\n",
      "step n=730 with reward -0.02\n",
      "step n=735 with reward -0.02\n",
      "step n=740 with reward -0.02\n",
      "step n=745 with reward -0.02\n",
      "step n=750 with reward -0.02\n",
      "step n=755 with reward -0.02\n",
      "step n=760 with reward -0.02\n",
      "step n=765 with reward -0.02\n",
      "step n=770 with reward -0.02\n",
      "step n=775 with reward -0.02\n",
      "step n=780 with reward -0.02\n",
      "step n=785 with reward -0.02\n",
      "step n=790 with reward -0.02\n",
      "step n=795 with reward -0.02\n",
      "step n=800 with reward -0.02\n",
      "step n=805 with reward -0.02\n",
      "step n=810 with reward -0.02\n",
      "step n=815 with reward -0.02\n",
      "step n=820 with reward -0.02\n",
      "step n=825 with reward -0.02\n",
      "step n=830 with reward -0.02\n",
      "step n=835 with reward 0.0\n",
      "step n=840 with reward 0.0\n",
      "step n=845 with reward -0.02\n",
      "step n=850 with reward 0.0\n",
      "step n=855 with reward 0.0\n",
      "step n=860 with reward -0.02\n",
      "step n=865 with reward -0.02\n",
      "step n=870 with reward -0.02\n",
      "step n=875 with reward -0.02\n",
      "step n=880 with reward -0.02\n",
      "step n=885 with reward -0.02\n",
      "step n=890 with reward -0.02\n",
      "step n=895 with reward -0.02\n",
      "step n=900 with reward -0.02\n",
      "step n=905 with reward -0.02\n",
      "step n=910 with reward -0.02\n",
      "step n=915 with reward -0.02\n",
      "step n=920 with reward -0.02\n",
      "step n=925 with reward -0.02\n",
      "step n=930 with reward -0.02\n",
      "step n=935 with reward -0.02\n",
      "step n=940 with reward -0.02\n",
      "step n=945 with reward -0.02\n",
      "step n=950 with reward -0.02\n",
      "step n=955 with reward -0.02\n",
      "step n=960 with reward 0.0\n",
      "step n=965 with reward 0.0\n",
      "step n=970 with reward -0.02\n",
      "step n=975 with reward 0.0\n",
      "step n=980 with reward 0.0\n",
      "step n=985 with reward -0.02\n",
      "step n=990 with reward -0.02\n",
      "step n=995 with reward -0.02\n",
      "step n=1000 with reward -0.02\n",
      "step n=1005 with reward -0.02\n",
      "step n=1010 with reward -0.02\n",
      "step n=1015 with reward -0.02\n",
      "step n=1020 with reward -0.02\n",
      "step n=1025 with reward -0.02\n",
      "step n=1030 with reward -0.02\n",
      "step n=1035 with reward -0.02\n",
      "step n=1040 with reward -0.02\n",
      "step n=1045 with reward -0.02\n",
      "step n=1050 with reward 0.0\n",
      "step n=1055 with reward 0.0\n",
      "step n=1060 with reward -0.02\n",
      "step n=1065 with reward -0.02\n",
      "step n=1070 with reward -0.02\n",
      "step n=1075 with reward 0.0\n",
      "step n=1080 with reward -0.02\n",
      "step n=1085 with reward -0.02\n",
      "step n=1090 with reward -0.02\n",
      "step n=1095 with reward -0.02\n",
      "step n=1100 with reward -0.02\n",
      "step n=1105 with reward -0.02\n",
      "step n=1110 with reward -0.02\n",
      "step n=1115 with reward -0.02\n",
      "step n=1120 with reward -0.02\n",
      "step n=1125 with reward -0.02\n",
      "step n=1130 with reward -0.02\n",
      "step n=1135 with reward -0.02\n",
      "step n=1140 with reward -0.02\n",
      "step n=1145 with reward -0.02\n",
      "step n=1150 with reward -0.02\n",
      "step n=1155 with reward -0.02\n",
      "step n=1160 with reward -0.02\n",
      "step n=1165 with reward -0.02\n",
      "step n=1170 with reward -0.02\n",
      "step n=1175 with reward -0.02\n",
      "step n=1180 with reward -0.02\n",
      "step n=1185 with reward -0.02\n",
      "step n=1190 with reward -0.02\n",
      "step n=1195 with reward -0.02\n",
      "step n=1200 with reward -0.02\n",
      "step n=1205 with reward -0.02\n",
      "step n=1210 with reward -0.02\n",
      "step n=1215 with reward -0.02\n",
      "step n=1220 with reward -0.02\n",
      "step n=1225 with reward -0.02\n",
      "step n=1230 with reward -0.02\n",
      "step n=1235 with reward -0.02\n",
      "step n=1240 with reward -0.02\n",
      "step n=1245 with reward -0.02\n",
      "step n=1250 with reward -0.02\n",
      "step n=1255 with reward -0.02\n",
      "step n=1260 with reward -0.02\n",
      "step n=1265 with reward 0.0\n",
      "step n=1270 with reward 0.0\n",
      "step n=1275 with reward 0.0\n",
      "step n=1280 with reward -0.02\n",
      "step n=1285 with reward 0.0\n",
      "step n=1290 with reward -0.02\n",
      "step n=1295 with reward -0.02\n",
      "step n=1300 with reward -0.02\n",
      "step n=1305 with reward -0.02\n",
      "step n=1310 with reward -0.02\n",
      "step n=1315 with reward -0.02\n",
      "step n=1320 with reward -0.02\n",
      "step n=1325 with reward -0.02\n",
      "step n=1330 with reward -0.02\n",
      "step n=1335 with reward -0.02\n",
      "step n=1340 with reward -0.02\n",
      "step n=1345 with reward -0.02\n",
      "step n=1350 with reward -0.02\n",
      "step n=1355 with reward -0.02\n",
      "step n=1360 with reward -0.02\n",
      "step n=1365 with reward -0.02\n",
      "step n=1370 with reward -0.02\n",
      "step n=1375 with reward -0.02\n",
      "step n=1380 with reward -0.02\n",
      "step n=1385 with reward -0.02\n",
      "step n=1390 with reward -0.02\n",
      "step n=1395 with reward -0.02\n",
      "step n=1400 with reward -0.02\n",
      "step n=1405 with reward -0.02\n",
      "step n=1410 with reward -0.02\n",
      "step n=1415 with reward -0.02\n",
      "step n=1420 with reward -0.02\n",
      "step n=1425 with reward -0.02\n",
      "step n=1430 with reward -0.02\n",
      "step n=1435 with reward -0.02\n",
      "step n=1440 with reward -0.02\n",
      "step n=1445 with reward -0.02\n",
      "step n=1450 with reward -0.02\n",
      "step n=1455 with reward -0.02\n",
      "step n=1460 with reward -0.02\n",
      "step n=1465 with reward -0.02\n",
      "step n=1470 with reward -0.02\n",
      "step n=1475 with reward -0.02\n",
      "step n=1480 with reward -0.02\n",
      "step n=1485 with reward -0.02\n",
      "step n=1490 with reward -0.02\n",
      "step n=1495 with reward -0.02\n",
      "step n=1500 with reward -0.02\n",
      "step n=1505 with reward -0.02\n",
      "step n=1510 with reward 0.0\n",
      "step n=1515 with reward 0.0\n",
      "step n=1520 with reward 0.0\n",
      "step n=1525 with reward 0.0\n",
      "step n=1530 with reward -0.02\n",
      "step n=1535 with reward 0.0\n",
      "step n=1540 with reward -0.02\n",
      "step n=1545 with reward -0.02\n",
      "step n=1550 with reward -0.02\n",
      "step n=1555 with reward -0.02\n",
      "step n=1560 with reward -0.02\n",
      "step n=1565 with reward -0.02\n",
      "step n=1570 with reward -0.02\n",
      "step n=1575 with reward -0.02\n",
      "step n=1580 with reward 0.0\n",
      "step n=1585 with reward 0.02\n",
      "step n=1590 with reward 0.02\n",
      "step n=1595 with reward 0.0\n",
      "step n=1600 with reward 0.0\n",
      "step n=1605 with reward 0.0\n",
      "step n=1610 with reward -0.02\n",
      "step n=1615 with reward -0.02\n",
      "step n=1620 with reward -0.02\n",
      "step n=1625 with reward -0.02\n",
      "step n=1630 with reward -0.02\n",
      "step n=1635 with reward -0.02\n",
      "step n=1640 with reward -0.02\n",
      "step n=1645 with reward -0.02\n",
      "step n=1650 with reward -0.02\n",
      "step n=1655 with reward -0.02\n",
      "step n=1660 with reward -0.02\n",
      "step n=1665 with reward -0.02\n",
      "step n=1670 with reward -0.02\n",
      "step n=1675 with reward -0.02\n",
      "step n=1680 with reward -0.02\n",
      "step n=1685 with reward -0.02\n",
      "step n=1690 with reward -0.02\n",
      "step n=1695 with reward -0.02\n",
      "step n=1700 with reward -0.02\n",
      "step n=1705 with reward -0.02\n",
      "step n=1710 with reward -0.02\n",
      "step n=1715 with reward -0.02\n",
      "step n=1720 with reward -0.02\n",
      "step n=1725 with reward -0.02\n",
      "step n=1730 with reward -0.02\n",
      "step n=1735 with reward -0.02\n",
      "step n=1740 with reward -0.02\n",
      "step n=1745 with reward -0.02\n",
      "step n=1750 with reward -0.02\n",
      "step n=1755 with reward -0.02\n",
      "step n=1760 with reward -0.02\n",
      "step n=1765 with reward -0.02\n",
      "step n=1770 with reward -0.02\n",
      "step n=1775 with reward -0.02\n",
      "step n=1780 with reward -0.02\n",
      "step n=1785 with reward -0.02\n",
      "step n=1790 with reward -0.02\n",
      "step n=1795 with reward -0.02\n",
      "step n=1800 with reward -0.02\n",
      "step n=1805 with reward -0.02\n",
      "step n=1810 with reward -0.02\n",
      "step n=1815 with reward -0.02\n",
      "step n=1820 with reward -0.02\n",
      "step n=1825 with reward -0.02\n",
      "step n=1830 with reward -0.02\n",
      "step n=1835 with reward -0.02\n",
      "step n=1840 with reward -0.02\n",
      "step n=1845 with reward -0.02\n",
      "step n=1850 with reward -0.02\n",
      "step n=1855 with reward 0.0\n",
      "step n=1860 with reward 0.02\n",
      "step n=1865 with reward 0.0\n",
      "step n=1870 with reward 0.0\n",
      "step n=1875 with reward 0.0\n",
      "step n=1880 with reward 0.0\n",
      "step n=1885 with reward -0.02\n",
      "step n=1890 with reward -0.02\n",
      "step n=1895 with reward -0.02\n",
      "step n=1900 with reward -0.02\n",
      "step n=1905 with reward -0.02\n",
      "step n=1910 with reward -0.02\n",
      "step n=1915 with reward -0.02\n",
      "step n=1920 with reward -0.02\n",
      "step n=1925 with reward -0.02\n",
      "step n=1930 with reward -0.02\n",
      "step n=1935 with reward -0.02\n",
      "step n=1940 with reward -0.02\n",
      "step n=1945 with reward -0.02\n",
      "step n=1950 with reward -0.02\n",
      "step n=1955 with reward -0.02\n",
      "step n=1960 with reward -0.02\n",
      "step n=1965 with reward -0.02\n",
      "step n=1970 with reward -0.02\n",
      "step n=1975 with reward -0.02\n",
      "step n=1980 with reward -0.02\n",
      "step n=1985 with reward -0.02\n",
      "step n=1990 with reward -0.02\n",
      "step n=1995 with reward -0.02\n",
      "step n=2000 with reward -0.02\n",
      "step n=2005 with reward -0.02\n",
      "step n=2010 with reward -0.02\n",
      "step n=2015 with reward -0.02\n",
      "step n=2020 with reward -0.02\n",
      "step n=2025 with reward -0.02\n",
      "step n=2030 with reward -0.02\n",
      "step n=2035 with reward -0.02\n",
      "step n=2040 with reward -0.02\n",
      "step n=2045 with reward -0.02\n",
      "step n=2050 with reward 0.02\n",
      "step n=2055 with reward 0.0\n",
      "step n=2060 with reward 0.0\n",
      "step n=2065 with reward 0.0\n",
      "step n=2070 with reward 0.0\n",
      "step n=2075 with reward -0.02\n",
      "step n=2080 with reward -0.02\n",
      "step n=2085 with reward -0.02\n",
      "step n=2090 with reward -0.02\n",
      "step n=2095 with reward -0.02\n",
      "step n=2100 with reward -0.02\n",
      "step n=2105 with reward -0.02\n",
      "step n=2110 with reward -0.02\n",
      "step n=2115 with reward -0.02\n",
      "step n=2120 with reward -0.02\n",
      "step n=2125 with reward -0.02\n",
      "step n=2130 with reward -0.02\n",
      "step n=2135 with reward -0.02\n",
      "step n=2140 with reward -0.02\n",
      "step n=2145 with reward -0.02\n",
      "step n=2150 with reward -0.02\n",
      "step n=2155 with reward -0.02\n",
      "step n=2160 with reward -0.02\n",
      "step n=2165 with reward 0.0\n",
      "step n=2170 with reward 0.0\n",
      "step n=2175 with reward 0.0\n",
      "step n=2180 with reward 0.0\n",
      "step n=2185 with reward 0.0\n",
      "step n=2190 with reward -0.02\n",
      "step n=2195 with reward -0.02\n",
      "step n=2200 with reward -0.02\n",
      "step n=2205 with reward -0.02\n",
      "step n=2210 with reward -0.02\n",
      "step n=2215 with reward -0.02\n",
      "step n=2220 with reward -0.02\n",
      "step n=2225 with reward -0.02\n",
      "step n=2230 with reward -0.02\n",
      "step n=2235 with reward -0.02\n",
      "step n=2240 with reward -0.02\n",
      "step n=2245 with reward -0.02\n",
      "step n=2250 with reward -0.02\n",
      "step n=2255 with reward -0.02\n",
      "step n=2260 with reward -0.02\n",
      "step n=2265 with reward -0.02\n",
      "step n=2270 with reward -0.02\n",
      "step n=2275 with reward -0.02\n",
      "step n=2280 with reward -0.02\n",
      "step n=2285 with reward -0.02\n",
      "step n=2290 with reward -0.02\n",
      "step n=2295 with reward -0.02\n",
      "step n=2300 with reward -0.02\n",
      "step n=2305 with reward 0.0\n",
      "step n=2310 with reward 0.0\n",
      "step n=2315 with reward -0.02\n",
      "step n=2320 with reward -0.02\n",
      "step n=2325 with reward 0.0\n",
      "step n=2330 with reward 0.0\n",
      "step n=2335 with reward -0.02\n",
      "step n=2340 with reward -0.02\n",
      "step n=2345 with reward -0.02\n",
      "step n=2350 with reward -0.02\n",
      "step n=2355 with reward -0.02\n",
      "step n=2360 with reward -0.02\n",
      "step n=2365 with reward -0.02\n",
      "step n=2370 with reward -0.02\n",
      "step n=2375 with reward -0.02\n",
      "step n=2380 with reward -0.02\n",
      "step n=2385 with reward -0.02\n",
      "step n=2390 with reward -0.02\n",
      "step n=2395 with reward -0.02\n",
      "step n=2400 with reward -0.02\n",
      "step n=2405 with reward -0.02\n",
      "step n=2410 with reward -0.02\n",
      "step n=2415 with reward -0.02\n",
      "step n=2420 with reward -0.02\n",
      "step n=2425 with reward -0.02\n",
      "step n=2430 with reward -0.02\n",
      "step n=2435 with reward -0.02\n",
      "step n=2440 with reward -0.02\n",
      "step n=2445 with reward -0.02\n",
      "step n=2450 with reward -0.02\n",
      "step n=2455 with reward -0.02\n",
      "step n=2460 with reward -0.02\n",
      "step n=2465 with reward -0.02\n",
      "step n=2470 with reward -0.02\n",
      "step n=2475 with reward -0.02\n",
      "step n=2480 with reward -0.02\n",
      "step n=2485 with reward -0.02\n",
      "step n=2490 with reward -0.02\n",
      "step n=2495 with reward -0.02\n",
      "step n=2500 with reward -0.02\n",
      "step n=2505 with reward -0.02\n",
      "step n=2510 with reward -0.02\n",
      "step n=2515 with reward -0.02\n",
      "step n=2520 with reward -0.02\n",
      "step n=2525 with reward -0.02\n",
      "step n=2530 with reward -0.02\n",
      "step n=2535 with reward -0.02\n",
      "step n=2540 with reward -0.02\n",
      "step n=2545 with reward -0.02\n",
      "step n=2550 with reward -0.02\n",
      "step n=2555 with reward -0.02\n",
      "step n=2560 with reward -0.02\n",
      "step n=2565 with reward -0.02\n",
      "step n=2570 with reward 0.0\n",
      "step n=2575 with reward 0.0\n",
      "step n=2580 with reward 0.0\n",
      "step n=2585 with reward -0.02\n",
      "step n=2590 with reward -0.02\n",
      "step n=2595 with reward -0.02\n",
      "step n=2600 with reward -0.02\n",
      "step n=2605 with reward -0.02\n",
      "step n=2610 with reward -0.02\n",
      "step n=2615 with reward -0.02\n",
      "step n=2620 with reward 0.02\n",
      "step n=2625 with reward 0.02\n",
      "step n=2630 with reward 0.0\n",
      "step n=2635 with reward 0.0\n",
      "step n=2640 with reward 0.0\n",
      "step n=2645 with reward 0.0\n",
      "step n=2650 with reward -0.02\n",
      "step n=2655 with reward -0.02\n",
      "step n=2660 with reward -0.02\n",
      "step n=2665 with reward -0.02\n",
      "step n=2670 with reward -0.02\n",
      "step n=2675 with reward -0.02\n",
      "step n=2680 with reward -0.02\n",
      "step n=2685 with reward -0.02\n",
      "step n=2690 with reward -0.02\n",
      "step n=2695 with reward -0.02\n",
      "step n=2700 with reward -0.02\n",
      "step n=2705 with reward -0.02\n",
      "step n=2710 with reward -0.02\n",
      "step n=2715 with reward -0.02\n",
      "step n=2720 with reward -0.02\n",
      "step n=2725 with reward -0.02\n",
      "step n=2730 with reward -0.02\n",
      "step n=2735 with reward -0.02\n",
      "step n=2740 with reward -0.02\n",
      "step n=2745 with reward -0.02\n",
      "step n=2750 with reward -0.02\n",
      "step n=2755 with reward -0.02\n",
      "step n=2760 with reward -0.02\n",
      "step n=2765 with reward -0.02\n",
      "step n=2770 with reward -0.02\n",
      "step n=2775 with reward 0.0\n",
      "step n=2780 with reward 0.0\n",
      "step n=2785 with reward 0.0\n",
      "step n=2790 with reward 0.0\n",
      "step n=2795 with reward 0.0\n",
      "step n=2800 with reward 0.0\n",
      "step n=2805 with reward -0.02\n",
      "step n=2810 with reward -0.02\n",
      "step n=2815 with reward -0.02\n",
      "step n=2820 with reward -0.02\n",
      "step n=2825 with reward -0.02\n",
      "step n=2830 with reward -0.02\n",
      "step n=2835 with reward -0.02\n",
      "step n=2840 with reward -0.02\n",
      "step n=2845 with reward -0.02\n",
      "step n=2850 with reward -0.02\n",
      "step n=2855 with reward -0.02\n",
      "step n=2860 with reward -0.02\n",
      "step n=2865 with reward -0.02\n",
      "step n=2870 with reward -0.02\n",
      "step n=2875 with reward -0.02\n",
      "step n=2880 with reward -0.02\n",
      "step n=2885 with reward -0.02\n",
      "step n=2890 with reward -0.02\n",
      "step n=2895 with reward -0.02\n",
      "step n=2900 with reward -0.02\n",
      "step n=2905 with reward -0.02\n",
      "step n=2910 with reward -0.02\n",
      "step n=2915 with reward -0.02\n",
      "step n=2920 with reward -0.02\n",
      "step n=2925 with reward 0.0\n",
      "step n=2930 with reward 0.0\n",
      "step n=2935 with reward 0.0\n",
      "step n=2940 with reward 0.0\n",
      "step n=2945 with reward 0.0\n",
      "step n=2950 with reward -0.02\n",
      "step n=2955 with reward -0.02\n",
      "step n=2960 with reward -0.02\n",
      "step n=2965 with reward -0.02\n",
      "step n=2970 with reward -0.02\n",
      "step n=2975 with reward -0.02\n",
      "step n=2980 with reward -0.02\n",
      "step n=2985 with reward -0.02\n",
      "step n=2990 with reward -0.02\n",
      "step n=2995 with reward -0.02\n",
      "step n=3000 with reward -0.02\n",
      "step n=3005 with reward -0.02\n",
      "step n=3010 with reward -0.02\n",
      "step n=3015 with reward -0.02\n",
      "step n=3020 with reward -0.02\n",
      "step n=3025 with reward -0.02\n",
      "step n=3030 with reward -0.02\n",
      "step n=3035 with reward -0.02\n",
      "step n=3040 with reward -0.02\n",
      "step n=3045 with reward 0.02\n",
      "step n=3050 with reward 0.02\n",
      "step n=3055 with reward 0.0\n",
      "step n=3060 with reward 0.02\n",
      "step n=3065 with reward 0.02\n",
      "step n=3070 with reward 0.02\n",
      "step n=3075 with reward 0.0\n",
      "step n=3080 with reward -0.02\n",
      "step n=3085 with reward -0.02\n",
      "step n=3090 with reward -0.02\n",
      "step n=3095 with reward -0.02\n",
      "step n=3100 with reward -0.02\n",
      "step n=3105 with reward -0.02\n",
      "step n=3110 with reward -0.02\n",
      "step n=3115 with reward -0.02\n",
      "step n=3120 with reward -0.02\n",
      "step n=3125 with reward -0.02\n",
      "step n=3130 with reward -0.02\n",
      "step n=3135 with reward -0.02\n",
      "step n=3140 with reward -0.02\n",
      "step n=3145 with reward -0.02\n",
      "step n=3150 with reward -0.02\n",
      "step n=3155 with reward -0.02\n",
      "step n=3160 with reward -0.02\n",
      "step n=3165 with reward -0.02\n",
      "step n=3170 with reward -0.02\n",
      "step n=3175 with reward -0.02\n",
      "step n=3180 with reward -0.02\n",
      "step n=3185 with reward -0.02\n",
      "step n=3190 with reward -0.02\n",
      "step n=3195 with reward -0.02\n",
      "step n=3200 with reward -0.02\n",
      "step n=3205 with reward -0.02\n",
      "step n=3210 with reward -0.02\n",
      "step n=3215 with reward -0.02\n",
      "step n=3220 with reward -0.02\n",
      "step n=3225 with reward -0.02\n",
      "step n=3230 with reward -0.02\n",
      "step n=3235 with reward -0.02\n",
      "step n=3240 with reward -0.02\n",
      "step n=3245 with reward -0.02\n",
      "step n=3250 with reward -0.02\n",
      "step n=3255 with reward -0.02\n",
      "step n=3260 with reward -0.02\n",
      "step n=3265 with reward -0.02\n",
      "step n=3270 with reward -0.02\n",
      "step n=3275 with reward -0.02\n",
      "step n=3280 with reward -0.02\n",
      "step n=3285 with reward -0.02\n",
      "step n=3290 with reward -0.02\n",
      "step n=3295 with reward -0.02\n",
      "step n=3300 with reward -0.02\n",
      "step n=3305 with reward -0.02\n",
      "step n=3310 with reward -0.02\n",
      "step n=3315 with reward -0.02\n",
      "step n=3320 with reward -0.02\n",
      "step n=3325 with reward -0.02\n",
      "step n=3330 with reward -0.02\n",
      "step n=3335 with reward -0.02\n",
      "step n=3340 with reward -0.02\n",
      "step n=3345 with reward -0.02\n",
      "step n=3350 with reward -0.02\n",
      "step n=3355 with reward -0.02\n",
      "step n=3360 with reward -0.02\n",
      "step n=3365 with reward -0.02\n",
      "step n=3370 with reward -0.02\n",
      "step n=3375 with reward -0.02\n",
      "step n=3380 with reward -0.02\n",
      "step n=3385 with reward -0.02\n",
      "step n=3390 with reward -0.02\n",
      "step n=3395 with reward -0.02\n",
      "step n=3400 with reward -0.02\n",
      "step n=3405 with reward -0.02\n",
      "step n=3410 with reward -0.02\n",
      "step n=3415 with reward -0.02\n",
      "step n=3420 with reward -0.02\n",
      "step n=3425 with reward -0.02\n",
      "step n=3430 with reward -0.02\n",
      "step n=3435 with reward -0.02\n",
      "step n=3440 with reward -0.02\n",
      "step n=3445 with reward -0.02\n",
      "step n=3450 with reward -0.02\n",
      "step n=3455 with reward -0.02\n",
      "step n=3460 with reward -0.02\n",
      "step n=3465 with reward -0.02\n",
      "step n=3470 with reward -0.02\n",
      "step n=3475 with reward -0.02\n",
      "step n=3480 with reward -0.02\n",
      "step n=3485 with reward -0.02\n",
      "step n=3490 with reward -0.02\n",
      "step n=3495 with reward -0.02\n",
      "step n=3500 with reward -0.02\n",
      "step n=3505 with reward -0.02\n",
      "step n=3510 with reward -0.02\n",
      "step n=3515 with reward -0.02\n",
      "step n=3520 with reward -0.02\n",
      "step n=3525 with reward -0.02\n",
      "step n=3530 with reward -0.02\n",
      "step n=3535 with reward -0.02\n",
      "step n=3540 with reward -0.02\n",
      "step n=3545 with reward -0.02\n",
      "step n=3550 with reward -0.02\n",
      "step n=3555 with reward -0.02\n",
      "step n=3560 with reward -0.02\n",
      "step n=3565 with reward -0.02\n",
      "step n=3570 with reward -0.02\n",
      "step n=3575 with reward -0.02\n",
      "step n=3580 with reward -0.02\n",
      "step n=3585 with reward -0.02\n",
      "step n=3590 with reward -0.02\n",
      "step n=3595 with reward -0.02\n",
      "step n=3600 with reward -0.02\n",
      "step n=3605 with reward -0.02\n",
      "step n=3610 with reward -0.02\n",
      "step n=3615 with reward -0.02\n",
      "step n=3620 with reward -0.02\n",
      "step n=3625 with reward -0.02\n",
      "step n=3630 with reward -0.02\n",
      "step n=3635 with reward -0.02\n",
      "step n=3640 with reward -0.02\n",
      "step n=3645 with reward -0.02\n",
      "step n=3650 with reward -0.02\n",
      "step n=3655 with reward -0.02\n",
      "step n=3660 with reward -0.02\n",
      "step n=3665 with reward -0.02\n",
      "step n=3670 with reward -0.02\n",
      "step n=3675 with reward -0.02\n",
      "step n=3680 with reward -0.02\n",
      "step n=3685 with reward -0.02\n",
      "step n=3690 with reward -0.02\n",
      "step n=3695 with reward -0.02\n",
      "step n=3700 with reward -0.02\n",
      "step n=3705 with reward -0.02\n",
      "step n=3710 with reward -0.02\n",
      "step n=3715 with reward -0.02\n",
      "step n=3720 with reward -0.02\n",
      "step n=3725 with reward -0.02\n",
      "step n=3730 with reward -0.02\n",
      "step n=3735 with reward -0.02\n",
      "step n=3740 with reward -0.02\n",
      "step n=3745 with reward -0.02\n",
      "step n=3750 with reward -0.02\n",
      "step n=3755 with reward -0.02\n",
      "step n=3760 with reward -0.02\n",
      "step n=3765 with reward -0.02\n",
      "step n=3770 with reward -0.02\n",
      "step n=3775 with reward -0.02\n",
      "step n=3780 with reward -0.02\n",
      "step n=3785 with reward -0.02\n",
      "step n=3790 with reward -0.02\n",
      "step n=3795 with reward -0.02\n",
      "step n=3800 with reward -0.02\n",
      "step n=3805 with reward -0.02\n",
      "step n=3810 with reward -0.02\n",
      "step n=3815 with reward -0.02\n",
      "step n=3820 with reward -0.02\n",
      "step n=3825 with reward -0.02\n",
      "step n=3830 with reward -0.02\n",
      "step n=3835 with reward -0.02\n",
      "step n=3840 with reward -0.02\n",
      "step n=3845 with reward -0.02\n",
      "step n=3850 with reward -0.02\n",
      "step n=3855 with reward -0.02\n",
      "step n=3860 with reward -0.02\n",
      "step n=3865 with reward -0.02\n",
      "step n=3870 with reward -0.02\n",
      "step n=3875 with reward -0.02\n",
      "step n=3880 with reward -0.02\n",
      "step n=3885 with reward -0.02\n",
      "step n=3890 with reward -0.02\n",
      "step n=3895 with reward -0.02\n",
      "step n=3900 with reward -0.02\n",
      "step n=3905 with reward -0.02\n",
      "step n=3910 with reward -0.02\n",
      "step n=3915 with reward -0.02\n",
      "step n=3920 with reward -0.02\n",
      "step n=3925 with reward -0.02\n",
      "step n=3930 with reward -0.02\n",
      "step n=3935 with reward -0.02\n",
      "step n=3940 with reward -0.02\n",
      "step n=3945 with reward -0.02\n",
      "step n=3950 with reward -0.02\n",
      "step n=3955 with reward -0.02\n",
      "step n=3960 with reward -0.02\n",
      "step n=3965 with reward -0.02\n",
      "step n=3970 with reward -0.02\n",
      "step n=3975 with reward -0.02\n",
      "step n=3980 with reward -0.02\n",
      "step n=3985 with reward -0.02\n",
      "step n=3990 with reward -0.02\n",
      "step n=3995 with reward -0.02\n",
      "step n=4000 with reward -0.02\n",
      "step n=4005 with reward -0.02\n",
      "step n=4010 with reward -0.02\n",
      "step n=4015 with reward -0.02\n",
      "step n=4020 with reward -0.02\n",
      "step n=4025 with reward -0.02\n",
      "step n=4030 with reward -0.02\n",
      "step n=4035 with reward -0.02\n",
      "step n=4040 with reward -0.02\n",
      "step n=4045 with reward -0.02\n",
      "step n=4050 with reward -0.02\n",
      "step n=4055 with reward -0.02\n",
      "step n=4060 with reward -0.02\n",
      "step n=4065 with reward -0.02\n",
      "step n=4070 with reward -0.02\n",
      "step n=4075 with reward -0.02\n",
      "step n=4080 with reward -0.02\n",
      "step n=4085 with reward -0.02\n",
      "step n=4090 with reward -0.02\n",
      "step n=4095 with reward -0.02\n",
      "step n=4100 with reward -0.02\n",
      "step n=4105 with reward -0.02\n",
      "step n=4110 with reward -0.02\n",
      "step n=4115 with reward -0.02\n",
      "step n=4120 with reward -0.02\n",
      "step n=4125 with reward -0.02\n",
      "step n=4130 with reward -0.02\n",
      "step n=4135 with reward -0.02\n",
      "step n=4140 with reward -0.02\n",
      "step n=4145 with reward -0.02\n",
      "step n=4150 with reward -0.02\n",
      "step n=4155 with reward -0.02\n",
      "step n=4160 with reward -0.02\n",
      "step n=4165 with reward -0.02\n",
      "step n=4170 with reward -0.02\n",
      "step n=4175 with reward -0.02\n",
      "step n=4180 with reward -0.02\n",
      "step n=4185 with reward -0.02\n",
      "step n=4190 with reward -0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m state \u001b[38;5;241m=\u001b[39m observation\n\u001b[1;32m     38\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m---> 39\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     41\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with reward \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 68\u001b[0m, in \u001b[0;36mConvDQNAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     67\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(next_state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m# Move to the correct device\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     69\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     70\u001b[0m current_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state)\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m, in \u001b[0;36mConvDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     24\u001b[0m     conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers(x)\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_out\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = retro.make(game=\"SonicTheHedgehog-Genesis\", render_mode='human')\n",
    "env = ButtonActionWrapper(env, buttons=['LEFT', 'RIGHT', 'A'])\n",
    "env = CustomRewardWrapper(env)\n",
    "env = MaxAndSkipObservation(env, skip=num_frame_skip)\n",
    "if RESIZE_ENV:\n",
    "  # input_shape = (num_stacked_frames, *new_size)\n",
    "  input_shape = (num_stacked_frames*3, *new_size)\n",
    "  env = ResizeObservation(env, new_size)\n",
    "else:\n",
    "  input_shape = (num_stacked_frames*3, 224, 320)\n",
    "env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "# env = RecordVideo(\n",
    "#     env,\n",
    "#     video_folder='/content/drive/MyDrive/Video_IA',    # Folder to save videos\n",
    "#     name_prefix=\"eval\",               # Prefix for video filenames\n",
    "#     episode_trigger=lambda x: True    # Record every episode\n",
    "# )\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "agent = ConvDQNAgent(input_shape, action_dim, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.99, buffer_size=10000)\n",
    "if LOAD_MODEL:\n",
    "  agent.model.state_dict(torch.load('../Saved_Models/DQN/'+prev_model, map_location=agent.device))\n",
    "  #agent.model = torch.jit.load('../Saved_Models/DQN/'+prev_model)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "  state, info = env.reset()\n",
    "  total_reward = 0\n",
    "  done = False\n",
    "  frame_count = 0\n",
    "  while not done:\n",
    "    frame_count += 1\n",
    "    action = agent.act(state = state)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    agent.remember(state, action, reward, observation, done)\n",
    "    state = observation\n",
    "    total_reward += reward\n",
    "    agent.replay_vectorized(batch_size)\n",
    "    if frame_count % 5 == 0:\n",
    "      print(f'step n={frame_count} with reward {reward}')\n",
    "    env.render()\n",
    "  print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "env.close()\n",
    "print(f\"Episode finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exitosamente guardado en ../Saved_Models/DQN/DQN-Sonic-V2-E41-S5400.pth\n"
     ]
    }
   ],
   "source": [
    "#Guardar Modelo\n",
    "episode = 41\n",
    "model_save_path = f'../Saved_Models/DQN/DQN-Sonic-V{version}-E{episode}-S{max_episode_steps}.pth' #ppt para jit, pth para statedict\n",
    "try:\n",
    " torch.save(agent.model.state_dict(), model_save_path)\n",
    " #torch.save(agent.model, model_save_path)\n",
    " print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "except Exception as e:\n",
    " print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/seba/Documentos/AI/RL/Video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/tmp/ipykernel_4048/1337217482.py:21: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  agent.model.state_dict(torch.load(f'../Saved_Models/DQN/DQN-Sonic-V{version}-E{episode}-S{max_episode_steps}.pth', map_location=agent.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv_layers.0.weight',\n",
       "              tensor([[[[ 1.7556e-02,  1.1842e-02, -2.5944e-02,  ...,  2.9013e-02,\n",
       "                         -3.2989e-02,  1.2482e-02],\n",
       "                        [-2.8496e-02,  3.5148e-02, -1.5386e-02,  ...,  6.3555e-03,\n",
       "                         -1.2710e-02,  1.7031e-03],\n",
       "                        [-3.5761e-02,  3.3965e-02, -3.2246e-02,  ..., -1.4445e-02,\n",
       "                          1.5948e-02, -2.9359e-02],\n",
       "                        ...,\n",
       "                        [ 1.2383e-02,  7.8581e-03,  2.9742e-02,  ..., -1.8044e-02,\n",
       "                          1.7866e-02,  1.8497e-02],\n",
       "                        [ 2.1378e-03, -4.3376e-03, -9.3637e-03,  ..., -1.2000e-02,\n",
       "                          1.0619e-02,  2.7562e-02],\n",
       "                        [-2.9843e-03, -2.2484e-02, -1.6880e-02,  ..., -4.0025e-03,\n",
       "                         -1.5386e-02,  7.7863e-03]],\n",
       "              \n",
       "                       [[ 6.9236e-03, -1.6538e-04, -3.8604e-03,  ...,  1.5502e-02,\n",
       "                         -2.9586e-02,  1.1670e-02],\n",
       "                        [ 1.9280e-02, -2.4491e-02,  1.1730e-02,  ..., -7.4566e-03,\n",
       "                          1.1497e-02,  1.8566e-02],\n",
       "                        [ 7.5232e-03,  1.7296e-02, -1.9487e-02,  ...,  3.0262e-02,\n",
       "                          2.9527e-03,  3.0584e-03],\n",
       "                        ...,\n",
       "                        [-1.5297e-02, -2.9996e-02, -2.6405e-02,  ..., -6.1937e-03,\n",
       "                          1.2316e-02,  2.6523e-02],\n",
       "                        [ 2.8977e-02,  2.4099e-02, -2.6287e-03,  ..., -1.4474e-03,\n",
       "                          1.2913e-02,  3.0060e-02],\n",
       "                        [ 1.9332e-02,  1.7642e-02,  3.2921e-02,  ..., -1.8005e-02,\n",
       "                          2.6480e-04,  4.5943e-03]],\n",
       "              \n",
       "                       [[-3.4351e-02,  1.6655e-02, -8.9465e-03,  ...,  2.3323e-03,\n",
       "                         -5.2195e-03,  1.2973e-02],\n",
       "                        [ 2.7061e-02,  1.5705e-02,  2.5376e-02,  ...,  3.5873e-03,\n",
       "                          3.1192e-02,  1.8337e-02],\n",
       "                        [ 2.6671e-02, -1.0605e-02, -2.5038e-04,  ...,  5.1513e-03,\n",
       "                         -3.3552e-02,  3.9242e-03],\n",
       "                        ...,\n",
       "                        [-1.4004e-02,  3.0027e-02,  1.5248e-02,  ..., -1.1005e-03,\n",
       "                         -2.4949e-02, -1.7330e-02],\n",
       "                        [-5.2857e-03, -2.0619e-02,  2.6560e-02,  ...,  2.9669e-02,\n",
       "                         -2.4370e-02,  2.8724e-02],\n",
       "                        [-1.8049e-02, -5.0989e-03, -1.4610e-03,  ..., -4.8475e-03,\n",
       "                         -2.9085e-02,  2.0967e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.0662e-02, -2.0563e-03, -2.0927e-02,  ...,  4.8172e-04,\n",
       "                          6.2607e-03,  9.8273e-03],\n",
       "                        [-1.7881e-02,  3.5007e-03, -9.1393e-03,  ...,  1.1434e-03,\n",
       "                          3.3843e-02,  1.1269e-02],\n",
       "                        [-9.0392e-03, -2.1979e-02,  2.9355e-02,  ..., -2.6744e-02,\n",
       "                         -2.6223e-02, -3.5161e-02],\n",
       "                        ...,\n",
       "                        [-1.6131e-02, -3.3269e-02,  2.3122e-02,  ...,  3.1567e-02,\n",
       "                          1.9335e-02, -1.7987e-02],\n",
       "                        [ 2.7197e-02, -2.0283e-02, -1.7888e-02,  ...,  9.7039e-03,\n",
       "                          2.9052e-03,  3.0940e-02],\n",
       "                        [ 5.8541e-03,  4.0435e-03,  3.5293e-02,  ...,  9.0625e-03,\n",
       "                          9.7355e-03,  1.5259e-02]],\n",
       "              \n",
       "                       [[-2.6733e-02, -9.2649e-03, -1.3947e-03,  ...,  3.4690e-02,\n",
       "                         -1.3159e-02, -7.3561e-03],\n",
       "                        [-2.7308e-02,  1.7885e-02,  5.7855e-03,  ..., -1.2621e-02,\n",
       "                          2.1516e-03, -2.9353e-02],\n",
       "                        [-6.9641e-03,  5.0439e-03,  3.1302e-02,  ...,  1.1446e-02,\n",
       "                          1.3784e-02,  7.2019e-03],\n",
       "                        ...,\n",
       "                        [ 7.6512e-03,  1.7073e-02, -1.9411e-02,  ..., -3.4110e-02,\n",
       "                         -5.8702e-03,  6.6995e-03],\n",
       "                        [ 1.0382e-02, -2.3415e-02, -3.8806e-03,  ...,  2.4589e-02,\n",
       "                         -2.8400e-02,  2.3065e-02],\n",
       "                        [ 3.3564e-02,  4.9895e-03, -2.8837e-02,  ...,  3.0945e-02,\n",
       "                          8.8739e-03,  2.4031e-02]],\n",
       "              \n",
       "                       [[ 4.5384e-03,  1.3300e-02, -2.1567e-02,  ...,  7.8095e-03,\n",
       "                         -1.3528e-02,  1.2168e-02],\n",
       "                        [-3.1213e-02, -3.0649e-02,  2.1427e-02,  ...,  3.1861e-02,\n",
       "                          2.0960e-02,  2.2979e-02],\n",
       "                        [ 3.2419e-02, -1.0449e-02, -2.4792e-02,  ...,  2.3682e-02,\n",
       "                          2.7273e-02, -4.4814e-03],\n",
       "                        ...,\n",
       "                        [ 2.0427e-02,  3.3955e-02, -2.1622e-02,  ..., -2.3725e-02,\n",
       "                         -1.5468e-02, -3.1844e-02],\n",
       "                        [ 2.1250e-02,  2.4845e-02, -8.2118e-03,  ..., -1.9198e-02,\n",
       "                          4.8083e-03,  3.5173e-02],\n",
       "                        [ 3.2236e-02,  4.0284e-03, -1.5574e-02,  ..., -2.1762e-02,\n",
       "                          3.0032e-02, -2.2739e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6681e-02, -2.3623e-02, -2.2391e-02,  ..., -1.1423e-02,\n",
       "                         -6.7415e-03, -3.5291e-02],\n",
       "                        [-7.1567e-03,  1.4973e-02, -2.1335e-02,  ..., -3.2102e-02,\n",
       "                         -1.9210e-02, -2.3494e-03],\n",
       "                        [ 2.0670e-02,  2.0379e-02,  1.4024e-02,  ..., -1.5067e-02,\n",
       "                          3.3678e-02,  1.7290e-02],\n",
       "                        ...,\n",
       "                        [ 2.8478e-02,  1.8467e-02, -2.9083e-02,  ..., -4.5841e-03,\n",
       "                         -1.5977e-02,  1.7619e-03],\n",
       "                        [ 1.3165e-03,  1.1174e-03,  2.3490e-03,  ..., -2.1368e-02,\n",
       "                         -3.8954e-03, -1.1452e-02],\n",
       "                        [ 2.8246e-02, -3.2446e-02, -9.4458e-03,  ..., -2.3632e-02,\n",
       "                         -1.1188e-02, -2.5863e-03]],\n",
       "              \n",
       "                       [[-2.6985e-02,  1.8073e-04,  2.8456e-02,  ...,  2.2607e-02,\n",
       "                          2.7979e-02, -2.7223e-03],\n",
       "                        [-3.1880e-04,  1.6705e-02, -2.6324e-02,  ..., -1.1687e-02,\n",
       "                          1.9032e-02, -4.9874e-03],\n",
       "                        [ 6.6459e-03, -1.5483e-03,  2.7227e-02,  ..., -2.3251e-02,\n",
       "                         -2.1607e-02,  5.6897e-03],\n",
       "                        ...,\n",
       "                        [ 4.3733e-03,  3.1931e-02,  8.2566e-03,  ...,  1.8702e-02,\n",
       "                          1.3744e-02, -2.1332e-02],\n",
       "                        [-1.2448e-03,  7.5299e-04, -2.2198e-02,  ..., -9.6830e-03,\n",
       "                         -2.9162e-02,  1.0483e-02],\n",
       "                        [-1.1493e-02,  2.6762e-02, -2.9171e-02,  ..., -3.4315e-02,\n",
       "                         -1.8476e-03,  1.4266e-02]],\n",
       "              \n",
       "                       [[-1.8984e-02,  3.3018e-02, -1.2642e-02,  ..., -1.3179e-02,\n",
       "                         -1.8786e-02, -3.5038e-03],\n",
       "                        [ 1.3277e-02,  1.7796e-02, -1.3058e-02,  ..., -1.1824e-02,\n",
       "                          1.4816e-02,  3.4535e-02],\n",
       "                        [ 2.2379e-02,  3.4166e-02,  2.4215e-02,  ...,  1.9341e-02,\n",
       "                          2.3301e-03, -9.9760e-03],\n",
       "                        ...,\n",
       "                        [ 3.2805e-02,  3.0256e-02,  9.4760e-03,  ..., -3.2432e-02,\n",
       "                          1.2902e-02, -3.5619e-03],\n",
       "                        [-2.4080e-02,  2.8077e-02, -1.1802e-02,  ...,  2.3530e-02,\n",
       "                          2.2539e-02,  1.4905e-02],\n",
       "                        [-2.0154e-02,  1.1220e-02,  1.7410e-02,  ..., -1.7869e-02,\n",
       "                          2.7460e-02, -3.3147e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.6022e-03,  3.5430e-02,  9.7878e-03,  ...,  1.1049e-02,\n",
       "                          2.2340e-02, -1.0335e-02],\n",
       "                        [ 2.9693e-02, -3.3225e-02, -1.3777e-02,  ..., -3.2215e-02,\n",
       "                          1.1076e-02, -3.4502e-02],\n",
       "                        [-2.3655e-02,  2.2303e-02,  1.9730e-04,  ...,  2.7498e-03,\n",
       "                         -2.0200e-02,  1.3452e-02],\n",
       "                        ...,\n",
       "                        [-1.3268e-02, -2.6844e-02, -2.3883e-02,  ..., -1.5552e-03,\n",
       "                          3.4215e-02,  3.1462e-02],\n",
       "                        [ 2.5472e-02,  2.0155e-02,  2.4846e-02,  ...,  2.4671e-02,\n",
       "                          5.7449e-03, -6.7160e-03],\n",
       "                        [-1.3620e-03, -2.5168e-03,  3.5879e-02,  ..., -2.7058e-02,\n",
       "                         -2.5811e-02,  2.8350e-02]],\n",
       "              \n",
       "                       [[ 9.3470e-03, -4.9274e-03,  1.3690e-02,  ...,  2.3904e-02,\n",
       "                          1.8440e-02,  2.7481e-02],\n",
       "                        [-2.9106e-02,  3.1397e-02,  2.6142e-02,  ...,  1.7762e-02,\n",
       "                          1.7484e-02,  2.8833e-02],\n",
       "                        [ 2.6172e-02, -1.9532e-02, -1.3142e-02,  ...,  2.0983e-02,\n",
       "                         -8.4351e-03,  9.5332e-05],\n",
       "                        ...,\n",
       "                        [-1.5457e-02,  5.9737e-03,  1.8517e-02,  ...,  5.9192e-03,\n",
       "                          1.1457e-02, -2.7249e-02],\n",
       "                        [ 5.7243e-03,  2.9724e-02,  2.3255e-03,  ...,  7.4474e-04,\n",
       "                         -2.5958e-02, -2.6955e-02],\n",
       "                        [ 1.4928e-02, -2.8448e-02, -2.0723e-02,  ..., -1.8591e-02,\n",
       "                          5.9494e-03, -2.9580e-02]],\n",
       "              \n",
       "                       [[ 1.2370e-02, -2.5533e-04, -2.3758e-02,  ..., -3.4688e-02,\n",
       "                         -3.8372e-03,  1.7576e-03],\n",
       "                        [ 1.4833e-02, -3.5478e-02, -3.4301e-02,  ..., -9.3198e-03,\n",
       "                          1.6861e-02, -2.7483e-05],\n",
       "                        [ 2.5198e-02, -9.1558e-03, -3.4285e-02,  ..., -3.6073e-02,\n",
       "                          2.8537e-02, -2.9725e-02],\n",
       "                        ...,\n",
       "                        [ 2.7327e-02, -3.1705e-02,  1.7919e-02,  ...,  2.7153e-03,\n",
       "                          2.3840e-02,  3.6339e-03],\n",
       "                        [ 1.0560e-02,  2.5410e-02, -3.4240e-02,  ..., -3.0482e-02,\n",
       "                         -3.4200e-03,  6.8131e-03],\n",
       "                        [ 2.9002e-02, -6.3459e-03,  1.8225e-02,  ...,  1.5172e-02,\n",
       "                         -1.9203e-02, -2.7574e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2688e-02, -3.1456e-02,  2.1560e-02,  ...,  1.1738e-02,\n",
       "                          9.0689e-03, -1.8899e-02],\n",
       "                        [-3.0448e-02, -2.5799e-02, -1.1636e-02,  ...,  1.6209e-02,\n",
       "                         -2.9901e-03,  1.5474e-02],\n",
       "                        [ 8.5394e-04, -2.6129e-03, -1.2861e-02,  ...,  3.3126e-02,\n",
       "                          2.6392e-02, -2.2253e-03],\n",
       "                        ...,\n",
       "                        [-1.9272e-02, -1.2028e-02, -1.6742e-02,  ...,  1.5004e-02,\n",
       "                         -7.9692e-03,  6.9569e-03],\n",
       "                        [ 5.5402e-04,  2.8025e-02, -1.9834e-02,  ..., -3.4989e-02,\n",
       "                          2.9920e-02,  1.5983e-02],\n",
       "                        [-1.2830e-02,  2.8994e-03,  1.4948e-02,  ...,  9.5069e-03,\n",
       "                         -6.7356e-03,  1.8633e-02]],\n",
       "              \n",
       "                       [[ 1.5716e-02,  4.4272e-03, -1.0497e-02,  ..., -4.5347e-04,\n",
       "                          1.0680e-03, -2.9162e-02],\n",
       "                        [ 1.6544e-04, -2.8350e-02, -4.0958e-03,  ..., -5.8683e-03,\n",
       "                         -1.6938e-02,  3.1923e-02],\n",
       "                        [ 2.0422e-02,  1.3530e-02,  1.7126e-02,  ..., -2.1271e-02,\n",
       "                         -3.2152e-02, -7.9875e-03],\n",
       "                        ...,\n",
       "                        [-2.0031e-02, -3.2783e-02, -7.2098e-03,  ..., -1.1900e-02,\n",
       "                         -2.4284e-02,  8.9299e-03],\n",
       "                        [ 1.6435e-02, -2.6389e-02, -3.0393e-02,  ...,  2.3841e-02,\n",
       "                          3.4167e-02, -2.3212e-02],\n",
       "                        [ 3.2737e-02,  2.9785e-03,  3.4799e-02,  ..., -7.8213e-03,\n",
       "                         -1.9126e-02,  2.1081e-02]],\n",
       "              \n",
       "                       [[-2.0426e-03,  2.8535e-02,  2.7071e-02,  ...,  9.8296e-03,\n",
       "                         -2.7548e-03, -1.4460e-02],\n",
       "                        [-2.6673e-02, -2.3125e-02, -7.5734e-03,  ...,  1.5025e-02,\n",
       "                          1.6390e-02,  1.8987e-04],\n",
       "                        [ 2.0771e-02, -1.5433e-02,  2.2109e-02,  ..., -2.8818e-02,\n",
       "                          1.0047e-03, -3.5866e-02],\n",
       "                        ...,\n",
       "                        [-1.8034e-02, -1.2282e-03, -9.3478e-03,  ...,  1.0579e-03,\n",
       "                         -3.6077e-02,  3.4096e-02],\n",
       "                        [ 2.1945e-02, -1.7109e-03, -5.0810e-03,  ...,  1.4624e-02,\n",
       "                         -1.5363e-02,  1.4413e-02],\n",
       "                        [ 8.0091e-04, -6.6558e-03,  2.4363e-02,  ..., -2.6891e-02,\n",
       "                         -1.0724e-02, -3.3625e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.7320e-03,  5.3377e-04, -1.4582e-02,  ..., -3.2823e-03,\n",
       "                         -1.2697e-02, -2.8301e-02],\n",
       "                        [-1.9245e-03,  1.2881e-02, -1.3815e-02,  ..., -3.5888e-02,\n",
       "                         -1.8995e-02,  1.3373e-02],\n",
       "                        [-2.9410e-03,  2.9000e-02,  1.3472e-02,  ..., -3.0181e-02,\n",
       "                          3.0024e-02, -2.8732e-02],\n",
       "                        ...,\n",
       "                        [-9.2013e-03,  2.0353e-02, -8.2698e-04,  ..., -1.2536e-02,\n",
       "                          2.0583e-03,  1.2092e-02],\n",
       "                        [-1.0942e-02, -3.4141e-02, -1.3192e-02,  ..., -3.5933e-04,\n",
       "                         -3.3600e-02, -3.4578e-02],\n",
       "                        [-2.9933e-03, -4.4572e-03,  2.8553e-03,  ..., -2.2107e-02,\n",
       "                         -1.2765e-02,  3.1051e-02]],\n",
       "              \n",
       "                       [[ 2.9519e-02, -3.1960e-02, -1.2068e-02,  ..., -9.7350e-03,\n",
       "                         -3.1108e-02,  1.3638e-02],\n",
       "                        [-2.2150e-02, -1.2732e-02,  2.2343e-03,  ...,  1.2784e-02,\n",
       "                          1.6696e-02, -1.4814e-03],\n",
       "                        [ 3.5486e-02, -1.6061e-02,  3.1651e-02,  ...,  2.9476e-02,\n",
       "                          2.0429e-03,  2.1114e-02],\n",
       "                        ...,\n",
       "                        [-6.5287e-03, -3.1625e-02,  1.8886e-02,  ...,  2.6175e-02,\n",
       "                         -3.2198e-02, -3.4800e-02],\n",
       "                        [-2.4914e-02,  1.2550e-02, -3.2370e-02,  ...,  5.9809e-03,\n",
       "                          2.8373e-03,  1.6498e-02],\n",
       "                        [ 5.2084e-03,  3.2378e-02, -2.8873e-02,  ..., -2.2836e-02,\n",
       "                          4.0101e-03, -1.8600e-02]],\n",
       "              \n",
       "                       [[ 1.4911e-02, -2.3748e-02,  2.7193e-02,  ...,  3.2050e-02,\n",
       "                         -3.2722e-02,  2.3959e-02],\n",
       "                        [-2.1849e-02,  2.6789e-04,  1.1542e-02,  ...,  1.3006e-02,\n",
       "                          2.1140e-02, -2.8266e-02],\n",
       "                        [ 1.1607e-02,  1.1066e-02,  3.4756e-02,  ...,  2.7280e-02,\n",
       "                         -2.5668e-02,  1.7236e-02],\n",
       "                        ...,\n",
       "                        [-6.7751e-03, -2.0872e-02,  3.4176e-02,  ...,  2.3871e-02,\n",
       "                         -1.8933e-02, -9.9402e-03],\n",
       "                        [ 2.1626e-02,  2.7507e-02, -2.0859e-02,  ..., -1.8185e-02,\n",
       "                          1.8241e-02, -7.7013e-03],\n",
       "                        [ 1.6237e-03, -1.9443e-02,  1.0597e-02,  ..., -2.0368e-02,\n",
       "                         -1.6375e-02, -4.7015e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.0249e-03,  1.8741e-02, -2.7270e-02,  ..., -1.2488e-02,\n",
       "                         -3.6149e-03, -7.4968e-03],\n",
       "                        [ 2.6929e-02,  2.4126e-02,  1.4357e-02,  ...,  1.8760e-02,\n",
       "                          8.3231e-03, -1.5064e-02],\n",
       "                        [-2.1668e-02,  1.1168e-02,  5.4451e-03,  ..., -7.1445e-03,\n",
       "                         -2.0034e-02, -2.6281e-02],\n",
       "                        ...,\n",
       "                        [ 1.0798e-02, -1.5284e-02,  5.1371e-03,  ...,  2.3058e-02,\n",
       "                          7.4243e-04,  8.3193e-03],\n",
       "                        [ 1.2037e-02, -3.1913e-02, -7.8932e-04,  ..., -4.0017e-03,\n",
       "                          1.0048e-02,  1.3518e-02],\n",
       "                        [-3.0110e-02,  3.0343e-02,  1.0564e-02,  ..., -2.5135e-02,\n",
       "                         -1.5815e-02,  2.1443e-02]],\n",
       "              \n",
       "                       [[ 1.6794e-02,  5.5168e-03, -1.8256e-02,  ..., -2.0186e-02,\n",
       "                         -2.2562e-02,  3.3753e-02],\n",
       "                        [ 8.9391e-03, -4.6027e-03,  1.8498e-02,  ..., -1.6891e-02,\n",
       "                         -2.5025e-02, -2.0719e-02],\n",
       "                        [-8.8415e-03, -3.9453e-03,  7.6169e-03,  ...,  1.9763e-02,\n",
       "                          3.1881e-02, -2.7966e-02],\n",
       "                        ...,\n",
       "                        [-1.9927e-02, -3.0818e-02,  2.6280e-02,  ..., -1.0172e-02,\n",
       "                          3.2270e-02, -2.4637e-02],\n",
       "                        [-8.2100e-03, -6.9449e-04, -2.5195e-02,  ...,  2.6301e-02,\n",
       "                          2.0649e-02, -2.0780e-02],\n",
       "                        [ 6.1596e-03,  2.3966e-02, -3.5066e-02,  ...,  9.6395e-03,\n",
       "                          3.3980e-02, -2.5752e-02]],\n",
       "              \n",
       "                       [[ 3.1726e-02, -3.3246e-02, -6.8437e-03,  ..., -1.0940e-02,\n",
       "                          2.0796e-02,  3.4711e-02],\n",
       "                        [ 8.0828e-03,  9.2397e-04,  2.9320e-03,  ...,  2.8007e-02,\n",
       "                         -1.2043e-02,  1.1253e-02],\n",
       "                        [-2.3176e-02, -5.1693e-03, -3.2024e-02,  ..., -4.8713e-03,\n",
       "                         -2.9841e-02,  2.7036e-02],\n",
       "                        ...,\n",
       "                        [ 9.3352e-03,  1.3284e-02,  8.8698e-03,  ...,  7.3306e-03,\n",
       "                         -1.0075e-02,  2.0492e-02],\n",
       "                        [-6.2940e-03,  2.0508e-03, -2.3768e-02,  ..., -2.9841e-02,\n",
       "                          3.5307e-03, -1.5599e-02],\n",
       "                        [-1.5307e-03,  2.2273e-02,  2.2329e-02,  ...,  6.0540e-03,\n",
       "                          1.9816e-02, -3.4066e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.6945e-02,  3.2070e-02, -2.0761e-02,  ..., -1.9766e-02,\n",
       "                          3.0357e-03,  2.8790e-02],\n",
       "                        [-2.4400e-02,  1.6780e-02, -2.1403e-03,  ...,  3.3486e-02,\n",
       "                         -3.3284e-02,  3.1626e-02],\n",
       "                        [-1.0049e-02, -2.5898e-02,  3.4369e-02,  ..., -1.5232e-02,\n",
       "                          1.4518e-02, -1.4717e-02],\n",
       "                        ...,\n",
       "                        [ 3.0684e-03,  3.2673e-02, -2.2143e-02,  ..., -6.2718e-03,\n",
       "                         -3.4812e-02,  3.0407e-02],\n",
       "                        [ 1.7328e-02,  5.1836e-03, -1.7473e-02,  ..., -3.1006e-03,\n",
       "                          1.6282e-02,  1.8350e-02],\n",
       "                        [ 2.3080e-02, -2.5434e-02, -3.4216e-02,  ...,  1.6475e-02,\n",
       "                          2.6330e-02,  3.2315e-02]],\n",
       "              \n",
       "                       [[-1.0768e-02,  1.1855e-02,  2.9059e-02,  ...,  6.6612e-03,\n",
       "                          2.6284e-02,  2.1711e-02],\n",
       "                        [ 2.2839e-02, -1.5400e-02,  1.8295e-02,  ..., -1.1130e-02,\n",
       "                          1.2577e-02, -8.6245e-03],\n",
       "                        [ 1.9109e-02, -1.4986e-02,  3.1976e-02,  ..., -3.4500e-02,\n",
       "                          3.0936e-02, -2.1713e-02],\n",
       "                        ...,\n",
       "                        [-8.8617e-05, -8.5512e-03, -6.8861e-03,  ..., -3.0718e-02,\n",
       "                         -2.5944e-02,  3.3447e-02],\n",
       "                        [-3.4893e-02, -1.9182e-02,  1.3241e-02,  ..., -1.1008e-02,\n",
       "                          3.3099e-04,  2.3886e-02],\n",
       "                        [-1.0106e-02, -2.6017e-02, -1.5521e-02,  ..., -2.1752e-02,\n",
       "                         -3.5956e-02, -2.8632e-02]],\n",
       "              \n",
       "                       [[ 2.4794e-02, -1.1413e-02,  2.3872e-02,  ...,  4.0671e-03,\n",
       "                          1.9589e-02,  6.2975e-03],\n",
       "                        [-9.4082e-03,  2.7313e-02,  1.8112e-02,  ...,  3.2586e-02,\n",
       "                          1.4009e-03,  6.6624e-03],\n",
       "                        [ 2.9744e-02, -6.1694e-03, -2.3195e-02,  ..., -3.1849e-02,\n",
       "                         -2.8939e-02, -3.5959e-02],\n",
       "                        ...,\n",
       "                        [ 3.1812e-02, -3.1713e-02,  1.1641e-02,  ..., -6.9213e-03,\n",
       "                         -4.0122e-03, -1.7792e-02],\n",
       "                        [-3.9603e-04, -3.5953e-03,  2.4607e-03,  ...,  2.9121e-02,\n",
       "                          1.5282e-02, -1.5159e-02],\n",
       "                        [-1.5928e-02,  1.3740e-02,  1.2257e-02,  ..., -3.3901e-02,\n",
       "                         -4.7593e-03, -3.2618e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0583e-03,  2.4406e-03, -2.4060e-02,  ...,  3.5533e-02,\n",
       "                          3.2597e-02,  3.0532e-02],\n",
       "                        [-2.6549e-02, -3.6958e-03,  5.6415e-03,  ...,  2.9371e-02,\n",
       "                          1.1326e-03,  3.3404e-02],\n",
       "                        [ 1.2219e-02, -2.3117e-03, -9.2405e-03,  ..., -1.3596e-03,\n",
       "                         -2.0159e-02, -2.7411e-02],\n",
       "                        ...,\n",
       "                        [-1.0467e-02, -1.7488e-02, -1.2519e-02,  ..., -9.5880e-03,\n",
       "                          1.0143e-02,  1.1061e-02],\n",
       "                        [-2.4064e-02,  3.0980e-02,  6.4380e-03,  ..., -1.7213e-02,\n",
       "                         -1.5833e-03, -2.0252e-02],\n",
       "                        [ 1.9609e-02,  1.7538e-02, -2.3435e-02,  ..., -2.1826e-02,\n",
       "                          7.0377e-03, -1.2073e-02]],\n",
       "              \n",
       "                       [[ 6.7164e-03,  7.0416e-03,  1.5725e-03,  ..., -2.7739e-03,\n",
       "                         -2.8417e-03,  8.5050e-03],\n",
       "                        [ 2.3922e-02, -1.2585e-02, -1.1503e-02,  ...,  2.5342e-02,\n",
       "                         -3.3737e-02, -2.5168e-02],\n",
       "                        [ 3.3271e-02,  1.6113e-02,  2.2485e-02,  ..., -1.4251e-02,\n",
       "                         -2.6019e-02, -5.5243e-03],\n",
       "                        ...,\n",
       "                        [ 1.5477e-02,  2.5367e-02, -2.4758e-02,  ...,  2.5529e-02,\n",
       "                         -1.6534e-02,  8.1627e-03],\n",
       "                        [-2.4002e-02,  2.2248e-02, -4.8484e-03,  ...,  3.3951e-02,\n",
       "                          3.3318e-02,  9.9846e-04],\n",
       "                        [-3.2327e-02,  6.3109e-03,  1.3386e-02,  ...,  2.4163e-02,\n",
       "                         -2.4225e-02,  2.0542e-02]],\n",
       "              \n",
       "                       [[-3.3310e-03,  2.4934e-02, -6.7331e-03,  ..., -1.4634e-02,\n",
       "                          2.1389e-02, -1.6951e-02],\n",
       "                        [ 1.1787e-03,  2.7988e-02, -4.3557e-03,  ..., -1.4297e-02,\n",
       "                          2.5357e-02, -8.3047e-03],\n",
       "                        [-2.1442e-02, -3.2420e-02, -1.4370e-02,  ...,  2.0306e-03,\n",
       "                          3.3013e-02,  3.0396e-03],\n",
       "                        ...,\n",
       "                        [-2.3580e-02, -5.0539e-03, -2.2574e-02,  ..., -1.1769e-02,\n",
       "                         -2.3353e-02, -9.9472e-03],\n",
       "                        [-8.2584e-03, -9.0619e-03, -1.7581e-02,  ..., -2.9979e-02,\n",
       "                         -1.8547e-02, -2.9397e-02],\n",
       "                        [ 1.6833e-02,  1.8451e-02,  8.8453e-03,  ...,  2.0980e-02,\n",
       "                          1.8618e-02, -9.3683e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.5448e-02, -1.7495e-02,  7.2838e-03,  ..., -9.8978e-03,\n",
       "                          9.5416e-03, -3.4585e-02],\n",
       "                        [-3.5008e-02, -1.4323e-02,  8.0839e-03,  ..., -1.5355e-02,\n",
       "                         -2.1104e-02,  1.8465e-02],\n",
       "                        [-2.8604e-02, -2.9806e-02, -1.7638e-02,  ...,  2.5787e-02,\n",
       "                         -1.8469e-02,  2.6367e-03],\n",
       "                        ...,\n",
       "                        [ 3.4717e-02,  7.5657e-03,  9.7145e-03,  ...,  2.0609e-02,\n",
       "                          3.0493e-02, -2.6009e-02],\n",
       "                        [-2.1152e-02,  1.7446e-02,  3.1849e-02,  ...,  1.9690e-02,\n",
       "                         -6.2313e-03,  2.9084e-02],\n",
       "                        [ 2.4450e-02,  2.2035e-02, -1.8838e-02,  ...,  2.8796e-02,\n",
       "                          1.3716e-02,  2.8838e-02]],\n",
       "              \n",
       "                       [[-6.0487e-03,  2.5042e-02,  5.0245e-03,  ...,  7.7142e-03,\n",
       "                         -9.3763e-03, -2.0600e-02],\n",
       "                        [-2.7168e-04, -2.4832e-02, -2.5540e-02,  ...,  1.6199e-02,\n",
       "                          1.0805e-02,  3.2005e-02],\n",
       "                        [-1.6234e-02,  2.9377e-02,  2.4970e-02,  ..., -5.1709e-03,\n",
       "                          3.1163e-03,  1.4438e-02],\n",
       "                        ...,\n",
       "                        [-1.1103e-02, -2.8296e-02,  3.3676e-02,  ..., -8.0073e-03,\n",
       "                          1.0733e-02,  2.4053e-02],\n",
       "                        [ 8.1060e-03,  9.6221e-04,  8.0749e-03,  ...,  2.1232e-02,\n",
       "                         -1.8177e-03, -2.9781e-02],\n",
       "                        [-3.1128e-02, -2.0758e-02,  1.1311e-02,  ..., -2.7494e-03,\n",
       "                          9.6246e-03,  3.6888e-03]],\n",
       "              \n",
       "                       [[ 3.0726e-02, -7.0962e-03,  3.2692e-02,  ...,  1.3547e-02,\n",
       "                          2.5941e-02,  9.2719e-03],\n",
       "                        [-3.0399e-02, -1.1774e-02, -2.9797e-03,  ...,  2.9942e-02,\n",
       "                          1.8840e-02,  3.4243e-02],\n",
       "                        [-1.1964e-03, -2.0622e-02, -1.8911e-02,  ...,  3.4633e-02,\n",
       "                         -1.2124e-02, -3.4681e-02],\n",
       "                        ...,\n",
       "                        [ 2.0019e-02, -1.4091e-02, -3.2539e-02,  ..., -3.1793e-02,\n",
       "                          3.5983e-02, -3.4394e-02],\n",
       "                        [ 7.4613e-03,  3.1745e-02, -1.2266e-02,  ..., -1.8923e-02,\n",
       "                          2.0733e-02, -2.2399e-02],\n",
       "                        [ 1.2786e-02,  2.2192e-03,  2.6532e-02,  ...,  1.2884e-02,\n",
       "                          1.7009e-02,  1.3253e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.8462e-03,  3.0771e-02,  1.0455e-02,  ..., -1.4563e-02,\n",
       "                         -1.8264e-02, -3.2614e-02],\n",
       "                        [-2.7981e-02,  2.4499e-02,  2.8447e-02,  ..., -2.8066e-03,\n",
       "                         -1.1072e-02,  3.3795e-02],\n",
       "                        [-2.9093e-02, -2.2599e-02,  2.1547e-02,  ..., -4.0937e-03,\n",
       "                          1.8566e-03, -2.3104e-02],\n",
       "                        ...,\n",
       "                        [-2.0291e-02, -9.1706e-03, -8.6673e-03,  ..., -2.0651e-02,\n",
       "                         -2.2539e-02,  2.4395e-03],\n",
       "                        [ 1.4813e-02,  1.1302e-02, -2.0154e-03,  ...,  2.5269e-02,\n",
       "                          8.4386e-03,  2.1345e-02],\n",
       "                        [-2.8448e-02, -5.4255e-03, -2.5426e-02,  ..., -2.8985e-02,\n",
       "                         -3.1236e-02, -1.7694e-02]],\n",
       "              \n",
       "                       [[ 2.3301e-04, -2.1303e-02, -2.3552e-03,  ..., -1.7827e-02,\n",
       "                         -7.3375e-04, -4.4784e-04],\n",
       "                        [-2.0604e-02, -2.7259e-02,  4.2760e-03,  ...,  9.1582e-03,\n",
       "                          5.9437e-03,  3.1989e-02],\n",
       "                        [ 1.4312e-02,  5.4771e-03, -3.3944e-02,  ..., -3.1008e-02,\n",
       "                         -3.3976e-02,  1.1090e-02],\n",
       "                        ...,\n",
       "                        [-1.5508e-02, -2.0146e-02, -1.3306e-02,  ...,  1.2928e-02,\n",
       "                         -2.1609e-02, -3.4294e-02],\n",
       "                        [-7.4571e-03,  1.8280e-02,  3.1932e-02,  ...,  1.8184e-02,\n",
       "                          2.2727e-02, -2.4160e-02],\n",
       "                        [ 3.0514e-02,  1.9146e-02, -8.6706e-04,  ...,  5.4323e-03,\n",
       "                          2.5375e-02, -3.4972e-02]],\n",
       "              \n",
       "                       [[-2.1810e-02,  1.5576e-03,  3.3993e-02,  ...,  3.4134e-02,\n",
       "                         -3.3135e-02, -1.7812e-02],\n",
       "                        [ 1.5672e-02,  2.7808e-02, -2.9622e-02,  ...,  2.0263e-02,\n",
       "                         -2.0491e-02,  1.4278e-02],\n",
       "                        [-4.5867e-03,  1.3230e-02,  2.7311e-02,  ...,  1.4960e-02,\n",
       "                          6.8856e-03, -1.4900e-02],\n",
       "                        ...,\n",
       "                        [ 3.2992e-02, -2.0181e-02, -1.6154e-02,  ...,  2.1532e-02,\n",
       "                         -1.6167e-02, -2.6669e-03],\n",
       "                        [-3.2594e-02,  2.6999e-02, -8.5333e-03,  ...,  2.0390e-02,\n",
       "                          2.6771e-02,  2.2521e-02],\n",
       "                        [ 1.0157e-02, -1.1240e-02,  2.1900e-02,  ...,  3.5679e-02,\n",
       "                          1.8662e-02, -1.6943e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.4841e-02, -2.8948e-02,  5.0454e-03,  ..., -1.3509e-02,\n",
       "                         -2.1157e-02,  3.1698e-03],\n",
       "                        [ 3.3863e-02, -2.2955e-03, -9.9886e-03,  ...,  2.0547e-02,\n",
       "                          3.0378e-03,  3.7997e-03],\n",
       "                        [-2.6060e-02, -3.0441e-03, -2.8057e-02,  ...,  1.3149e-02,\n",
       "                         -5.6182e-03, -2.4584e-02],\n",
       "                        ...,\n",
       "                        [-1.2573e-02, -5.9263e-03,  2.6488e-02,  ..., -6.9533e-03,\n",
       "                         -1.2383e-02,  2.6333e-02],\n",
       "                        [ 2.6593e-02,  7.2447e-03, -2.0499e-02,  ...,  9.0235e-03,\n",
       "                         -1.2412e-03, -1.0712e-02],\n",
       "                        [-8.4505e-03,  3.5012e-02,  2.0258e-02,  ..., -1.5170e-02,\n",
       "                          3.1964e-02, -7.6532e-04]],\n",
       "              \n",
       "                       [[ 1.9441e-02, -5.4622e-03, -2.5685e-02,  ...,  9.6369e-03,\n",
       "                         -3.4536e-02,  3.2330e-02],\n",
       "                        [-1.4653e-02, -1.9612e-02, -1.0906e-02,  ...,  9.8224e-03,\n",
       "                          1.7204e-02,  1.0640e-02],\n",
       "                        [ 3.5329e-02, -1.5655e-02, -1.8441e-02,  ...,  2.3449e-02,\n",
       "                          2.4213e-03,  2.3415e-02],\n",
       "                        ...,\n",
       "                        [ 1.9244e-02,  3.2309e-02, -3.2425e-02,  ...,  2.7836e-02,\n",
       "                         -1.8377e-03, -1.4533e-02],\n",
       "                        [ 4.6950e-03,  3.3063e-02, -3.1528e-02,  ...,  2.4485e-02,\n",
       "                          3.2264e-02, -5.3469e-04],\n",
       "                        [ 3.4378e-02,  2.4787e-02,  4.2401e-03,  ..., -3.5929e-02,\n",
       "                          9.0120e-03, -3.7623e-03]],\n",
       "              \n",
       "                       [[ 3.3609e-02,  5.4683e-03, -3.3786e-02,  ..., -2.6867e-02,\n",
       "                         -1.4591e-03,  9.2915e-03],\n",
       "                        [ 1.0210e-02,  1.6460e-02,  2.7312e-02,  ..., -8.1090e-03,\n",
       "                          2.0064e-02,  3.2724e-03],\n",
       "                        [ 1.9530e-03,  2.1910e-02, -3.2306e-02,  ..., -1.5566e-03,\n",
       "                          2.2025e-02, -2.0605e-05],\n",
       "                        ...,\n",
       "                        [-3.5757e-02,  1.3216e-02,  3.0130e-03,  ...,  1.4641e-02,\n",
       "                          9.1471e-03,  1.0046e-02],\n",
       "                        [-2.3841e-02,  6.2599e-03, -1.3513e-02,  ...,  4.2740e-03,\n",
       "                          1.6204e-02, -9.1930e-03],\n",
       "                        [-1.2461e-02,  9.0446e-03,  3.2650e-02,  ..., -1.4680e-02,\n",
       "                          1.0784e-02, -2.6124e-02]]]], device='cuda:0')),\n",
       "             ('conv_layers.0.bias',\n",
       "              tensor([ 0.0317,  0.0285, -0.0085,  0.0044, -0.0099, -0.0324,  0.0070, -0.0149,\n",
       "                      -0.0358,  0.0109, -0.0280,  0.0098,  0.0278, -0.0261,  0.0247,  0.0232],\n",
       "                     device='cuda:0')),\n",
       "             ('conv_layers.2.weight',\n",
       "              tensor([[[[-6.1993e-02,  6.0477e-02, -2.7843e-02, -4.9023e-02],\n",
       "                        [ 2.1555e-02,  4.2436e-02,  3.7445e-02,  6.0984e-02],\n",
       "                        [-8.3527e-04,  1.6251e-02, -2.5065e-02,  2.8549e-02],\n",
       "                        [-5.3834e-02, -3.5545e-02,  3.8136e-02, -3.7979e-02]],\n",
       "              \n",
       "                       [[-1.6003e-02, -1.4327e-02,  3.5600e-02,  1.8790e-02],\n",
       "                        [-3.5879e-02,  2.0293e-02, -2.4315e-02, -3.9816e-02],\n",
       "                        [-5.4807e-02,  5.1666e-02,  8.8543e-03,  1.7703e-02],\n",
       "                        [ 2.3325e-02, -1.8857e-05, -1.4951e-03,  1.9649e-02]],\n",
       "              \n",
       "                       [[-3.9788e-02, -5.1663e-02,  4.6237e-02,  3.8585e-02],\n",
       "                        [-5.0019e-02, -6.1093e-02,  4.5477e-02, -5.7769e-02],\n",
       "                        [-4.3962e-02,  5.4450e-02, -5.2572e-02, -1.1731e-02],\n",
       "                        [-3.6967e-02,  2.4479e-02, -4.3441e-02,  2.0534e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 6.0481e-02, -4.9682e-02, -5.5165e-02,  3.2276e-02],\n",
       "                        [-5.2275e-02,  4.4818e-02, -5.8613e-02, -1.9503e-02],\n",
       "                        [ 3.3050e-02,  9.8281e-03, -3.5302e-02,  3.3725e-02],\n",
       "                        [-5.8089e-02,  4.8164e-02,  3.1128e-02, -2.9903e-02]],\n",
       "              \n",
       "                       [[-1.9722e-02, -5.3047e-02,  1.6153e-02,  4.6045e-02],\n",
       "                        [ 3.6720e-02,  5.5979e-02,  3.5550e-02,  4.2583e-03],\n",
       "                        [-9.0917e-03, -5.2438e-03,  4.8669e-02,  1.8541e-04],\n",
       "                        [ 5.3299e-02,  5.6433e-02,  2.6600e-02,  4.0043e-02]],\n",
       "              \n",
       "                       [[ 3.0617e-02,  5.1738e-02, -4.3062e-03, -3.7527e-02],\n",
       "                        [ 4.5822e-02,  4.3937e-03,  5.6377e-03,  3.5207e-02],\n",
       "                        [ 4.2239e-02, -2.6227e-02,  3.3687e-02,  5.9571e-02],\n",
       "                        [ 5.5483e-02, -3.2034e-02, -5.8513e-02, -6.7957e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8582e-02,  3.1256e-02,  4.8103e-02, -1.7508e-02],\n",
       "                        [-2.9739e-02, -6.0225e-02,  1.0464e-02,  2.9957e-02],\n",
       "                        [ 3.7279e-02,  1.1433e-02, -2.5153e-02,  6.0222e-03],\n",
       "                        [-2.5064e-03, -2.8038e-02,  6.2118e-03, -2.4199e-02]],\n",
       "              \n",
       "                       [[ 2.6313e-02, -4.3059e-02,  2.4977e-02,  2.5487e-02],\n",
       "                        [-5.2689e-02,  5.6238e-02, -1.1113e-02,  2.7467e-02],\n",
       "                        [-5.6928e-02,  4.7678e-02,  4.9929e-02, -2.4567e-02],\n",
       "                        [-4.0126e-02,  2.6165e-02,  2.8096e-02, -1.3262e-02]],\n",
       "              \n",
       "                       [[-1.5241e-02, -4.9865e-02, -3.0649e-02, -5.6201e-02],\n",
       "                        [-3.7809e-02, -5.8392e-02,  3.2957e-02, -5.3015e-02],\n",
       "                        [ 5.6134e-02, -4.5059e-02,  5.3139e-02,  1.6422e-02],\n",
       "                        [-3.3656e-02, -1.3156e-02,  3.3731e-02, -5.6526e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.0993e-04, -5.9903e-02,  7.2492e-03,  3.2686e-02],\n",
       "                        [ 3.5348e-02,  1.3062e-02,  7.3268e-03,  5.6285e-02],\n",
       "                        [-3.6863e-02,  3.5167e-02, -5.4465e-02,  7.9646e-03],\n",
       "                        [ 4.0057e-02, -3.0512e-02, -4.8166e-02, -1.0640e-02]],\n",
       "              \n",
       "                       [[-4.1827e-02, -4.0593e-02, -5.6102e-02, -1.9078e-02],\n",
       "                        [-6.2157e-02, -4.2616e-02, -5.1376e-02,  1.6529e-02],\n",
       "                        [-4.6894e-02, -5.8045e-02, -4.0588e-02, -3.9475e-02],\n",
       "                        [-4.4794e-04, -2.0172e-02,  2.7549e-02, -2.4028e-02]],\n",
       "              \n",
       "                       [[ 2.4436e-02,  2.0295e-02, -1.5302e-02, -3.4627e-02],\n",
       "                        [-3.9566e-02, -2.5330e-02, -3.1956e-02,  3.9019e-02],\n",
       "                        [-4.4450e-02,  3.9916e-02, -5.8966e-02, -1.5856e-02],\n",
       "                        [-3.5399e-02, -2.5612e-03, -6.1692e-02,  3.9768e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.9485e-03,  7.0500e-03, -5.7181e-02,  1.7521e-02],\n",
       "                        [-3.4388e-02, -1.4993e-03, -5.1873e-02, -4.0488e-02],\n",
       "                        [ 3.6909e-02, -4.3262e-02, -3.9441e-02,  1.5778e-02],\n",
       "                        [ 1.2003e-02,  2.9909e-02, -2.2277e-02,  3.1387e-02]],\n",
       "              \n",
       "                       [[ 9.1421e-03, -3.8035e-02,  6.1290e-02,  4.0876e-03],\n",
       "                        [-2.9452e-02, -7.7149e-03, -3.0937e-02,  2.4356e-02],\n",
       "                        [ 5.9988e-04, -3.9889e-02, -9.7106e-03,  2.7208e-02],\n",
       "                        [-5.9400e-02,  5.2807e-02, -4.6414e-02,  4.4889e-02]],\n",
       "              \n",
       "                       [[-4.6721e-02,  4.2596e-02, -1.2048e-02,  2.1231e-02],\n",
       "                        [-2.6004e-02,  5.6774e-03,  6.0925e-02,  2.1559e-02],\n",
       "                        [-1.8617e-02, -4.7925e-03, -3.6504e-03, -3.4825e-02],\n",
       "                        [-1.3388e-03, -5.4640e-02,  1.3641e-02,  5.4313e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.0369e-04, -2.9879e-02, -2.4864e-02,  3.5682e-02],\n",
       "                        [ 3.2134e-02,  2.5731e-03,  4.6492e-02,  3.9593e-03],\n",
       "                        [ 3.6527e-02, -1.8192e-02,  3.5986e-02, -3.9634e-02],\n",
       "                        [ 3.7879e-02,  4.1232e-02,  3.2137e-02, -3.3070e-02]],\n",
       "              \n",
       "                       [[-5.1837e-02, -1.8820e-02, -1.9754e-02,  4.2061e-02],\n",
       "                        [ 3.4883e-03, -4.7338e-02,  1.6460e-02,  7.8565e-03],\n",
       "                        [ 2.0687e-02,  5.2775e-02,  1.0706e-02, -5.9181e-02],\n",
       "                        [ 5.4233e-02,  3.8564e-02, -2.7210e-03, -2.6818e-03]],\n",
       "              \n",
       "                       [[-2.2075e-02, -5.6564e-02,  6.0787e-02,  5.3957e-02],\n",
       "                        [-1.5016e-02, -4.6033e-02,  3.8001e-02,  4.6788e-02],\n",
       "                        [-2.7069e-02, -4.2620e-02, -4.9488e-02,  5.3889e-02],\n",
       "                        [-1.0741e-03, -6.4507e-03,  5.8484e-02, -1.3932e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-4.7334e-02, -1.1218e-02,  3.2147e-02, -5.9014e-02],\n",
       "                        [ 3.5534e-02,  3.8153e-02, -1.0480e-02,  2.7438e-02],\n",
       "                        [ 6.1076e-02, -1.0379e-02, -3.6575e-02, -1.3679e-02],\n",
       "                        [ 1.8572e-02, -5.7360e-02, -2.7986e-02,  4.5955e-03]],\n",
       "              \n",
       "                       [[-5.0357e-02,  5.8122e-02, -1.9839e-02,  7.8327e-03],\n",
       "                        [-2.7062e-02, -5.9159e-02,  5.5140e-02, -5.7384e-03],\n",
       "                        [-1.2959e-02, -4.0346e-02, -3.6978e-02,  4.2834e-02],\n",
       "                        [-3.1957e-02, -5.6072e-02, -3.5015e-02, -1.3666e-02]],\n",
       "              \n",
       "                       [[ 5.9449e-02,  3.5633e-02, -4.7788e-02, -5.5567e-02],\n",
       "                        [-4.0969e-02,  3.5742e-02,  2.8422e-02, -4.8786e-02],\n",
       "                        [-2.9487e-02, -5.2865e-03, -2.7101e-02,  2.4940e-02],\n",
       "                        [ 3.2121e-02,  3.3698e-02,  1.9454e-02,  4.3159e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.3660e-02, -5.4711e-02,  3.3758e-03, -4.0293e-05],\n",
       "                        [-2.5991e-02,  5.2329e-02,  7.8575e-03, -5.2232e-02],\n",
       "                        [-5.9627e-02,  3.5354e-02,  4.1538e-03, -9.8517e-04],\n",
       "                        [-4.7051e-02, -4.1820e-02, -1.8338e-03, -2.3216e-02]],\n",
       "              \n",
       "                       [[-1.6059e-02,  6.2380e-02, -1.7699e-02,  2.9884e-02],\n",
       "                        [-3.6022e-02, -5.4449e-02, -4.0585e-02,  1.0454e-02],\n",
       "                        [ 3.1613e-02,  4.1116e-02,  5.5792e-02,  5.9884e-03],\n",
       "                        [-4.7941e-02,  3.4906e-02, -3.5450e-02, -2.5319e-02]],\n",
       "              \n",
       "                       [[ 2.2873e-02,  7.8797e-03,  2.9297e-02, -5.4051e-02],\n",
       "                        [ 2.6016e-02,  3.7316e-02, -3.0579e-02,  3.9642e-02],\n",
       "                        [-4.0528e-02,  5.2607e-02,  3.8784e-02, -5.2643e-02],\n",
       "                        [ 6.1662e-02, -4.8618e-02, -5.2284e-02, -1.0700e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.8192e-02,  5.5772e-02, -2.6558e-02, -5.4630e-02],\n",
       "                        [ 4.8412e-03, -1.3178e-02,  1.1782e-02,  5.0072e-02],\n",
       "                        [ 2.4409e-02,  2.5412e-02, -4.9216e-03, -1.7495e-02],\n",
       "                        [-4.5547e-02, -3.8591e-02,  4.4319e-02,  7.0357e-03]],\n",
       "              \n",
       "                       [[ 5.5134e-02,  5.1092e-02,  5.2476e-02, -3.6920e-02],\n",
       "                        [-7.4673e-03, -5.0422e-02, -2.9849e-02, -9.1266e-04],\n",
       "                        [-5.5282e-02,  1.4865e-02,  3.7764e-03, -3.1235e-03],\n",
       "                        [ 4.0764e-02, -1.2592e-02,  4.9597e-02,  1.2545e-03]],\n",
       "              \n",
       "                       [[-2.0082e-02,  3.7491e-02,  5.4027e-02, -3.2179e-02],\n",
       "                        [ 4.8842e-02, -1.4052e-02,  4.5880e-02,  3.3213e-02],\n",
       "                        [ 1.3757e-02,  2.4105e-03, -5.1314e-02, -5.8854e-02],\n",
       "                        [ 3.3042e-02, -3.1485e-02,  3.3265e-02,  3.4736e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.5242e-02, -1.3015e-02,  6.1212e-02,  3.6428e-02],\n",
       "                        [ 4.7507e-02,  2.9543e-02, -8.5250e-03,  2.3761e-02],\n",
       "                        [ 2.9567e-02, -3.5936e-03, -2.4744e-02, -1.8357e-02],\n",
       "                        [-3.0582e-02,  1.5246e-02,  1.8942e-03, -2.8626e-02]],\n",
       "              \n",
       "                       [[ 5.0297e-02, -5.3383e-02, -2.3885e-02, -4.6049e-02],\n",
       "                        [ 2.9328e-02,  5.2413e-02, -5.7513e-02,  3.2827e-02],\n",
       "                        [-1.5637e-02,  4.6576e-02,  5.3093e-02,  1.6009e-02],\n",
       "                        [-3.2343e-03,  2.7535e-02,  2.4024e-02,  5.9173e-02]],\n",
       "              \n",
       "                       [[-3.3294e-02,  4.7926e-02, -3.0155e-02, -6.0373e-02],\n",
       "                        [ 1.1034e-03, -2.4374e-02,  5.7345e-02, -1.9953e-02],\n",
       "                        [-5.5382e-02, -5.6842e-02,  3.3445e-02, -1.7794e-02],\n",
       "                        [-3.8359e-02, -3.3685e-02, -1.2111e-02,  3.6564e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.2137e-02, -3.1251e-02, -1.5894e-02,  4.2714e-02],\n",
       "                        [-2.6678e-02, -4.0588e-02,  5.9119e-02,  5.8771e-02],\n",
       "                        [-1.7831e-03, -1.0144e-02,  5.3411e-02,  1.1220e-02],\n",
       "                        [-3.3064e-02, -1.1364e-02,  4.7289e-02, -4.1539e-02]],\n",
       "              \n",
       "                       [[-1.1626e-02,  1.3532e-03,  8.8496e-03,  2.5037e-03],\n",
       "                        [ 1.1285e-02, -4.8692e-02,  1.3780e-02,  2.6923e-02],\n",
       "                        [-6.1242e-02, -1.4644e-02, -4.4013e-02,  1.4648e-02],\n",
       "                        [ 9.8156e-04,  1.1448e-02, -2.4040e-02, -5.1197e-02]],\n",
       "              \n",
       "                       [[-1.3302e-02,  1.4474e-03,  9.8755e-04,  2.0428e-02],\n",
       "                        [ 2.2899e-03,  5.0622e-02, -2.1193e-03, -3.8900e-02],\n",
       "                        [ 3.9090e-02, -7.9422e-03,  3.5033e-02, -2.9254e-02],\n",
       "                        [ 3.6532e-02, -2.2789e-02,  5.0878e-02, -1.5894e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.4829e-02,  5.4809e-02,  1.2536e-02,  2.8972e-02],\n",
       "                        [ 7.3898e-03, -5.4227e-02,  4.0795e-02,  1.0174e-02],\n",
       "                        [-4.4367e-02,  1.4229e-03, -5.4333e-02,  5.7786e-03],\n",
       "                        [-3.1679e-03,  3.4772e-02, -2.2157e-02, -4.7109e-02]],\n",
       "              \n",
       "                       [[ 1.6717e-03,  3.4280e-02,  4.3444e-02,  4.0784e-02],\n",
       "                        [ 3.7986e-02,  1.8985e-02,  4.1727e-02,  3.2097e-05],\n",
       "                        [-2.2136e-04, -3.3108e-02, -4.5384e-02,  4.4007e-02],\n",
       "                        [ 3.1160e-02,  2.7344e-02, -3.1872e-03, -6.0422e-02]],\n",
       "              \n",
       "                       [[ 2.7917e-02, -2.4440e-02,  2.5465e-02, -3.6117e-02],\n",
       "                        [-4.8516e-02, -5.7481e-02,  4.8586e-02,  5.6354e-02],\n",
       "                        [ 9.4610e-03,  1.4249e-02,  2.7876e-02, -2.4138e-02],\n",
       "                        [-6.2341e-02,  1.3941e-02, -3.0571e-02,  5.7327e-02]]]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv_layers.2.bias',\n",
       "              tensor([ 0.0216, -0.0231,  0.0135, -0.0329, -0.0459, -0.0434, -0.0388,  0.0480,\n",
       "                      -0.0214,  0.0423, -0.0266, -0.0306,  0.0061,  0.0401, -0.0189,  0.0145,\n",
       "                       0.0253,  0.0282,  0.0321,  0.0413, -0.0578,  0.0325,  0.0452, -0.0189,\n",
       "                      -0.0007, -0.0533, -0.0488, -0.0124,  0.0149, -0.0009, -0.0259,  0.0040],\n",
       "                     device='cuda:0')),\n",
       "             ('conv_layers.4.weight',\n",
       "              tensor([[[[ 0.0711,  0.0357],\n",
       "                        [-0.0780,  0.0665]],\n",
       "              \n",
       "                       [[ 0.0308,  0.0537],\n",
       "                        [ 0.0508, -0.0314]],\n",
       "              \n",
       "                       [[ 0.0166, -0.0171],\n",
       "                        [ 0.0772,  0.0646]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0606, -0.0235],\n",
       "                        [-0.0677,  0.0630]],\n",
       "              \n",
       "                       [[-0.0723,  0.0378],\n",
       "                        [ 0.0296,  0.0689]],\n",
       "              \n",
       "                       [[-0.0465, -0.0228],\n",
       "                        [ 0.0881,  0.0029]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0371,  0.0809],\n",
       "                        [-0.0475,  0.0188]],\n",
       "              \n",
       "                       [[-0.0539,  0.0847],\n",
       "                        [ 0.0153, -0.0297]],\n",
       "              \n",
       "                       [[ 0.0854,  0.0605],\n",
       "                        [-0.0108, -0.0154]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0854, -0.0440],\n",
       "                        [ 0.0140,  0.0573]],\n",
       "              \n",
       "                       [[ 0.0068, -0.0600],\n",
       "                        [ 0.0677, -0.0165]],\n",
       "              \n",
       "                       [[-0.0110,  0.0479],\n",
       "                        [ 0.0715, -0.0392]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0071, -0.0209],\n",
       "                        [ 0.0802,  0.0490]],\n",
       "              \n",
       "                       [[ 0.0791, -0.0453],\n",
       "                        [-0.0033, -0.0753]],\n",
       "              \n",
       "                       [[-0.0579, -0.0680],\n",
       "                        [-0.0644,  0.0236]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0167,  0.0121],\n",
       "                        [-0.0528, -0.0257]],\n",
       "              \n",
       "                       [[ 0.0110,  0.0717],\n",
       "                        [ 0.0588, -0.0372]],\n",
       "              \n",
       "                       [[ 0.0546,  0.0745],\n",
       "                        [ 0.0376,  0.0548]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0078,  0.0626],\n",
       "                        [-0.0337,  0.0493]],\n",
       "              \n",
       "                       [[ 0.0054, -0.0460],\n",
       "                        [ 0.0824,  0.0685]],\n",
       "              \n",
       "                       [[ 0.0566, -0.0599],\n",
       "                        [ 0.0336,  0.0475]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0399, -0.0266],\n",
       "                        [-0.0433, -0.0571]],\n",
       "              \n",
       "                       [[-0.0415,  0.0585],\n",
       "                        [ 0.0582, -0.0076]],\n",
       "              \n",
       "                       [[-0.0421,  0.0183],\n",
       "                        [-0.0152,  0.0254]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0727, -0.0016],\n",
       "                        [ 0.0330,  0.0054]],\n",
       "              \n",
       "                       [[ 0.0047, -0.0391],\n",
       "                        [-0.0650, -0.0830]],\n",
       "              \n",
       "                       [[ 0.0619, -0.0140],\n",
       "                        [-0.0151,  0.0273]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0871, -0.0386],\n",
       "                        [-0.0229, -0.0536]],\n",
       "              \n",
       "                       [[ 0.0817,  0.0563],\n",
       "                        [ 0.0537,  0.0055]],\n",
       "              \n",
       "                       [[ 0.0784, -0.0506],\n",
       "                        [-0.0785, -0.0856]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0034,  0.0542],\n",
       "                        [ 0.0417,  0.0661]],\n",
       "              \n",
       "                       [[-0.0651, -0.0819],\n",
       "                        [ 0.0867,  0.0119]],\n",
       "              \n",
       "                       [[ 0.0525,  0.0421],\n",
       "                        [ 0.0659,  0.0270]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0636, -0.0305],\n",
       "                        [ 0.0143, -0.0467]],\n",
       "              \n",
       "                       [[-0.0735, -0.0751],\n",
       "                        [-0.0406, -0.0738]],\n",
       "              \n",
       "                       [[-0.0543,  0.0137],\n",
       "                        [ 0.0388, -0.0404]]]], device='cuda:0')),\n",
       "             ('conv_layers.4.bias',\n",
       "              tensor([-1.5932e-02, -6.8645e-02,  7.3928e-02, -1.4258e-02,  2.1565e-02,\n",
       "                       3.2587e-03, -5.8345e-02,  1.4597e-02,  2.8209e-02, -3.2449e-02,\n",
       "                      -1.2962e-02, -7.7798e-02, -3.8827e-02,  2.8943e-02,  8.6967e-02,\n",
       "                       6.1710e-02, -2.1072e-02,  1.0022e-02, -3.6781e-02,  3.9960e-02,\n",
       "                       3.2095e-02,  8.3937e-02, -5.7627e-02, -6.3744e-03, -8.3817e-02,\n",
       "                      -3.7172e-02,  6.3679e-02, -6.4356e-02,  8.0760e-02,  3.7670e-02,\n",
       "                       6.2609e-02, -8.8475e-03,  8.0527e-02, -3.0587e-02, -3.8175e-02,\n",
       "                      -3.5771e-02, -3.0528e-02, -1.8359e-02, -3.9429e-02, -6.1133e-02,\n",
       "                       3.6311e-02, -3.4811e-02, -7.9025e-02,  2.0327e-02, -1.0577e-02,\n",
       "                       4.1166e-02, -8.6907e-05, -8.0293e-02, -1.2052e-02,  3.6232e-02,\n",
       "                       1.5239e-02, -6.8000e-02,  5.5777e-03, -7.2437e-02,  7.4833e-02,\n",
       "                       4.9510e-02,  7.1025e-02, -5.4968e-02,  5.5925e-02, -2.3978e-02,\n",
       "                       4.4967e-02, -3.5788e-03, -3.0697e-02,  9.5427e-03], device='cuda:0')),\n",
       "             ('fc_layers.0.weight',\n",
       "              tensor([[ 9.5516e-03,  6.9742e-03,  1.1173e-02,  ...,  1.2493e-03,\n",
       "                       -6.3855e-03,  9.1106e-03],\n",
       "                      [-3.0791e-03,  1.7385e-03,  3.9872e-03,  ..., -2.3222e-03,\n",
       "                        8.7020e-05,  2.5939e-03],\n",
       "                      [ 1.1803e-02, -2.9767e-03, -2.1611e-04,  ...,  9.9648e-04,\n",
       "                       -6.0788e-03,  5.2207e-03],\n",
       "                      ...,\n",
       "                      [ 1.1674e-02,  1.1405e-02,  2.8120e-03,  ...,  6.2287e-03,\n",
       "                       -3.8663e-03,  5.7241e-03],\n",
       "                      [ 3.1014e-04,  5.5312e-03,  3.3690e-03,  ...,  4.5849e-03,\n",
       "                       -1.7472e-03, -9.5154e-03],\n",
       "                      [ 9.0280e-04,  7.0967e-04, -1.1119e-02,  ...,  4.5342e-03,\n",
       "                       -9.6984e-03, -1.5071e-03]], device='cuda:0')),\n",
       "             ('fc_layers.0.bias',\n",
       "              tensor([-1.2421e-03, -6.6582e-03,  8.7771e-03, -7.8777e-03,  1.2610e-02,\n",
       "                      -1.2544e-02, -9.4505e-03, -1.6684e-03, -1.0052e-03, -2.6267e-03,\n",
       "                      -9.0405e-03, -3.4709e-03, -4.8225e-03,  1.0309e-02,  6.9346e-03,\n",
       "                      -5.5520e-04, -1.8342e-03,  5.7579e-03,  1.2051e-02,  5.5535e-03,\n",
       "                       2.5964e-03,  4.7463e-03,  1.0791e-02, -1.1870e-02, -4.1597e-03,\n",
       "                       1.3316e-03,  6.8292e-03, -1.1990e-02,  2.8067e-04,  4.8187e-03,\n",
       "                      -4.2941e-04,  1.1652e-02, -5.4589e-04, -9.1770e-03, -6.1122e-03,\n",
       "                       8.2809e-03,  7.4792e-03,  7.3131e-03,  1.1273e-02, -9.9195e-03,\n",
       "                      -4.5375e-03, -1.0020e-02,  2.6745e-03, -7.3486e-03,  3.9437e-03,\n",
       "                      -1.9098e-03,  6.4657e-03,  2.6935e-03, -5.4172e-03,  1.2057e-02,\n",
       "                       1.0826e-03, -6.9680e-03, -3.1471e-03, -3.7301e-03,  1.0552e-02,\n",
       "                       7.4466e-03,  2.5872e-03,  9.7249e-03, -3.6708e-03,  1.7779e-03,\n",
       "                      -5.1006e-03, -8.0916e-03,  1.1740e-02,  1.2474e-02, -6.3701e-03,\n",
       "                      -1.0476e-02, -1.0479e-03, -8.7977e-03,  1.1962e-04,  1.2470e-02,\n",
       "                      -5.2948e-03,  4.9500e-03,  4.9683e-03, -6.8230e-03,  5.1950e-03,\n",
       "                      -7.1477e-03,  8.8536e-03, -1.2299e-02, -1.0079e-02, -9.6891e-03,\n",
       "                      -8.9474e-03,  9.7443e-03,  1.1820e-02,  1.2185e-02,  4.5459e-03,\n",
       "                      -7.5274e-03, -6.0753e-03, -8.3735e-03, -8.7302e-03,  5.8834e-03,\n",
       "                      -5.4177e-04,  7.4444e-03,  4.0486e-03,  8.1595e-03, -1.2412e-02,\n",
       "                      -9.1582e-04, -1.1922e-02,  1.1437e-02,  5.9750e-03,  8.0749e-03,\n",
       "                      -1.2025e-02, -1.2280e-02, -7.7150e-03,  1.2039e-03,  8.3058e-03,\n",
       "                      -1.2533e-02,  1.2107e-03,  1.2484e-02, -2.5763e-03, -8.8370e-03,\n",
       "                      -4.0335e-03, -8.4564e-03,  3.3496e-03,  5.4212e-03,  5.3708e-03,\n",
       "                      -2.7552e-03, -9.6461e-04, -9.4269e-03, -4.8222e-03,  1.2222e-02,\n",
       "                      -1.1961e-02, -9.4004e-03,  2.5490e-03,  1.1147e-02, -6.2205e-03,\n",
       "                       8.3431e-03,  8.1439e-03, -1.2363e-02, -6.4591e-03,  7.5438e-03,\n",
       "                      -2.4167e-03, -3.8564e-04, -1.2619e-02,  8.4597e-03, -4.7258e-03,\n",
       "                      -6.1839e-03, -5.3322e-03,  5.8684e-03, -5.3361e-03,  3.7101e-04,\n",
       "                      -1.0669e-03, -5.6764e-03, -4.1372e-03, -8.4467e-03,  1.2242e-02,\n",
       "                      -2.5641e-04,  7.3762e-03, -5.8664e-03, -9.0524e-03,  1.0597e-02,\n",
       "                      -1.5549e-03, -4.4315e-03, -4.9926e-03,  3.3992e-03, -7.7673e-03,\n",
       "                       3.2920e-03,  1.1816e-02,  3.6445e-03, -8.5388e-03,  7.8128e-03,\n",
       "                      -1.1569e-02, -7.3368e-03, -1.1165e-02,  1.2048e-02, -2.4617e-03,\n",
       "                       1.1759e-02,  1.1370e-02, -2.1497e-03,  1.9032e-03,  5.8607e-03,\n",
       "                      -5.1834e-03, -1.0566e-02,  1.2084e-03,  4.7375e-03, -1.7755e-03,\n",
       "                      -4.6591e-03, -1.2298e-02,  4.1739e-04,  1.9128e-03,  5.6588e-03,\n",
       "                      -7.3706e-03,  1.1893e-02, -5.1073e-03, -2.7468e-03, -3.5975e-03,\n",
       "                      -8.0980e-03,  7.1203e-03, -5.0504e-03,  8.1215e-03,  1.1950e-02,\n",
       "                      -7.4219e-03, -2.2265e-03, -7.4294e-03, -1.0244e-02,  1.0840e-02,\n",
       "                       9.5077e-03, -3.7420e-03,  8.6399e-04, -5.5871e-03,  6.7418e-04,\n",
       "                      -9.3567e-03, -8.2691e-03,  2.4317e-03,  1.0532e-02,  5.1376e-03,\n",
       "                       6.0305e-03, -1.2024e-03,  9.9180e-03, -5.6279e-03,  8.7997e-03,\n",
       "                      -1.6645e-03, -5.7128e-03, -1.2231e-02, -6.8821e-03,  9.9051e-03,\n",
       "                       7.5755e-03,  1.1375e-02, -1.1804e-02,  1.1180e-02, -1.0105e-03,\n",
       "                       8.6282e-03, -1.2789e-03, -1.1148e-02,  1.2260e-03, -1.0785e-03,\n",
       "                       1.2328e-02, -4.4437e-03, -7.0573e-03, -3.0131e-03,  1.2350e-02,\n",
       "                      -6.9437e-05, -1.0947e-02,  1.2281e-02,  8.2951e-03,  1.1895e-02,\n",
       "                      -3.9900e-03,  9.0780e-03,  1.0749e-02,  1.0485e-02,  6.1110e-03,\n",
       "                      -2.6648e-03,  1.3545e-03,  1.5800e-03, -1.2389e-03,  1.0344e-02,\n",
       "                       1.8517e-03, -9.5160e-03, -4.6958e-03,  1.1866e-02,  2.2954e-03,\n",
       "                       3.4001e-03,  7.9865e-03,  1.2378e-02,  4.3695e-03,  5.3268e-03,\n",
       "                      -1.0473e-03,  6.5875e-03,  9.5394e-04, -5.2126e-03,  3.8548e-03,\n",
       "                       8.0010e-03, -4.0075e-03,  6.3923e-03,  7.6848e-03,  6.8207e-03,\n",
       "                       5.8557e-03,  7.0715e-03, -8.8695e-03, -8.5962e-03, -1.0616e-02,\n",
       "                      -8.5923e-03,  4.0514e-03, -6.6249e-03, -1.8401e-03, -1.7122e-03,\n",
       "                       7.4473e-04,  9.0079e-03, -9.6784e-03, -9.0256e-03, -1.1430e-02,\n",
       "                       7.2997e-03,  1.0432e-02, -6.3682e-03,  5.8455e-03,  7.6342e-03,\n",
       "                      -7.6578e-03, -3.6686e-03, -3.5231e-03,  8.4582e-03, -3.3761e-04,\n",
       "                       4.7780e-03, -2.1906e-03, -6.0557e-03, -7.5702e-03, -9.4471e-03,\n",
       "                       7.9373e-03, -1.0423e-04, -1.1861e-02, -1.0980e-02,  9.6804e-03,\n",
       "                       6.5036e-04,  6.0176e-03,  3.3915e-03, -9.0857e-03, -1.1177e-02,\n",
       "                       8.3300e-03, -4.9226e-03,  5.1685e-03,  1.2716e-02,  7.0894e-03,\n",
       "                      -7.2095e-03, -4.5379e-03, -1.2031e-02,  3.8643e-04, -1.1475e-02,\n",
       "                       3.7847e-03,  9.1346e-03, -7.4912e-03, -2.2752e-03,  1.6425e-03,\n",
       "                      -9.6693e-03,  9.2118e-03, -1.1586e-02, -1.6952e-04,  5.1587e-03,\n",
       "                      -1.2135e-02,  4.8647e-03,  6.2609e-03, -2.3216e-03, -3.4470e-03,\n",
       "                       8.9438e-03,  1.0331e-02, -7.7902e-03,  4.6442e-03, -1.8262e-03,\n",
       "                       9.2717e-03,  9.1575e-03,  8.3251e-03, -1.2628e-02, -4.6230e-03,\n",
       "                      -1.0446e-02,  4.4000e-03, -1.8333e-03, -7.5100e-04,  5.0906e-03,\n",
       "                       4.3226e-03,  7.4818e-03,  2.6299e-03, -6.2710e-05,  1.5050e-03,\n",
       "                       6.6190e-03,  2.4659e-03, -1.2041e-02,  7.2632e-03,  3.4707e-04,\n",
       "                       6.1542e-03,  8.5364e-03, -1.0066e-03, -8.1661e-03, -1.2615e-03,\n",
       "                       1.0607e-02,  8.8424e-03, -1.1381e-02,  4.9483e-03,  4.0408e-03,\n",
       "                       9.2120e-03,  4.8626e-03, -5.5134e-03, -6.2623e-03,  1.7559e-03,\n",
       "                       4.3641e-03,  9.6732e-03,  1.2144e-02, -1.0948e-02,  3.7767e-03,\n",
       "                      -1.2918e-03,  4.4730e-03, -5.4183e-03, -5.2171e-04,  7.0789e-03,\n",
       "                       1.0288e-02, -5.3743e-03, -1.5778e-03,  1.3661e-03,  2.4956e-03,\n",
       "                       9.6699e-03,  7.3570e-03, -2.1927e-03, -3.6296e-03, -9.6944e-03,\n",
       "                      -1.5099e-04,  6.7443e-03, -7.1523e-03, -1.1735e-02,  6.6919e-03,\n",
       "                      -8.9302e-03,  7.5575e-03, -5.4713e-03, -8.6442e-03,  8.0163e-03,\n",
       "                       2.8552e-03,  1.0895e-02,  9.9699e-03, -8.2452e-03,  5.6171e-03,\n",
       "                      -7.1795e-03,  5.6583e-03, -9.6061e-03,  1.2798e-03,  1.3553e-04,\n",
       "                      -7.3069e-03,  2.1193e-03,  3.1996e-03, -9.3455e-03, -4.4172e-03,\n",
       "                      -7.5983e-03,  2.7064e-03,  1.0730e-02, -3.4536e-04, -7.4183e-03,\n",
       "                      -5.1559e-03, -1.0127e-02, -8.2178e-03, -9.9048e-03,  3.2830e-03,\n",
       "                      -1.6948e-03,  7.9632e-03,  2.3376e-04,  2.7857e-03, -2.2520e-03,\n",
       "                      -4.9141e-03, -4.1256e-03,  3.9639e-03,  7.8799e-03,  1.9490e-03,\n",
       "                       6.6749e-03,  6.9302e-03, -9.5950e-03,  1.0968e-02,  1.0780e-02,\n",
       "                      -8.7800e-04,  1.2633e-02,  1.0309e-02,  5.3979e-03, -7.3690e-03,\n",
       "                      -1.9598e-03,  1.9849e-03, -1.0766e-02, -1.0988e-02,  1.1741e-02,\n",
       "                      -9.0575e-03,  1.2094e-02,  1.1665e-02,  2.7708e-03,  1.2598e-02,\n",
       "                       9.7162e-03,  4.6412e-03,  1.2657e-03, -2.4567e-04, -4.3441e-03,\n",
       "                       9.9605e-03, -2.0098e-03,  2.4886e-03,  2.0186e-03,  3.0391e-03,\n",
       "                      -6.6806e-03,  1.0240e-02,  1.0904e-02,  1.2237e-02,  6.6645e-03,\n",
       "                      -9.0331e-03,  6.2019e-03,  9.2711e-04,  1.6261e-03, -1.8975e-03,\n",
       "                       1.1498e-02, -7.0754e-03,  1.1468e-02,  3.9826e-03,  1.2656e-02,\n",
       "                       1.1637e-02,  6.2004e-03, -1.0226e-02, -9.9670e-04, -3.7765e-03,\n",
       "                       2.1371e-03,  8.1474e-03,  9.5955e-03, -8.9626e-03,  6.4555e-03,\n",
       "                      -4.4142e-03,  3.3543e-03,  1.2013e-02,  9.1745e-03, -1.0143e-02,\n",
       "                      -5.3030e-03, -2.9091e-04,  1.1670e-02,  4.4173e-03, -1.0961e-02,\n",
       "                      -4.3380e-04, -6.4895e-04,  9.3003e-03,  7.2315e-03,  4.8968e-03,\n",
       "                      -1.0726e-03, -8.5893e-03,  4.7850e-03, -2.2979e-03,  5.3573e-03,\n",
       "                      -2.0503e-03,  1.2242e-02], device='cuda:0')),\n",
       "             ('fc_layers.2.weight',\n",
       "              tensor([[ 0.0174,  0.0258,  0.0350,  ...,  0.0185, -0.0193,  0.0005],\n",
       "                      [ 0.0077, -0.0084,  0.0057,  ...,  0.0205, -0.0303, -0.0161],\n",
       "                      [ 0.0414, -0.0259,  0.0356,  ..., -0.0103, -0.0277,  0.0251]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_layers.2.bias',\n",
       "              tensor([-0.0259,  0.0417, -0.0336], device='cuda:0'))])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = retro.make(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array')\n",
    "env = ButtonActionWrapper(env, buttons=['LEFT', 'RIGHT', 'A'])\n",
    "env = CustomRewardWrapper(env)\n",
    "env = MaxAndSkipObservation(env, skip=num_frame_skip)\n",
    "if RESIZE_ENV:\n",
    "  input_shape = (num_stacked_frames, *new_size)\n",
    "  env = ResizeObservation(env, new_size)\n",
    "else:\n",
    "  input_shape = (num_stacked_frames, 320, 224)\n",
    "env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder='../Video',    # Folder to save videos\n",
    "    name_prefix=f'eval-V{version}-E{episode}-S{max_episode_steps}',               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: True    # Record every episode\n",
    ")\n",
    "dim = env.action_space.n\n",
    "print(action_dim)\n",
    "agent = ConvDQNAgent(input_shape, action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "agent.model.state_dict(torch.load(f'../Saved_Models/DQN/DQN-Sonic-V{version}-E{episode}-S{max_episode_steps}.pth', map_location=agent.device))\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "error",
     "timestamp": 1758631647937,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "BqJxUZBDJ3tC",
    "outputId": "00a8a9ef-2810-4895-96d6-efea0c4d1df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: -107.99999999999565\n",
      "Episode: 1 Reward: -107.99999999999565\n",
      "Episode: 2 Reward: -107.99999999999565\n"
     ]
    }
   ],
   "source": [
    "episode = 3\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state = obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar Modelo\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM/XnYwvV6uxmzGUZDVavMC",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
