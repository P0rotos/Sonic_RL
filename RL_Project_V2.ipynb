{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23421,
     "status": "ok",
     "timestamp": 1758605515480,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "uYoFOi1N7Kg8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758605515508,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "dIX408-VQbQi"
   },
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = True\n",
    "LOAD_MODEL = False\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "batch_size = 32\n",
    "num_episodes = 200\n",
    "max_episode_steps = 5400\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 2\n",
    "version = 3\n",
    "prev_model = 'DQN-Sonic-V1-E15-S5400.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758605515485,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "TKpUD3uBV687"
   },
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper that maps discrete actions to a set of button presses for the game.\n",
    "    This simplifies the action space for the agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, buttons):\n",
    "        super().__init__(env)\n",
    "        self.buttons = buttons\n",
    "        # Create a mapping from a single action index to the full button array.\n",
    "        self._actions = np.identity(len(buttons), dtype=np.int8)\n",
    "        self.action_space = gym.spaces.Discrete(len(buttons))\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(self._actions[action])\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758605515487,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "nJBsk4nqTB6B"
   },
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Custom reward shaping to encourage forward movement.\n",
    "    This wrapper modifies the reward based on the agent's horizontal position.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, mov_rew=0.01, score_rew=0.05, hp_rew=4, ring_rew=1, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= (self.mov_rew/2)\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758605515489,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "dWSxhNSlGiUa"
   },
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1758605515501,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "ceJ7e377Gizg"
   },
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, input_shape, num_actions, lr, gamma, epsilon, epsilon_decay, buffer_size):\n",
    "        self.input_shape = input_shape # input_shape is (Stack, Height, Width) or (Stack, Height, Width, Channels)\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        # Correct the input shape calculation for ConvDQN to be (Stack * Channels, Height, Width)\n",
    "        # Assuming input_shape is (Stack, Height, Width) and images are RGB (3 channels)\n",
    "        num_channels = 3 # Assuming RGB images\n",
    "        # The input shape from the environment after wrappers is (Stack, Height, Width, Channels)\n",
    "        # We need to transform it to (Stack * Channels, Height, Width) for the ConvDQN\n",
    "        conv_input_shape = (input_shape[0] * num_channels, *input_shape[1:])\n",
    "\n",
    "        # If the original input_shape included channels, we need to adjust\n",
    "        # Assuming input_shape is (Stack, Height, Width, Channels)\n",
    "        if len(input_shape) == 4:\n",
    "             # input_shape is (Stack, Height, Width, Channels)\n",
    "             # We want (Stack * Channels, Height, Width)\n",
    "             conv_input_shape = (input_shape[0] * input_shape[3], input_shape[1], input_shape[2])\n",
    "\n",
    "        self.device = 'cuda' #if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = ConvDQN(conv_input_shape, num_actions).to(self.device)\n",
    "        #dummy_input = torch.randn(1, conv_input_shape[0], conv_input_shape[1], conv_input_shape[2]).to(self.device)\n",
    "        #self.model = torch.jit.trace(ConvDQN(conv_input_shape, num_actions).to(self.device), dummy_input)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        # self.optimizer = optim.SGD(\n",
    "        #     self.model.parameters(), \n",
    "        #     lr=lr, \n",
    "        #     momentum=0.9\n",
    "        # )\n",
    "        # self.scheduler = ExponentialLR(self.optimizer, gamma=0.997696) \n",
    "\n",
    "    def preprocess(self, state):\n",
    "        # Convert the numpy array (Stack, Height, Width, Channels) to a PyTorch tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        # Permute dimensions from (Stack, Height, Width, Channels) to (Stack, Channels, Height, Width)\n",
    "        state = state.permute(0, 3, 1, 2)\n",
    "        # Reshape to (Stack * Channels, Height, Width)\n",
    "        state = state.reshape(-1, state.shape[2], state.shape[3])\n",
    "        return state\n",
    "    \n",
    "    def preprocess_vectorized(self, state):\n",
    "        is_batch = state.ndim == 5\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        print(state.shape)\n",
    "        if is_batch:\n",
    "            state = state.permute(1, 4, 0, 2, 3)\n",
    "            state = state.reshape(-1, state.shape[2], state.shape[3], state.shape[4])\n",
    "            state = state.permute(1, 0, 2, 3)\n",
    "        else:\n",
    "            state = state.permute(0, 3, 1, 2)\n",
    "            state = state.reshape(-1, state.shape[2], state.shape[3])\n",
    "        return state\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess(state).unsqueeze(0).to(self.device)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = self.preprocess(next_state).unsqueeze(0).to(self.device) # Move to the correct device\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "            state = self.preprocess(state).unsqueeze(0).to(self.device)\n",
    "            current_q_values = self.model(state)\n",
    "            target_f = current_q_values.clone().detach()\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(target_f, current_q_values)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # self.scheduler.step() #for testing scheduler lr\n",
    "            # for param_group in self.optimizer.param_groups:\n",
    "            #     if param_group['lr'] < 0.0001:\n",
    "            #         param_group['lr'] = 0.0001\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def replay_vectorized(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        # Unzip the minibatch into separate lists\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        # Convert lists to tensors and move them to the device\n",
    "        states_tensor = torch.tensor(states, device=self.device)\n",
    "        actions_tensor = torch.tensor(actions, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        next_states_tensor = torch.tensor(next_states, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, device=self.device)\n",
    "        \n",
    "        # Preprocess all states and next states at once\n",
    "        states_tensor = self.preprocess_vectorized(states_tensor)\n",
    "        next_states_tensor = self.preprocess_vectorized(next_states_tensor)\n",
    "        \n",
    "        # Get current Q-values for all states in the batch\n",
    "        current_q_values = self.model(states_tensor)\n",
    "        \n",
    "        # Get max Q-values for all next states in the batch\n",
    "        next_q_values = self.model(next_states_tensor)\n",
    "        max_next_q = torch.max(next_q_values, 1)[0]\n",
    "        \n",
    "        # Calculate target Q-values using the Bellman equation\n",
    "        # Create a new tensor for the targets\n",
    "        target_q_values = rewards_tensor + self.gamma * max_next_q * (~dones_tensor)\n",
    "        \n",
    "        # Create the tensor for target_f\n",
    "        target_f = current_q_values.clone().detach()\n",
    "\n",
    "        # Update the Q-value for the action that was taken\n",
    "        target_f[range(batch_size), actions_tensor.long()] = target_q_values\n",
    "\n",
    "        # Perform a single optimization step for the entire batch\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(target_f, current_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/DQN/DQN-Sonic-V{version}-E{episode}-S{max_episode_steps}.pth' #ppt para jit, pth para statedict\n",
    "    try:\n",
    "        torch.save(agent.model.state_dict(), model_save_path)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('no enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26132141,
     "status": "ok",
     "timestamp": 1758631647667,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "NNa_hjqQhy2S",
    "outputId": "c1030ee9-1e71-465a-be7a-7e279d83b62f"
   },
   "outputs": [],
   "source": [
    "env = retro.make(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array') #rgb_array\n",
    "env = ButtonActionWrapper(env, buttons=['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z']) #['LEFT', 'RIGHT', 'A']\n",
    "env = CustomRewardWrapper(env)\n",
    "env = MaxAndSkipObservation(env, skip=num_frame_skip)\n",
    "if RESIZE_ENV:\n",
    "  # input_shape = (num_stacked_frames, *new_size)\n",
    "  input_shape = (num_stacked_frames, *new_size)\n",
    "  env = ResizeObservation(env, new_size)\n",
    "else:\n",
    "  input_shape = (num_stacked_frames, 224, 320)\n",
    "env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "# env = RecordVideo(\n",
    "#     env,\n",
    "#     video_folder='/content/drive/MyDrive/Video_IA',    # Folder to save videos\n",
    "#     name_prefix=\"eval\",               # Prefix for video filenames\n",
    "#     episode_trigger=lambda x: True    # Record every episode\n",
    "# )\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "agent = ConvDQNAgent(input_shape, action_dim, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.99, buffer_size=10000)\n",
    "if LOAD_MODEL:\n",
    "  agent.model.state_dict(torch.load('../Saved_Models/DQN/'+prev_model, map_location=agent.device))\n",
    "  #agent.model = torch.jit.load('../Saved_Models/DQN/'+prev_model)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "  state, info = env.reset()\n",
    "  total_reward = 0\n",
    "  done = False\n",
    "  frame_count = 0\n",
    "  temp_reward = 0\n",
    "  while not done:\n",
    "    frame_count += 1\n",
    "    action = agent.act(state = state)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    agent.remember(state, action, reward, observation, done)\n",
    "    state = observation\n",
    "    total_reward += reward\n",
    "    temp_reward += reward\n",
    "    agent.replay(batch_size)\n",
    "    if frame_count % 5 == 0:\n",
    "      print(f'step n={frame_count} with reward {temp_reward}')\n",
    "      temp_reward = 0\n",
    "    #env.render()\n",
    "  if (episode+1) % 5 == 0:\n",
    "    save_model(agent, episode)\n",
    "  print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "env.close()\n",
    "print(f\"Episode finished with total reward: {total_reward}\")\n",
    "save_model(agent, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = retro.make(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.buttons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(agent, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array')\n",
    "env = ButtonActionWrapper(env, buttons=['LEFT', 'RIGHT', 'A'])\n",
    "env = CustomRewardWrapper(env)\n",
    "env = MaxAndSkipObservation(env, skip=num_frame_skip)\n",
    "if RESIZE_ENV:\n",
    "  input_shape = (num_stacked_frames, *new_size)\n",
    "  env = ResizeObservation(env, new_size)\n",
    "else:\n",
    "  input_shape = (num_stacked_frames, 320, 224)\n",
    "env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder='../Video',    # Folder to save videos\n",
    "    name_prefix=f'eval-V{version}-E{episode}-S{max_episode_steps}',               # Prefix for video filenames\n",
    "    episode_trigger=lambda x: True    # Record every episode\n",
    ")\n",
    "dim = env.action_space.n\n",
    "print(action_dim)\n",
    "agent = ConvDQNAgent(input_shape, action_dim, lr=0.001, gamma=0.99, epsilon=0, epsilon_decay=0.9955, buffer_size=10000)\n",
    "agent.model.state_dict(torch.load(f'../Saved_Models/DQN/DQN-Sonic-V{version}-E{episode}-S{max_episode_steps}.pth', map_location=agent.device))\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "error",
     "timestamp": 1758631647937,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "BqJxUZBDJ3tC",
    "outputId": "00a8a9ef-2810-4895-96d6-efea0c4d1df9"
   },
   "outputs": [],
   "source": [
    "episode = 3\n",
    "for temp_episode in range(episode):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state = obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {temp_episode} Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar Modelo\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM/XnYwvV6uxmzGUZDVavMC",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
