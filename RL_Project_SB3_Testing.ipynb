{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 23421,
     "status": "ok",
     "timestamp": 1758605515480,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "uYoFOi1N7Kg8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758605515508,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "dIX408-VQbQi"
   },
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = True\n",
    "LOAD_MODEL = False\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "batch_size = 32\n",
    "num_episodes = 200\n",
    "max_episode_steps = 1800\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 2\n",
    "version = 3\n",
    "prev_model = 'DQN-Sonic-V1-E15-S5400.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758605515485,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "TKpUD3uBV687"
   },
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper that maps discrete actions to a set of button presses for the game.\n",
    "    This simplifies the action space for the agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, buttons):\n",
    "        super().__init__(env)\n",
    "        self.buttons = buttons\n",
    "        # Create a mapping from a single action index to the full button array.\n",
    "        self._actions = np.identity(len(buttons), dtype=np.int8)\n",
    "        self.action_space = gym.spaces.Discrete(len(buttons))\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(self._actions[action])\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758605515487,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "nJBsk4nqTB6B"
   },
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Custom reward shaping to encourage forward movement.\n",
    "    This wrapper modifies the reward based on the agent's horizontal position.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, mov_rew=0.01, score_rew=0.05, hp_rew=4, ring_rew=1, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= (self.mov_rew/2)\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758605515489,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "dWSxhNSlGiUa"
   },
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 16, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStackObservation(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Stacks frames manually to ensure the final output shape is (height, width, channel x frames).\n",
    "    \n",
    "    The input to this wrapper is (H, W, C) where C=3 (RGB).\n",
    "    The output is (H, W, stack_size * C), i.e., (84, 84, 12) in the current setup.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, stack_size):\n",
    "        super().__init__(env)\n",
    "        self.stack_size = stack_size\n",
    "        # Use deque to efficiently manage the stack of frames\n",
    "        self.frames = deque(maxlen=stack_size)\n",
    "\n",
    "        # Calculate the new observation space shape\n",
    "        obs_shape = env.observation_space.shape\n",
    "        H, W, C = obs_shape\n",
    "        \n",
    "        # New shape: (H, W, stack_size * C). This achieves the requested H, W, (C*T) format.\n",
    "        new_channels = C * stack_size\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(H, W, new_channels),\n",
    "            dtype=env.observation_space.dtype\n",
    "        )\n",
    "        print(f\"MODIFIED FrameStack (RGB): Final shape is (H, W, C*T) = {self.observation_space.shape}\")\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Concatenate the stacked frames along the last axis (channel axis)\n",
    "        # Resulting shape is (H, W, C*T)\n",
    "        assert len(self.frames) == self.stack_size\n",
    "        return np.concatenate(self.frames, axis=-1)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        # Initialize the stack with the first observation, replicated\n",
    "        for _ in range(self.stack_size):\n",
    "            self.frames.append(observation)\n",
    "        return self._get_observation(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(observation)\n",
    "        return self._get_observation(), reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1758605515501,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "ceJ7e377Gizg"
   },
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, input_shape, num_actions, lr, gamma, epsilon, epsilon_decay, buffer_size):\n",
    "        self.input_shape = input_shape # input_shape is (Stack, Height, Width) or (Stack, Height, Width, Channels)\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        # Correct the input shape calculation for ConvDQN to be (Stack * Channels, Height, Width)\n",
    "        # Assuming input_shape is (Stack, Height, Width) and images are RGB (3 channels)\n",
    "        num_channels = 3 # Assuming RGB images\n",
    "        # The input shape from the environment after wrappers is (Stack, Height, Width, Channels)\n",
    "        # We need to transform it to (Stack * Channels, Height, Width) for the ConvDQN\n",
    "        conv_input_shape = (input_shape[0] * num_channels, *input_shape[1:])\n",
    "\n",
    "        # If the original input_shape included channels, we need to adjust\n",
    "        # Assuming input_shape is (Stack, Height, Width, Channels)\n",
    "        if len(input_shape) == 4:\n",
    "             # input_shape is (Stack, Height, Width, Channels)\n",
    "             # We want (Stack * Channels, Height, Width)\n",
    "             conv_input_shape = (input_shape[0] * input_shape[3], input_shape[1], input_shape[2])\n",
    "\n",
    "        self.device = 'cuda' #if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = ConvDQN(conv_input_shape, num_actions).to(self.device)\n",
    "        #dummy_input = torch.randn(1, conv_input_shape[0], conv_input_shape[1], conv_input_shape[2]).to(self.device)\n",
    "        #self.model = torch.jit.trace(ConvDQN(conv_input_shape, num_actions).to(self.device), dummy_input)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        # self.optimizer = optim.SGD(\n",
    "        #     self.model.parameters(), \n",
    "        #     lr=lr, \n",
    "        #     momentum=0.9\n",
    "        # )\n",
    "        # self.scheduler = ExponentialLR(self.optimizer, gamma=0.997696) \n",
    "\n",
    "    def preprocess(self, state):\n",
    "        # Convert the numpy array (Stack, Height, Width, Channels) to a PyTorch tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        # Permute dimensions from (Stack, Height, Width, Channels) to (Stack, Channels, Height, Width)\n",
    "        state = state.permute(0, 3, 1, 2)\n",
    "        # Reshape to (Stack * Channels, Height, Width)\n",
    "        state = state.reshape(-1, state.shape[2], state.shape[3])\n",
    "        return state\n",
    "    \n",
    "    def preprocess_vectorized(self, state):\n",
    "        is_batch = state.ndim == 5\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        print(state.shape)\n",
    "        if is_batch:\n",
    "            state = state.permute(1, 4, 0, 2, 3)\n",
    "            state = state.reshape(-1, state.shape[2], state.shape[3], state.shape[4])\n",
    "            state = state.permute(1, 0, 2, 3)\n",
    "        else:\n",
    "            state = state.permute(0, 3, 1, 2)\n",
    "            state = state.reshape(-1, state.shape[2], state.shape[3])\n",
    "        return state\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess(state).unsqueeze(0).to(self.device)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = self.preprocess(next_state).unsqueeze(0).to(self.device) # Move to the correct device\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "            state = self.preprocess(state).unsqueeze(0).to(self.device)\n",
    "            current_q_values = self.model(state)\n",
    "            target_f = current_q_values.clone().detach()\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(target_f, current_q_values)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # self.scheduler.step() #for testing scheduler lr\n",
    "            # for param_group in self.optimizer.param_groups:\n",
    "            #     if param_group['lr'] < 0.0001:\n",
    "            #         param_group['lr'] = 0.0001\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def replay_vectorized(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        # Unzip the minibatch into separate lists\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        # Convert lists to tensors and move them to the device\n",
    "        states_tensor = torch.tensor(states, device=self.device)\n",
    "        actions_tensor = torch.tensor(actions, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        next_states_tensor = torch.tensor(next_states, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, device=self.device)\n",
    "        \n",
    "        # Preprocess all states and next states at once\n",
    "        states_tensor = self.preprocess_vectorized(states_tensor)\n",
    "        next_states_tensor = self.preprocess_vectorized(next_states_tensor)\n",
    "        \n",
    "        # Get current Q-values for all states in the batch\n",
    "        current_q_values = self.model(states_tensor)\n",
    "        \n",
    "        # Get max Q-values for all next states in the batch\n",
    "        next_q_values = self.model(next_states_tensor)\n",
    "        max_next_q = torch.max(next_q_values, 1)[0]\n",
    "        \n",
    "        # Calculate target Q-values using the Bellman equation\n",
    "        # Create a new tensor for the targets\n",
    "        target_q_values = rewards_tensor + self.gamma * max_next_q * (~dones_tensor)\n",
    "        \n",
    "        # Create the tensor for target_f\n",
    "        target_f = current_q_values.clone().detach()\n",
    "\n",
    "        # Update the Q-value for the action that was taken\n",
    "        target_f[range(batch_size), actions_tensor.long()] = target_q_values\n",
    "\n",
    "        # Perform a single optimization step for the entire batch\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(target_f, current_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/DQN/DQN-Sonic-V{version}-E{episode}-S{max_episode_steps}.pth' #ppt para jit, pth para statedict\n",
    "    try:\n",
    "        torch.save(agent.model.state_dict(), model_save_path)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('no enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26132141,
     "status": "ok",
     "timestamp": 1758631647667,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "NNa_hjqQhy2S",
    "outputId": "c1030ee9-1e71-465a-be7a-7e279d83b62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODIFIED FrameStack (RGB): Final shape is (H, W, C*T) = (84, 120, 12)\n",
      "12\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.42GB > 2.02GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(action_dim)\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmax_episode_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Saved_Models/DQN/SB3-DQN-Sonic-V\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-E\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-S\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_episode_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:272\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[1;32m    265\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:335\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 335\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:582\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos, dones)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m# Store data in replay buffer (normalized action and unnormalized observation)\u001b[39;00m\n\u001b[0;32m--> 582\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfos\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_current_progress_remaining(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_timesteps)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;66;03m# For DQN, check if the target network should be updated\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;66;03m# and update the exploration schedule\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m# For SAC/TD3, the update is dones as the same time as the gradient update\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# see https://github.com/hill-a/stable-baselines/issues/900\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:499\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._store_transition\u001b[0;34m(self, replay_buffer, buffer_action, new_obs, reward, dones, infos)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vec_normalize_env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m                 next_obs[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vec_normalize_env\u001b[38;5;241m.\u001b[39munnormalize_obs(next_obs[i, :])  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m \u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_original_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# Save the unnormalized observation\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:268\u001b[0m, in \u001b[0;36mReplayBuffer.add\u001b[0;34m(self, obs, next_obs, action, reward, done, infos)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Copy to avoid modification by reference\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(obs)\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize_memory_usage:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations[(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_obs)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = retro.make(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array') #rgb_array\n",
    "env = ButtonActionWrapper(env, buttons=['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z']) #['LEFT', 'RIGHT', 'A']\n",
    "env = CustomRewardWrapper(env)\n",
    "env = MaxAndSkipObservation(env, skip=num_frame_skip)\n",
    "if RESIZE_ENV:\n",
    "  # input_shape = (num_stacked_frames, *new_size)\n",
    "  input_shape = (num_stacked_frames, *new_size)\n",
    "  env = ResizeObservation(env, new_size)\n",
    "else:\n",
    "  input_shape = (num_stacked_frames, 224, 320)\n",
    "env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "\n",
    "model = DQN(\"CnnPolicy\", env, verbose=1, buffer_size=10000)\n",
    "model.learn(total_timesteps=num_episodes*max_episode_steps)\n",
    "model.save(f'../Saved_Models/DQN/SB3-DQN-Sonic-V{version}-E{num_episodes}-S{max_episode_steps}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z']\n"
     ]
    }
   ],
   "source": [
    "print(env.buttons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODIFIED FrameStack (RGB): Final shape is (H, W, C*T) = (84, 120, 12)\n",
      "12\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.42GB > 1.65GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.42GB > 1.57GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "env = retro.make(game=\"SonicTheHedgehog-Genesis\", render_mode='human') #rgb_array\n",
    "env = ButtonActionWrapper(env, buttons=['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z']) #['LEFT', 'RIGHT', 'A']\n",
    "env = CustomRewardWrapper(env)\n",
    "env = MaxAndSkipObservation(env, skip=num_frame_skip)\n",
    "if RESIZE_ENV:\n",
    "  # input_shape = (num_stacked_frames, *new_size)\n",
    "  input_shape = (num_stacked_frames, *new_size)\n",
    "  env = ResizeObservation(env, new_size)\n",
    "else:\n",
    "  input_shape = (num_stacked_frames, 224, 320)\n",
    "env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "model = DQN(\"CnnPolicy\", env, verbose=1, buffer_size=10000)\n",
    "model = DQN.load(f'../Saved_Models/DQN/SB3-DQN-Sonic-V3-E200-S5400', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "error",
     "timestamp": 1758631647937,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "BqJxUZBDJ3tC",
    "outputId": "00a8a9ef-2810-4895-96d6-efea0c4d1df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Reward: 6.970000000000246\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m      8\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#print(f\"Reward: {reward}\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m, in \u001b[0;36mFrameStackObservation.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 42\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_observation(), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/core.py:560\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    559\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/stateful_observation.py:619\u001b[0m, in \u001b[0;36mMaxAndSkipObservation.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    617\u001b[0m info \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip):\n\u001b[0;32m--> 619\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_buffer[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/gymnasium/core.py:595\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 595\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), terminated, truncated, info\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mButtonActionWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/AI/.venv/lib/python3.10/site-packages/retro/retro_env.py:206\u001b[0m, in \u001b[0;36mRetroEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovie:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovie\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mupdate_ram()\n\u001b[1;32m    208\u001b[0m ob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_obs()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "for _ in range(10):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode+=1\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "\n",
    "    print(f\"Episode: {episode} Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar Modelo\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM/XnYwvV6uxmzGUZDVavMC",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
