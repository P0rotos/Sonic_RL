{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 23421,
     "status": "ok",
     "timestamp": 1758605515480,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "uYoFOi1N7Kg8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import TimeLimit, ResizeObservation, RecordVideo, MaxAndSkipObservation\n",
    "from collections import deque\n",
    "import retro\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758605515508,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "dIX408-VQbQi"
   },
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "RESIZE_ENV = True\n",
    "LOAD_MODEL = False\n",
    "Render_Frame_rate=4\n",
    "new_size = (84,120) #Original Size 320, 224\n",
    "batch_size = 32\n",
    "num_episodes = 200\n",
    "max_episode_steps = 5400\n",
    "num_stacked_frames = 4\n",
    "num_frame_skip = 2\n",
    "version = 3\n",
    "prev_model = 'DQN-Sonic-V1-E15-S5400.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758605515485,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "TKpUD3uBV687"
   },
   "outputs": [],
   "source": [
    "class ButtonActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper that maps discrete actions to a set of button presses for the game.\n",
    "    This simplifies the action space for the agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, buttons):\n",
    "        super().__init__(env)\n",
    "        self.buttons = buttons\n",
    "        # Create a mapping from a single action index to the full button array.\n",
    "        self._actions = np.identity(len(buttons), dtype=np.int8)\n",
    "        self.action_space = gym.spaces.Discrete(len(buttons))\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(self._actions[action])\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758605515487,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "nJBsk4nqTB6B"
   },
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Custom reward shaping to encourage forward movement.\n",
    "    This wrapper modifies the reward based on the agent's horizontal position.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, mov_rew=0.01, score_rew=0.05, hp_rew=4, ring_rew=1, end_bonus=100):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.mov_rew = mov_rew\n",
    "        self.score_rew = score_rew\n",
    "        self.hp_rew = hp_rew\n",
    "        self.ring_rew = ring_rew\n",
    "        self.end_bonus = end_bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        game_variables = self.env.unwrapped.data.lookup_all()\n",
    "\n",
    "        self.previous_pos_x = game_variables['x']\n",
    "        self.previous_score = game_variables['score']\n",
    "        self.previous_lives = game_variables['lives']\n",
    "        self.previous_rings = game_variables['rings']\n",
    "        self.previous_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #print(f\"Reward original: {reward}\")\n",
    "        custom_reward = reward\n",
    "        game_state = self.env.unwrapped.data\n",
    "\n",
    "        if game_state:\n",
    "            game_variables = game_state.lookup_all()\n",
    "            current_pos_x = game_variables['x']\n",
    "            current_score = game_variables['score']\n",
    "            current_lives = game_variables['lives']\n",
    "            current_rings = game_variables['rings']\n",
    "            current_end_bonus = game_variables['level_end_bonus']\n",
    "\n",
    "            # moverse hacia la derecha\n",
    "            if current_pos_x > self.previous_pos_x:\n",
    "                #Recompensa\n",
    "                custom_reward += self.mov_rew\n",
    "            else:\n",
    "                #Penalizacion\n",
    "                custom_reward -= (self.mov_rew/2)\n",
    "\n",
    "            #Recompensa por puntaje\n",
    "            if current_score > self.previous_score:\n",
    "                custom_reward += self.score_rew*(current_score-self.previous_score)\n",
    "            \n",
    "            #Recompensa por ganar vida\n",
    "            if current_lives > self.previous_lives:\n",
    "                custom_reward += self.hp_rew*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Penalizacion por perder vida\n",
    "            if current_lives < self.previous_lives:\n",
    "                custom_reward += (self.hp_rew/2)*(current_lives-self.previous_lives)\n",
    "\n",
    "            #Recompensa por conseguir anillos\n",
    "            if current_rings > self.previous_rings:\n",
    "                custom_reward += self.ring_rew*(current_rings-self.previous_rings)\n",
    "            \n",
    "            #Penalizacion por perder anillos\n",
    "            if current_rings < self.previous_rings:\n",
    "                custom_reward += (self.ring_rew/2)*(current_rings-self.previous_rings)\n",
    "\n",
    "            #Recompensa por completar nivel\n",
    "            if current_end_bonus > self.previous_end_bonus:\n",
    "                custom_reward += self.end_bonus\n",
    "\n",
    "            self.previous_pos_x = current_pos_x\n",
    "            self.previous_score = current_score\n",
    "            self.previous_lives = current_lives\n",
    "            self.previous_rings = current_rings\n",
    "            self.previous_end_bonus = current_end_bonus\n",
    "\n",
    "\n",
    "        return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758605515489,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "dWSxhNSlGiUa"
   },
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 16, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.calc_conv_output(input_shape), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def calc_conv_output(self, shape):\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_layers(x).view(x.size()[0], -1)\n",
    "        return self.fc_layers(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStackObservation(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Stacks frames manually to ensure the final output shape is (height, width, channel x frames).\n",
    "    \n",
    "    The input to this wrapper is (H, W, C) where C=3 (RGB).\n",
    "    The output is (H, W, stack_size * C), i.e., (84, 84, 12) in the current setup.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, stack_size):\n",
    "        super().__init__(env)\n",
    "        self.stack_size = stack_size\n",
    "        # Use deque to efficiently manage the stack of frames\n",
    "        self.frames = deque(maxlen=stack_size)\n",
    "\n",
    "        # Calculate the new observation space shape\n",
    "        obs_shape = env.observation_space.shape\n",
    "        H, W, C = obs_shape\n",
    "        \n",
    "        # New shape: (H, W, stack_size * C). This achieves the requested H, W, (C*T) format.\n",
    "        new_channels = C * stack_size\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(H, W, new_channels),\n",
    "            dtype=env.observation_space.dtype\n",
    "        )\n",
    "        print(f\"MODIFIED FrameStack (RGB): Final shape is (H, W, C*T) = {self.observation_space.shape}\")\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Concatenate the stacked frames along the last axis (channel axis)\n",
    "        # Resulting shape is (H, W, C*T)\n",
    "        assert len(self.frames) == self.stack_size\n",
    "        return np.concatenate(self.frames, axis=-1)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        # Initialize the stack with the first observation, replicated\n",
    "        for _ in range(self.stack_size):\n",
    "            self.frames.append(observation)\n",
    "        return self._get_observation(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(observation)\n",
    "        return self._get_observation(), reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1758605515501,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "ceJ7e377Gizg"
   },
   "outputs": [],
   "source": [
    "class ConvDQNAgent:\n",
    "    def __init__(self, input_shape, num_actions, lr, gamma, epsilon, epsilon_decay, buffer_size):\n",
    "        self.input_shape = input_shape # input_shape is (Stack, Height, Width) or (Stack, Height, Width, Channels)\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        # Correct the input shape calculation for ConvDQN to be (Stack * Channels, Height, Width)\n",
    "        # Assuming input_shape is (Stack, Height, Width) and images are RGB (3 channels)\n",
    "        num_channels = 3 # Assuming RGB images\n",
    "        # The input shape from the environment after wrappers is (Stack, Height, Width, Channels)\n",
    "        # We need to transform it to (Stack * Channels, Height, Width) for the ConvDQN\n",
    "        conv_input_shape = (input_shape[0] * num_channels, *input_shape[1:])\n",
    "\n",
    "        # If the original input_shape included channels, we need to adjust\n",
    "        # Assuming input_shape is (Stack, Height, Width, Channels)\n",
    "        if len(input_shape) == 4:\n",
    "             # input_shape is (Stack, Height, Width, Channels)\n",
    "             # We want (Stack * Channels, Height, Width)\n",
    "             conv_input_shape = (input_shape[0] * input_shape[3], input_shape[1], input_shape[2])\n",
    "\n",
    "        self.device = 'cuda' #if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = ConvDQN(conv_input_shape, num_actions).to(self.device)\n",
    "        #dummy_input = torch.randn(1, conv_input_shape[0], conv_input_shape[1], conv_input_shape[2]).to(self.device)\n",
    "        #self.model = torch.jit.trace(ConvDQN(conv_input_shape, num_actions).to(self.device), dummy_input)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        # self.optimizer = optim.SGD(\n",
    "        #     self.model.parameters(), \n",
    "        #     lr=lr, \n",
    "        #     momentum=0.9\n",
    "        # )\n",
    "        # self.scheduler = ExponentialLR(self.optimizer, gamma=0.997696) \n",
    "\n",
    "    def preprocess(self, state):\n",
    "        # Convert the numpy array (Stack, Height, Width, Channels) to a PyTorch tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        # Permute dimensions from (Stack, Height, Width, Channels) to (Stack, Channels, Height, Width)\n",
    "        state = state.permute(0, 3, 1, 2)\n",
    "        # Reshape to (Stack * Channels, Height, Width)\n",
    "        state = state.reshape(-1, state.shape[2], state.shape[3])\n",
    "        return state\n",
    "    \n",
    "    def preprocess_vectorized(self, state):\n",
    "        is_batch = state.ndim == 5\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        print(state.shape)\n",
    "        if is_batch:\n",
    "            state = state.permute(1, 4, 0, 2, 3)\n",
    "            state = state.reshape(-1, state.shape[2], state.shape[3], state.shape[4])\n",
    "            state = state.permute(1, 0, 2, 3)\n",
    "        else:\n",
    "            state = state.permute(0, 3, 1, 2)\n",
    "            state = state.reshape(-1, state.shape[2], state.shape[3])\n",
    "        return state\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = self.preprocess(state).unsqueeze(0).to(self.device)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = self.preprocess(next_state).unsqueeze(0).to(self.device) # Move to the correct device\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "            state = self.preprocess(state).unsqueeze(0).to(self.device)\n",
    "            current_q_values = self.model(state)\n",
    "            target_f = current_q_values.clone().detach()\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(target_f, current_q_values)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # self.scheduler.step() #for testing scheduler lr\n",
    "            # for param_group in self.optimizer.param_groups:\n",
    "            #     if param_group['lr'] < 0.0001:\n",
    "            #         param_group['lr'] = 0.0001\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def replay_vectorized(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        # Unzip the minibatch into separate lists\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        # Convert lists to tensors and move them to the device\n",
    "        states_tensor = torch.tensor(states, device=self.device)\n",
    "        actions_tensor = torch.tensor(actions, device=self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        next_states_tensor = torch.tensor(next_states, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, device=self.device)\n",
    "        \n",
    "        # Preprocess all states and next states at once\n",
    "        states_tensor = self.preprocess_vectorized(states_tensor)\n",
    "        next_states_tensor = self.preprocess_vectorized(next_states_tensor)\n",
    "        \n",
    "        # Get current Q-values for all states in the batch\n",
    "        current_q_values = self.model(states_tensor)\n",
    "        \n",
    "        # Get max Q-values for all next states in the batch\n",
    "        next_q_values = self.model(next_states_tensor)\n",
    "        max_next_q = torch.max(next_q_values, 1)[0]\n",
    "        \n",
    "        # Calculate target Q-values using the Bellman equation\n",
    "        # Create a new tensor for the targets\n",
    "        target_q_values = rewards_tensor + self.gamma * max_next_q * (~dones_tensor)\n",
    "        \n",
    "        # Create the tensor for target_f\n",
    "        target_f = current_q_values.clone().detach()\n",
    "\n",
    "        # Update the Q-value for the action that was taken\n",
    "        target_f[range(batch_size), actions_tensor.long()] = target_q_values\n",
    "\n",
    "        # Perform a single optimization step for the entire batch\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()(target_f, current_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar Modelo\n",
    "def save_model(agent, episode):\n",
    "    model_save_path = f'../Saved_Models/DQN/DQN-Sonic-V{version}-E{episode}-S{max_episode_steps}.pth' #ppt para jit, pth para statedict\n",
    "    try:\n",
    "        torch.save(agent.model.state_dict(), model_save_path)\n",
    "        #torch.save(agent.model, model_save_path)\n",
    "        print(f'Modelo exitosamente guardado en {model_save_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error guardando el modelo error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no enviroment to close\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    print('no enviroment to close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26132141,
     "status": "ok",
     "timestamp": 1758631647667,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "NNa_hjqQhy2S",
    "outputId": "c1030ee9-1e71-465a-be7a-7e279d83b62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODIFIED FrameStack (RGB): Final shape is (H, W, C*T) = (84, 120, 12)\n",
      "12\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.4e+03  |\n",
      "|    ep_rew_mean      | 158      |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 158      |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 21600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000616 |\n",
      "|    n_updates        | 5374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.4e+03  |\n",
      "|    ep_rew_mean      | 182      |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 156      |\n",
      "|    time_elapsed     | 276      |\n",
      "|    total_timesteps  | 43200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.76e-05 |\n",
      "|    n_updates        | 10774    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.4e+03  |\n",
      "|    ep_rew_mean      | 153      |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 153      |\n",
      "|    time_elapsed     | 422      |\n",
      "|    total_timesteps  | 64800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000149 |\n",
      "|    n_updates        | 16174    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.4e+03  |\n",
      "|    ep_rew_mean      | 139      |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 150      |\n",
      "|    time_elapsed     | 574      |\n",
      "|    total_timesteps  | 86400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.56e-05 |\n",
      "|    n_updates        | 21574    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.4e+03  |\n",
      "|    ep_rew_mean      | 135      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 147      |\n",
      "|    time_elapsed     | 733      |\n",
      "|    total_timesteps  | 108000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000115 |\n",
      "|    n_updates        | 26974    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.14e+03 |\n",
      "|    ep_rew_mean      | 128      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 145      |\n",
      "|    time_elapsed     | 848      |\n",
      "|    total_timesteps  | 123264   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000156 |\n",
      "|    n_updates        | 30790    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.17e+03 |\n",
      "|    ep_rew_mean      | 159      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 143      |\n",
      "|    time_elapsed     | 1010     |\n",
      "|    total_timesteps  | 144864   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00068  |\n",
      "|    n_updates        | 36190    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.2e+03  |\n",
      "|    ep_rew_mean      | 160      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 141      |\n",
      "|    time_elapsed     | 1172     |\n",
      "|    total_timesteps  | 166464   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000941 |\n",
      "|    n_updates        | 41590    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.04e+03 |\n",
      "|    ep_rew_mean      | 167      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 141      |\n",
      "|    time_elapsed     | 1284     |\n",
      "|    total_timesteps  | 181349   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000907 |\n",
      "|    n_updates        | 45312    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.07e+03 |\n",
      "|    ep_rew_mean      | 164      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 1446     |\n",
      "|    total_timesteps  | 202949   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000211 |\n",
      "|    n_updates        | 50712    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.03e+03 |\n",
      "|    ep_rew_mean      | 175      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 139      |\n",
      "|    time_elapsed     | 1583     |\n",
      "|    total_timesteps  | 221119   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00151  |\n",
      "|    n_updates        | 55254    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.06e+03 |\n",
      "|    ep_rew_mean      | 173      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 139      |\n",
      "|    time_elapsed     | 1745     |\n",
      "|    total_timesteps  | 242719   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000307 |\n",
      "|    n_updates        | 60654    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.08e+03 |\n",
      "|    ep_rew_mean      | 185      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 138      |\n",
      "|    time_elapsed     | 1908     |\n",
      "|    total_timesteps  | 264319   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000257 |\n",
      "|    n_updates        | 66054    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.11e+03 |\n",
      "|    ep_rew_mean      | 182      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 138      |\n",
      "|    time_elapsed     | 2070     |\n",
      "|    total_timesteps  | 285919   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000413 |\n",
      "|    n_updates        | 71454    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.13e+03 |\n",
      "|    ep_rew_mean      | 178      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 137      |\n",
      "|    time_elapsed     | 2233     |\n",
      "|    total_timesteps  | 307519   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000338 |\n",
      "|    n_updates        | 76854    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.14e+03 |\n",
      "|    ep_rew_mean      | 182      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 137      |\n",
      "|    time_elapsed     | 2396     |\n",
      "|    total_timesteps  | 329119   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00069  |\n",
      "|    n_updates        | 82254    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.08e+03 |\n",
      "|    ep_rew_mean      | 181      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 137      |\n",
      "|    time_elapsed     | 2520     |\n",
      "|    total_timesteps  | 345570   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000528 |\n",
      "|    n_updates        | 86367    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.08e+03 |\n",
      "|    ep_rew_mean      | 181      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 136      |\n",
      "|    time_elapsed     | 2673     |\n",
      "|    total_timesteps  | 365891   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000397 |\n",
      "|    n_updates        | 91447    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.07e+03 |\n",
      "|    ep_rew_mean      | 177      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 136      |\n",
      "|    time_elapsed     | 2819     |\n",
      "|    total_timesteps  | 385368   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000464 |\n",
      "|    n_updates        | 96316    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.09e+03 |\n",
      "|    ep_rew_mean      | 172      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 136      |\n",
      "|    time_elapsed     | 2981     |\n",
      "|    total_timesteps  | 406968   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000514 |\n",
      "|    n_updates        | 101716   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.1e+03  |\n",
      "|    ep_rew_mean      | 165      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 136      |\n",
      "|    time_elapsed     | 3144     |\n",
      "|    total_timesteps  | 428568   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000519 |\n",
      "|    n_updates        | 107116   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.12e+03 |\n",
      "|    ep_rew_mean      | 162      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 136      |\n",
      "|    time_elapsed     | 3306     |\n",
      "|    total_timesteps  | 450168   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000439 |\n",
      "|    n_updates        | 112516   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.13e+03 |\n",
      "|    ep_rew_mean      | 163      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 136      |\n",
      "|    time_elapsed     | 3468     |\n",
      "|    total_timesteps  | 471768   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.09     |\n",
      "|    n_updates        | 117916   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.12e+03 |\n",
      "|    ep_rew_mean      | 159      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 3613     |\n",
      "|    total_timesteps  | 491158   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 122764   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.09e+03 |\n",
      "|    ep_rew_mean      | 158      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 3746     |\n",
      "|    total_timesteps  | 508746   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000631 |\n",
      "|    n_updates        | 127161   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.09e+03 |\n",
      "|    ep_rew_mean      | 153      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 3908     |\n",
      "|    total_timesteps  | 530346   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000312 |\n",
      "|    n_updates        | 132561   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.02e+03 |\n",
      "|    ep_rew_mean      | 152      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4017     |\n",
      "|    total_timesteps  | 544866   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00168  |\n",
      "|    n_updates        | 136191   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.02e+03 |\n",
      "|    ep_rew_mean      | 152      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4180     |\n",
      "|    total_timesteps  | 566466   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00138  |\n",
      "|    n_updates        | 141591   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.02e+03 |\n",
      "|    ep_rew_mean      | 159      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4342     |\n",
      "|    total_timesteps  | 588066   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000653 |\n",
      "|    n_updates        | 146991   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.87e+03 |\n",
      "|    ep_rew_mean      | 160      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4393     |\n",
      "|    total_timesteps  | 594705   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00166  |\n",
      "|    n_updates        | 148651   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.86e+03 |\n",
      "|    ep_rew_mean      | 163      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4500     |\n",
      "|    total_timesteps  | 609096   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00108  |\n",
      "|    n_updates        | 152248   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.78e+03 |\n",
      "|    ep_rew_mean      | 158      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4607     |\n",
      "|    total_timesteps  | 623263   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0013   |\n",
      "|    n_updates        | 155790   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.7e+03  |\n",
      "|    ep_rew_mean      | 160      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4708     |\n",
      "|    total_timesteps  | 636699   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00257  |\n",
      "|    n_updates        | 159149   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76e+03 |\n",
      "|    ep_rew_mean      | 162      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4867     |\n",
      "|    total_timesteps  | 657796   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00107  |\n",
      "|    n_updates        | 164423   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.72e+03 |\n",
      "|    ep_rew_mean      | 171      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4995     |\n",
      "|    total_timesteps  | 674783   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000823 |\n",
      "|    n_updates        | 168670   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.75e+03 |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 5157     |\n",
      "|    total_timesteps  | 696383   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00111  |\n",
      "|    n_updates        | 174070   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.63e+03 |\n",
      "|    ep_rew_mean      | 168      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 5226     |\n",
      "|    total_timesteps  | 705466   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00216  |\n",
      "|    n_updates        | 176341   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.51e+03 |\n",
      "|    ep_rew_mean      | 158      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 5303     |\n",
      "|    total_timesteps  | 715715   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00121  |\n",
      "|    n_updates        | 178903   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.43e+03 |\n",
      "|    ep_rew_mean      | 154      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 5399     |\n",
      "|    total_timesteps  | 728615   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000348 |\n",
      "|    n_updates        | 182128   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.43e+03 |\n",
      "|    ep_rew_mean      | 161      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 5562     |\n",
      "|    total_timesteps  | 750215   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000566 |\n",
      "|    n_updates        | 187528   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.43e+03 |\n",
      "|    ep_rew_mean      | 158      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 5725     |\n",
      "|    total_timesteps  | 771815   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00368  |\n",
      "|    n_updates        | 192928   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.48e+03 |\n",
      "|    ep_rew_mean      | 158      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 5887     |\n",
      "|    total_timesteps  | 793415   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000786 |\n",
      "|    n_updates        | 198328   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44e+03 |\n",
      "|    ep_rew_mean      | 162      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 6014     |\n",
      "|    total_timesteps  | 810288   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0021   |\n",
      "|    n_updates        | 202546   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44e+03 |\n",
      "|    ep_rew_mean      | 168      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 6158     |\n",
      "|    total_timesteps  | 829343   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0012   |\n",
      "|    n_updates        | 207310   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4e+03  |\n",
      "|    ep_rew_mean      | 173      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 6292     |\n",
      "|    total_timesteps  | 847146   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000864 |\n",
      "|    n_updates        | 211761   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4e+03  |\n",
      "|    ep_rew_mean      | 181      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 6455     |\n",
      "|    total_timesteps  | 868746   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00177  |\n",
      "|    n_updates        | 217161   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4e+03  |\n",
      "|    ep_rew_mean      | 187      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 6616     |\n",
      "|    total_timesteps  | 890346   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00187  |\n",
      "|    n_updates        | 222561   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36e+03 |\n",
      "|    ep_rew_mean      | 184      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 6749     |\n",
      "|    total_timesteps  | 908020   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00101  |\n",
      "|    n_updates        | 226979   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.32e+03 |\n",
      "|    ep_rew_mean      | 189      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 6863     |\n",
      "|    total_timesteps  | 923208   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.006    |\n",
      "|    n_updates        | 230776   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.3e+03  |\n",
      "|    ep_rew_mean      | 188      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 6981     |\n",
      "|    total_timesteps  | 938852   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0026   |\n",
      "|    n_updates        | 234687   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.22e+03 |\n",
      "|    ep_rew_mean      | 191      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 7084     |\n",
      "|    total_timesteps  | 952624   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00359  |\n",
      "|    n_updates        | 238130   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.29e+03 |\n",
      "|    ep_rew_mean      | 192      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 7246     |\n",
      "|    total_timesteps  | 974224   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00127  |\n",
      "|    n_updates        | 243530   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.29e+03 |\n",
      "|    ep_rew_mean      | 196      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 7409     |\n",
      "|    total_timesteps  | 995824   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0011   |\n",
      "|    n_updates        | 248930   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.29e+03 |\n",
      "|    ep_rew_mean      | 194      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 7572     |\n",
      "|    total_timesteps  | 1017424  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00169  |\n",
      "|    n_updates        | 254330   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.41e+03 |\n",
      "|    ep_rew_mean      | 195      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 7709     |\n",
      "|    total_timesteps  | 1035725  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00284  |\n",
      "|    n_updates        | 258906   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.48e+03 |\n",
      "|    ep_rew_mean      | 198      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 7872     |\n",
      "|    total_timesteps  | 1057325  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00176  |\n",
      "|    n_updates        | 264306   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.56e+03 |\n",
      "|    ep_rew_mean      | 195      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 8035     |\n",
      "|    total_timesteps  | 1078925  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00298  |\n",
      "|    n_updates        | 269706   |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = retro.make(game=\"SonicTheHedgehog-Genesis\", render_mode='rgb_array') #rgb_array\n",
    "env = ButtonActionWrapper(env, buttons=['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z']) #['LEFT', 'RIGHT', 'A']\n",
    "env = CustomRewardWrapper(env)\n",
    "env = MaxAndSkipObservation(env, skip=num_frame_skip)\n",
    "if RESIZE_ENV:\n",
    "  # input_shape = (num_stacked_frames, *new_size)\n",
    "  input_shape = (num_stacked_frames, *new_size)\n",
    "  env = ResizeObservation(env, new_size)\n",
    "else:\n",
    "  input_shape = (num_stacked_frames, 224, 320)\n",
    "env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "\n",
    "model = DQN(\"CnnPolicy\", env, verbose=1, buffer_size=10000)\n",
    "model.learn(total_timesteps=num_episodes*max_episode_steps)\n",
    "model.save(f'../Saved_Models/DQN/SB3-DQN-Sonic-V{version}-E{num_episodes}-S{max_episode_steps}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z']\n"
     ]
    }
   ],
   "source": [
    "print(env.buttons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODIFIED FrameStack (RGB): Final shape is (H, W, C*T) = (84, 120, 12)\n",
      "12\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seba/Documentos/AI/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.42GB > 0.49GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "env = retro.make(game=\"SonicTheHedgehog-Genesis\", render_mode='human') #rgb_array\n",
    "env = ButtonActionWrapper(env, buttons=['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z']) #['LEFT', 'RIGHT', 'A']\n",
    "env = CustomRewardWrapper(env)\n",
    "env = MaxAndSkipObservation(env, skip=num_frame_skip)\n",
    "if RESIZE_ENV:\n",
    "  # input_shape = (num_stacked_frames, *new_size)\n",
    "  input_shape = (num_stacked_frames, *new_size)\n",
    "  env = ResizeObservation(env, new_size)\n",
    "else:\n",
    "  input_shape = (num_stacked_frames, 224, 320)\n",
    "env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "env = FrameStackObservation(env, stack_size=num_stacked_frames)\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "model = DQN(\"CnnPolicy\", env, verbose=1, buffer_size=10000)\n",
    "model = DQN.load(f'../Saved_Models/DQN/SB3-DQN-Sonic-V{version}-E{num_episodes}-S{max_episode_steps}', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "error",
     "timestamp": 1758631647937,
     "user": {
      "displayName": "Sebastián Ignacio García Péndola",
      "userId": "08175071328414149400"
     },
     "user_tz": 180
    },
    "id": "BqJxUZBDJ3tC",
    "outputId": "00a8a9ef-2810-4895-96d6-efea0c4d1df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Reward: -29.03000000000139\n",
      "Episode: 2 Reward: -29.03000000000139\n",
      "Episode: 3 Reward: -29.03000000000139\n",
      "Episode: 4 Reward: -29.03000000000139\n",
      "Episode: 5 Reward: -29.03000000000139\n",
      "Episode: 6 Reward: -29.03000000000139\n",
      "Episode: 7 Reward: -29.03000000000139\n",
      "Episode: 8 Reward: -29.03000000000139\n",
      "Episode: 9 Reward: -29.03000000000139\n",
      "Episode: 10 Reward: -29.03000000000139\n"
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "for _ in range(10):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode+=1\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        #print(f\"Reward: {reward}\")\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "\n",
    "    print(f\"Episode: {episode} Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar Modelo\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM/XnYwvV6uxmzGUZDVavMC",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
